{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":"python","separator":"[\\s\\-]+"},"docs":[{"location":"","text":"","title":"About"},{"location":"publications/","text":"Whitepaper Blogs Outline our vision, the motivation, main ideas and the principles upon which Fybrik is built Additional reading on Fybrik concepts, use cases, components and more","title":"Publications"},{"location":"concepts/architecture/","text":"Architecture Fybrik takes a modular approach to provide an open platform for controlling and securing the use of data across an organization. The figure below showcases the current architecture of the Fybrik platform, running on top of Kubernetes. The storage systems shown in the lower half of the figure are merely an example. The core parts of Fybrik are based on Kubernetes controllers and Custom Resource Definitions (CRDs) in order to define its work items and reconcile the state of them. The primary interaction object for a data user is the FybrikApplication custom resource where a user defines which data should be used for which purpose. The following chart and description describe the architecture and components of Fybrik relative to when they are used. Before the data user can perform any actions a data operator has to install Fybrik and modules. Modules describe capabilities that can be included in a data plane. These may be existing open source or third party service, or custom ones. The module of a service indicates the capabilities it supports, the formats and interfaces, and how to deploy the service. Modules may describe externally deployed services, or services deployed by fybrik. Examples of modules are those that provide read/write access or produce implicit copies that serve as lower latency caches of remote assets. Modules also may also perform actions usesd to enforce data governance policy decisions, such as masking or redaction as examples. Fybrik connects to external services to receive data governance decisions, metadata about datasets and credentials. Policies, assets and access credentials to the assets have to be defined before the user can run an application. The current abstraction supports 2 different connectors : one for data catalog and one for policy manager. It is designed in an open way so that multiple different catalog and policy frameworks of all kinds of cloud and on-prem systems can be supported. The data steward configures policies in an external policy manager over assets defined in an external data catalog. Dataset credentials are retrieved from Vault by using Vault API . Vault uses a custom secret engine implemented with HashiCorp Vault plugins system to retrieve the credentials from where they are stored (data catalog for example). Once a developer submits a FybrikApplication CRD to Kubernetes, the FybrikApplicationController will make sure that all the specs are fulfilled and will make sure that the data is read/written/copied/deleted according to the data governance policies. The FybrikApplication holds metadata about the application such as the data assets required by the application, the processing purpose and the method of access the user wishes (protocol e.g. S3 or Arrow flight). It uses this information to check with the data governance policy manager (4) if the data flow requested is allowed and whether restrictive policies such as masking or hashing have to be applied. It compiles plotters based on the governance decisions received via the data governance policy connector and chooses the modules (5) which are best fit for the requirements that the user specified regarding the access protocol and availability, and based on the config policies defined. The plotter contains the modules required and all the information required to deploy them on the cluster where they will run, as well as the flow of data between the asset and the workload through the chosen modules. As data assets may reside in different clusters/clouds a Blueprint CRD is created for each cluster, containing the information regarding the services to be deployed or configured in the given cluster.Depending on the setup the PlotterController will use various methods to distribute the blueprints. In a multi cluster setup the default distribution implementation is using Razee to control remote blueprints, but several multi-cloud tools could be used as a replacement. The PlotterController also collects statuses and distributes updates of said blueprints. Once all the blueprints on all clusters are ready the plotter is marked as ready. A single blueprint contains the specification of all assets that shall be accessed in a single cluster by a single application. The BlueprintController makes sure that a blueprint can deploy all needed modules (8) and (9) and tracks their status (10). Once e.g. an implicit-copy module finishes the copy the blueprint is also in a ready state. A read or write module is in ready state as soon as the proxy service such as the arrow-flight module is running. In this example an implicit-copy module copies data from a remote postgres database into a S3 compatible ceph instance. The arrow-flight module then locally serves the data to the user via the Arrow flight protocol. Credentials are handled by the modules (11) and are never exposed to the user. The application reads from and writes data to allowed targets. Requests are handled by FybrikModule instances(12). The application can not interact with unauthorized targets.","title":"Architecture"},{"location":"concepts/architecture/#architecture","text":"Fybrik takes a modular approach to provide an open platform for controlling and securing the use of data across an organization. The figure below showcases the current architecture of the Fybrik platform, running on top of Kubernetes. The storage systems shown in the lower half of the figure are merely an example. The core parts of Fybrik are based on Kubernetes controllers and Custom Resource Definitions (CRDs) in order to define its work items and reconcile the state of them. The primary interaction object for a data user is the FybrikApplication custom resource where a user defines which data should be used for which purpose. The following chart and description describe the architecture and components of Fybrik relative to when they are used. Before the data user can perform any actions a data operator has to install Fybrik and modules. Modules describe capabilities that can be included in a data plane. These may be existing open source or third party service, or custom ones. The module of a service indicates the capabilities it supports, the formats and interfaces, and how to deploy the service. Modules may describe externally deployed services, or services deployed by fybrik. Examples of modules are those that provide read/write access or produce implicit copies that serve as lower latency caches of remote assets. Modules also may also perform actions usesd to enforce data governance policy decisions, such as masking or redaction as examples. Fybrik connects to external services to receive data governance decisions, metadata about datasets and credentials. Policies, assets and access credentials to the assets have to be defined before the user can run an application. The current abstraction supports 2 different connectors : one for data catalog and one for policy manager. It is designed in an open way so that multiple different catalog and policy frameworks of all kinds of cloud and on-prem systems can be supported. The data steward configures policies in an external policy manager over assets defined in an external data catalog. Dataset credentials are retrieved from Vault by using Vault API . Vault uses a custom secret engine implemented with HashiCorp Vault plugins system to retrieve the credentials from where they are stored (data catalog for example). Once a developer submits a FybrikApplication CRD to Kubernetes, the FybrikApplicationController will make sure that all the specs are fulfilled and will make sure that the data is read/written/copied/deleted according to the data governance policies. The FybrikApplication holds metadata about the application such as the data assets required by the application, the processing purpose and the method of access the user wishes (protocol e.g. S3 or Arrow flight). It uses this information to check with the data governance policy manager (4) if the data flow requested is allowed and whether restrictive policies such as masking or hashing have to be applied. It compiles plotters based on the governance decisions received via the data governance policy connector and chooses the modules (5) which are best fit for the requirements that the user specified regarding the access protocol and availability, and based on the config policies defined. The plotter contains the modules required and all the information required to deploy them on the cluster where they will run, as well as the flow of data between the asset and the workload through the chosen modules. As data assets may reside in different clusters/clouds a Blueprint CRD is created for each cluster, containing the information regarding the services to be deployed or configured in the given cluster.Depending on the setup the PlotterController will use various methods to distribute the blueprints. In a multi cluster setup the default distribution implementation is using Razee to control remote blueprints, but several multi-cloud tools could be used as a replacement. The PlotterController also collects statuses and distributes updates of said blueprints. Once all the blueprints on all clusters are ready the plotter is marked as ready. A single blueprint contains the specification of all assets that shall be accessed in a single cluster by a single application. The BlueprintController makes sure that a blueprint can deploy all needed modules (8) and (9) and tracks their status (10). Once e.g. an implicit-copy module finishes the copy the blueprint is also in a ready state. A read or write module is in ready state as soon as the proxy service such as the arrow-flight module is running. In this example an implicit-copy module copies data from a remote postgres database into a S3 compatible ceph instance. The arrow-flight module then locally serves the data to the user via the Arrow flight protocol. Credentials are handled by the modules (11) and are never exposed to the user. The application reads from and writes data to allowed targets. Requests are handled by FybrikModule instances(12). The application can not interact with unauthorized targets.","title":"Architecture"},{"location":"concepts/config-policies/","text":"Configuration Policies What are configuration policies? Configuration policies are the mechanism via which the organization may influence the construction of the data plane, taking into account infrastructure capabilities and costs. Fybrik takes into account the workload context, the data metadata, the data governance policies and the configuration policies when defining the data plane. The configuration policies influence what capabilities should be deployed (e.g. read, copy), in which clusters they should be deployed, and selection of the most appropriate module that implements the capability. Input to policies The input object includes general application data such as workload cluster and application properties, as well as dataset details (user requirements, metadata). Available properties: - cluster.name : name of the workload cluster - cluster.metadata.region : region of the workload cluster - properties : application/workload properties defined in FybrikApplication, e.g. properties.intent - request.metadata : asset metadata as defined in catalog taxonomy, e.g request.metadata.geography - usage : a set of boolean properties associated with data use: usage.read , usage.write , usage.copy Syntax Policies are written in rego files. Each file declares a package adminconfig . Rules are written in the following syntax: config[{\"capability\": capability, \"decision\": decision}] where capability represents a required module capability, such as \"read\", \"write\", \"transform\" and \"copy\". decision is a JSON structure that matches Decision defined above. { \"policy\": {\"ID\": <id>, \"policySetID\": <setId>, \"description\": <description>, \"version\": <version>}, \"deploy\": <\"True\", \"False\">, \"restrictions\": { \"modules\": <list of restrictions>, \"clusters\": <list of restrictions>, \"storageaccounts\": <list of restrictions>, }, } restriction restricts a property to either a set of values or a value in a given range . For example, the policy above restricts the choice of clusters and modules for a read capability by narrowing the choice of deployment clusters to the workload cluster, and restricting the module type to service. config[{\"capability\": \"read\", \"decision\": decision}] { input.request.usage == \"read\" policy := {\"ID\": \"read-location\", \"description\":\"Deploy read in the workload cluster\", \"version\": \"0.1\"} cluster_restrict := {\"property\": \"name\", \"values\": [ input.workload.cluster.name ] } module_restrict := {\"property\": \"type\", \"values\": [\"service\"]} decision := {\"policy\": policy, \"restrictions\": {\"clusters\": [cluster_restrict], \"modules\": [module_restrict]}} } policy provides policy metadata: unique ID, human-readable description, version and policySetID (see ### Policy Set ID) restrictions provides restrictions for modules , clusters and storageaccounts . Each restriction provides a list or a range of allowed values for a property of module/cluster/storageaccount object. For example, to restrict a module type to either \"service\" or \"plugin\", we'll use \"type\" as a property, and [ \"service\",\"plugin ] as a list of allowed values. Properties of a module can be found inside FybrikModule Spec. Properties of a storage account are listed inside FybrikStorageAccount . Cluster is not a custom resource. It has the following properties: - name: cluster name - metadata.region: cluster region - metadata.zone: cluster zone deploy receives \"True\"/\"False\" values. These values indicate whether the capability should or should not be deployed. If not specified in the policy, it's up to Fybrik to decide on the capability deployment. Policy Set ID Fybrik supports evaluating different sets of policies for different FybrikApplications. It is possible to define a policy for a specific policySetID which will be trigered only if it matches the policySetID defined in FybrikApplication. If a policy does not specify a policy set id, it will be considered as relevant for all FybrikApplications. In a similar way, all policies are relevant for a FybrikApplication that does not specify a policy set id, to support a use-case of a single policy set for all. Out of the box policies Out of the box policies come with the fybrik deployment. They define the deployment of basic capabilities, such as read, write, copy and delete. package adminconfig # read capability deployment config[{\"capability\": \"read\", \"decision\": decision}] { input.request.usage == \"read\" policy := {\"ID\": \"read-default-enabled\", \"description\":\"Read capability is requested for read workloads\", \"version\": \"0.1\"} decision := {\"policy\": policy, \"deploy\": \"True\"} } # write capability deployment config[{\"capability\": \"write\", \"decision\": decision}] { input.request.usage == \"write\" policy := {\"ID\": \"write-default-enabled\", \"description\":\"Write capability is requested for workloads that write data\", \"version\": \"0.1\"} decision := {\"policy\": policy, \"deploy\": \"True\"} } # copy requested by the user config[{\"capability\": \"copy\", \"decision\": decision}] { input.request.usage == \"copy\" policy := {\"ID\": \"copy-request\", \"description\":\"Copy (ingest) capability is requested by the user\", \"version\": \"0.1\"} decision := {\"policy\": policy, \"deploy\": \"True\"} } # delete capability deployment config[{\"capability\": \"delete\", \"decision\": decision}] { input.request.usage == \"delete\" policy := {\"ID\": \"delete-request\", \"description\":\"Delete capability is requested by the user\", \"version\": \"0.1\"} decision := {\"policy\": policy, \"deploy\": \"True\"} } # do not deploy copy in scenarios different from read or copy config[{\"capability\": \"copy\", \"decision\": decision}] { input.request.usage != \"read\" input.request.usage != \"copy\" policy := {\"ID\": \"copy-disabled\", \"description\":\"Copy capability is not requested\", \"version\": \"0.1\"} decision := {\"policy\": policy, \"deploy\": \"False\"} } # do not deploy read in other scenarios config[{\"capability\": \"read\", \"decision\": decision}] { input.request.usage != \"read\" policy := {\"ID\": \"read-disabled\", \"description\":\"Read capability is not requested\", \"version\": \"0.1\"} decision := {\"policy\": policy, \"deploy\": \"False\"} } # do not deploy write in other scenarios config[{\"capability\": \"write\", \"decision\": decision}] { input.request.usage != \"write\" policy := {\"ID\": \"write-disabled\", \"description\":\"Write capability is not requested\", \"version\": \"0.1\"} decision := {\"policy\": policy, \"deploy\": \"False\"} } # do not deploy delete in other scenarios config[{\"capability\": \"delete\", \"decision\": decision}] { input.request.usage != \"delete\" policy := {\"ID\": \"delete-disabled\", \"description\":\"Delete capability is not requested\", \"version\": \"0.1\"} decision := {\"policy\": policy, \"deploy\": \"False\"} } Extended policies The extended policies define advanced deployment requirements, such as where read or transform modules should run, what should be the scope of module deployments, and more. The policies below are provided as a sample and can be updated for the production deployment. package adminconfig # configure where transformations take place config[{\"capability\": \"transform\", \"decision\": decision}] { policy := {\"ID\": \"transform-geo\", \"description\":\"Governance based transformations must take place in the geography where the data is stored\", \"version\": \"0.1\"} cluster_restrict := {\"property\": \"metadata.region\", \"values\": [input.request.dataset.geography]} decision := {\"policy\": policy, \"restrictions\": {\"clusters\": [cluster_restrict]}} } # configure the scope of the read capability config[{\"capability\": \"read\", \"decision\": decision}] { input.request.usage == \"read\" policy := {\"ID\": \"read-scope\", \"description\":\"Deploy read at the workload scope\", \"version\": \"0.1\"} decision := {\"policy\": policy, \"restrictions\": {\"modules\": [{\"property\": \"capabilities.scope\", \"values\" : [\"workload\"]}]}} } # configure where the read capability will be deployed config[{\"capability\": \"read\", \"decision\": decision}] { input.request.usage == \"read\" policy := {\"ID\": \"read-location\", \"description\":\"Deploy read in the workload cluster\", \"version\": \"0.1\"} cluster_restrict := {\"property\": \"name\", \"values\": [ input.workload.cluster.name ] } decision := {\"policy\": policy, \"restrictions\": {\"clusters\": [cluster_restrict]}} } # allow implicit copies by default config[{\"capability\": \"copy\", \"decision\": decision}] { input.request.usage == \"read\" policy := {\"ID\": \"copy-default\", \"description\":\"Implicit copies are allowed in read scenarios\", \"version\": \"0.1\"} decision := {\"policy\": policy} } How to provide custom policies In order to deploy Fybrik with customized policies, perform the following steps Clone the github repository of Fybrik for the required release: git clone -b releases/<version> https://github.com/fybrik/fybrik.git Copy the rego files containing customized policies to fybrik/charts/fybrik/files/adminconfig/ folder Install Fybrik: cd fybrik helm install fybrik-crd charts/fybrik-crd -n fybrik-system --wait helm install fybrik charts/fybrik --set global.tag=master --set global.imagePullPolicy=Always -n fybrik-system --wait How to add start and/or expiry dates to policies By utilizing the time built-in functions of OPA, an effective date and/or expiry date of a policy can be defined. The related built-in functions are: output := time.now_ns() //the current date output := time.parse_rfc3339_ns(value) //the specified date in RFC3339 format parse_rfc3339_ns enables to add the expiry date as well as the the date for the policy to become effective, and now_ns captures the date when policies are applied. Through comparisons, it can be acquired whether the current policy is still valid. Below is an example. package adminconfig # vaild from 2022.1.1, expire on 2022.6.1 config[{\"capability\": \"copy\", \"decision\": decision}] { policy := {\"policySetID\": \"1\", \"ID\": \"test-1\"} nowDate := time.now_ns() startDate := time.parse_rfc3339_ns(\"2022-01-01T00:00:00Z\") expiration := time.parse_rfc3339_ns(\"2022-06-01T00:00:00Z\") nowDate >= startDate nowDate < expiration decision := {\"policy\": policy, \"deploy\": \"False\"} } Note that an empty ConfigDecisions map will be returned if the expiration date is exceeded by the time when the policy is applied. Taking infrastructure metrics into consideration When writing configuration policies, infrastructure metrics and costs may also be taken into account in order to optimize the generated data plane. For example, selection of a storage account may be based on a storage cost, selection of a cluster may provide a restriction on cluster capacity, and so on. Infrastructure attributes and metrics are stored in the /tmp/adminconfig/infrastructure.json directory of the manager pod. Collection of the metrics and their dynamic update is beyond the scope of Fybrik. One may develop or use 3rd party solutions for monitoring and updating these infrastructure metrics. How to define infrastructure attributes An infrastructure attribute is defined by a JSON object that includes the following fields: attribute - name of the infrastructure attribute, should be defined in the taxonomy description type - value type(can be numeric, string or boolean) value - the actual value of the metric units - measurement units, defined in the taxonomy object - a resource the attribute relates to (storageaccount, module, cluster) instance - a reference to the resource instance, e.g. storage account name scale - a scale of values (minimum and maximum) when applicable The infrastructure attributes are associated with resources managed by Fybrik: FybrikStorageAccount, FybrikModule and cluster (defined in the cluster-metadata config map). The valid values for the attribute object field are storageaccount , module and cluster , respectively. For example, the following attribute defines the storage cost of the \"account-theshire\" storage account. { \"attribute\": \"storage-cost\", \"description\": \"theshire object store\", \"value\": \"90\", \"type\": \"numeric\", \"units\": \"US Dollar per TB per month\", \"object\": \"storageaccount\", \"instance\": \"account-theshire\" } Add a new attribute definition to the taxonomy See https://github.com/fybrik/fybrik/blob/master/samples/taxonomy/example/infrastructure/attributepair.yaml for an example how to define an attribute and the corresponding measurement units. Usage of infrastructure attributes in policies An infrastructure attribute can be used as the property value in configuration policies. For example, the following policy restricts the storage account selection using the storage-cost infrastructure attribute: # restrict storage costs to a maximum of $95 when copying the data config[{\"capability\": \"copy\", \"decision\": decision}] { input.request.usage == \"copy\" input.request.dataset.geography != input.workload.cluster.metadata.region account_restrict := {\"property\": \"storage-cost\", \"range\": {\"max\": 95}} policy := {\"ID\": \"copy-restrict-storage\", \"description\":\"Use cheaper storage\", \"version\": \"0.1\"} decision := {\"policy\": policy, \"restrictions\": {\"storageaccounts\": [account_restrict]}} }","title":"Configuration Policies"},{"location":"concepts/config-policies/#configuration-policies","text":"","title":"Configuration Policies"},{"location":"concepts/config-policies/#what-are-configuration-policies","text":"Configuration policies are the mechanism via which the organization may influence the construction of the data plane, taking into account infrastructure capabilities and costs. Fybrik takes into account the workload context, the data metadata, the data governance policies and the configuration policies when defining the data plane. The configuration policies influence what capabilities should be deployed (e.g. read, copy), in which clusters they should be deployed, and selection of the most appropriate module that implements the capability.","title":"What are configuration policies?"},{"location":"concepts/config-policies/#input-to-policies","text":"The input object includes general application data such as workload cluster and application properties, as well as dataset details (user requirements, metadata). Available properties: - cluster.name : name of the workload cluster - cluster.metadata.region : region of the workload cluster - properties : application/workload properties defined in FybrikApplication, e.g. properties.intent - request.metadata : asset metadata as defined in catalog taxonomy, e.g request.metadata.geography - usage : a set of boolean properties associated with data use: usage.read , usage.write , usage.copy","title":"Input to policies"},{"location":"concepts/config-policies/#syntax","text":"Policies are written in rego files. Each file declares a package adminconfig . Rules are written in the following syntax: config[{\"capability\": capability, \"decision\": decision}] where capability represents a required module capability, such as \"read\", \"write\", \"transform\" and \"copy\". decision is a JSON structure that matches Decision defined above. { \"policy\": {\"ID\": <id>, \"policySetID\": <setId>, \"description\": <description>, \"version\": <version>}, \"deploy\": <\"True\", \"False\">, \"restrictions\": { \"modules\": <list of restrictions>, \"clusters\": <list of restrictions>, \"storageaccounts\": <list of restrictions>, }, } restriction restricts a property to either a set of values or a value in a given range . For example, the policy above restricts the choice of clusters and modules for a read capability by narrowing the choice of deployment clusters to the workload cluster, and restricting the module type to service. config[{\"capability\": \"read\", \"decision\": decision}] { input.request.usage == \"read\" policy := {\"ID\": \"read-location\", \"description\":\"Deploy read in the workload cluster\", \"version\": \"0.1\"} cluster_restrict := {\"property\": \"name\", \"values\": [ input.workload.cluster.name ] } module_restrict := {\"property\": \"type\", \"values\": [\"service\"]} decision := {\"policy\": policy, \"restrictions\": {\"clusters\": [cluster_restrict], \"modules\": [module_restrict]}} } policy provides policy metadata: unique ID, human-readable description, version and policySetID (see ### Policy Set ID) restrictions provides restrictions for modules , clusters and storageaccounts . Each restriction provides a list or a range of allowed values for a property of module/cluster/storageaccount object. For example, to restrict a module type to either \"service\" or \"plugin\", we'll use \"type\" as a property, and [ \"service\",\"plugin ] as a list of allowed values. Properties of a module can be found inside FybrikModule Spec. Properties of a storage account are listed inside FybrikStorageAccount . Cluster is not a custom resource. It has the following properties: - name: cluster name - metadata.region: cluster region - metadata.zone: cluster zone deploy receives \"True\"/\"False\" values. These values indicate whether the capability should or should not be deployed. If not specified in the policy, it's up to Fybrik to decide on the capability deployment.","title":"Syntax"},{"location":"concepts/config-policies/#policy-set-id","text":"Fybrik supports evaluating different sets of policies for different FybrikApplications. It is possible to define a policy for a specific policySetID which will be trigered only if it matches the policySetID defined in FybrikApplication. If a policy does not specify a policy set id, it will be considered as relevant for all FybrikApplications. In a similar way, all policies are relevant for a FybrikApplication that does not specify a policy set id, to support a use-case of a single policy set for all.","title":"Policy Set ID"},{"location":"concepts/config-policies/#out-of-the-box-policies","text":"Out of the box policies come with the fybrik deployment. They define the deployment of basic capabilities, such as read, write, copy and delete. package adminconfig # read capability deployment config[{\"capability\": \"read\", \"decision\": decision}] { input.request.usage == \"read\" policy := {\"ID\": \"read-default-enabled\", \"description\":\"Read capability is requested for read workloads\", \"version\": \"0.1\"} decision := {\"policy\": policy, \"deploy\": \"True\"} } # write capability deployment config[{\"capability\": \"write\", \"decision\": decision}] { input.request.usage == \"write\" policy := {\"ID\": \"write-default-enabled\", \"description\":\"Write capability is requested for workloads that write data\", \"version\": \"0.1\"} decision := {\"policy\": policy, \"deploy\": \"True\"} } # copy requested by the user config[{\"capability\": \"copy\", \"decision\": decision}] { input.request.usage == \"copy\" policy := {\"ID\": \"copy-request\", \"description\":\"Copy (ingest) capability is requested by the user\", \"version\": \"0.1\"} decision := {\"policy\": policy, \"deploy\": \"True\"} } # delete capability deployment config[{\"capability\": \"delete\", \"decision\": decision}] { input.request.usage == \"delete\" policy := {\"ID\": \"delete-request\", \"description\":\"Delete capability is requested by the user\", \"version\": \"0.1\"} decision := {\"policy\": policy, \"deploy\": \"True\"} } # do not deploy copy in scenarios different from read or copy config[{\"capability\": \"copy\", \"decision\": decision}] { input.request.usage != \"read\" input.request.usage != \"copy\" policy := {\"ID\": \"copy-disabled\", \"description\":\"Copy capability is not requested\", \"version\": \"0.1\"} decision := {\"policy\": policy, \"deploy\": \"False\"} } # do not deploy read in other scenarios config[{\"capability\": \"read\", \"decision\": decision}] { input.request.usage != \"read\" policy := {\"ID\": \"read-disabled\", \"description\":\"Read capability is not requested\", \"version\": \"0.1\"} decision := {\"policy\": policy, \"deploy\": \"False\"} } # do not deploy write in other scenarios config[{\"capability\": \"write\", \"decision\": decision}] { input.request.usage != \"write\" policy := {\"ID\": \"write-disabled\", \"description\":\"Write capability is not requested\", \"version\": \"0.1\"} decision := {\"policy\": policy, \"deploy\": \"False\"} } # do not deploy delete in other scenarios config[{\"capability\": \"delete\", \"decision\": decision}] { input.request.usage != \"delete\" policy := {\"ID\": \"delete-disabled\", \"description\":\"Delete capability is not requested\", \"version\": \"0.1\"} decision := {\"policy\": policy, \"deploy\": \"False\"} }","title":"Out of the box policies"},{"location":"concepts/config-policies/#extended-policies","text":"The extended policies define advanced deployment requirements, such as where read or transform modules should run, what should be the scope of module deployments, and more. The policies below are provided as a sample and can be updated for the production deployment. package adminconfig # configure where transformations take place config[{\"capability\": \"transform\", \"decision\": decision}] { policy := {\"ID\": \"transform-geo\", \"description\":\"Governance based transformations must take place in the geography where the data is stored\", \"version\": \"0.1\"} cluster_restrict := {\"property\": \"metadata.region\", \"values\": [input.request.dataset.geography]} decision := {\"policy\": policy, \"restrictions\": {\"clusters\": [cluster_restrict]}} } # configure the scope of the read capability config[{\"capability\": \"read\", \"decision\": decision}] { input.request.usage == \"read\" policy := {\"ID\": \"read-scope\", \"description\":\"Deploy read at the workload scope\", \"version\": \"0.1\"} decision := {\"policy\": policy, \"restrictions\": {\"modules\": [{\"property\": \"capabilities.scope\", \"values\" : [\"workload\"]}]}} } # configure where the read capability will be deployed config[{\"capability\": \"read\", \"decision\": decision}] { input.request.usage == \"read\" policy := {\"ID\": \"read-location\", \"description\":\"Deploy read in the workload cluster\", \"version\": \"0.1\"} cluster_restrict := {\"property\": \"name\", \"values\": [ input.workload.cluster.name ] } decision := {\"policy\": policy, \"restrictions\": {\"clusters\": [cluster_restrict]}} } # allow implicit copies by default config[{\"capability\": \"copy\", \"decision\": decision}] { input.request.usage == \"read\" policy := {\"ID\": \"copy-default\", \"description\":\"Implicit copies are allowed in read scenarios\", \"version\": \"0.1\"} decision := {\"policy\": policy} }","title":"Extended policies"},{"location":"concepts/config-policies/#how-to-provide-custom-policies","text":"In order to deploy Fybrik with customized policies, perform the following steps Clone the github repository of Fybrik for the required release: git clone -b releases/<version> https://github.com/fybrik/fybrik.git Copy the rego files containing customized policies to fybrik/charts/fybrik/files/adminconfig/ folder Install Fybrik: cd fybrik helm install fybrik-crd charts/fybrik-crd -n fybrik-system --wait helm install fybrik charts/fybrik --set global.tag=master --set global.imagePullPolicy=Always -n fybrik-system --wait","title":"How to provide custom policies"},{"location":"concepts/config-policies/#how-to-add-start-andor-expiry-dates-to-policies","text":"By utilizing the time built-in functions of OPA, an effective date and/or expiry date of a policy can be defined. The related built-in functions are: output := time.now_ns() //the current date output := time.parse_rfc3339_ns(value) //the specified date in RFC3339 format parse_rfc3339_ns enables to add the expiry date as well as the the date for the policy to become effective, and now_ns captures the date when policies are applied. Through comparisons, it can be acquired whether the current policy is still valid. Below is an example. package adminconfig # vaild from 2022.1.1, expire on 2022.6.1 config[{\"capability\": \"copy\", \"decision\": decision}] { policy := {\"policySetID\": \"1\", \"ID\": \"test-1\"} nowDate := time.now_ns() startDate := time.parse_rfc3339_ns(\"2022-01-01T00:00:00Z\") expiration := time.parse_rfc3339_ns(\"2022-06-01T00:00:00Z\") nowDate >= startDate nowDate < expiration decision := {\"policy\": policy, \"deploy\": \"False\"} } Note that an empty ConfigDecisions map will be returned if the expiration date is exceeded by the time when the policy is applied.","title":"How to add start and/or expiry dates to policies"},{"location":"concepts/config-policies/#taking-infrastructure-metrics-into-consideration","text":"When writing configuration policies, infrastructure metrics and costs may also be taken into account in order to optimize the generated data plane. For example, selection of a storage account may be based on a storage cost, selection of a cluster may provide a restriction on cluster capacity, and so on. Infrastructure attributes and metrics are stored in the /tmp/adminconfig/infrastructure.json directory of the manager pod. Collection of the metrics and their dynamic update is beyond the scope of Fybrik. One may develop or use 3rd party solutions for monitoring and updating these infrastructure metrics.","title":"Taking infrastructure metrics into consideration"},{"location":"concepts/config-policies/#how-to-define-infrastructure-attributes","text":"An infrastructure attribute is defined by a JSON object that includes the following fields: attribute - name of the infrastructure attribute, should be defined in the taxonomy description type - value type(can be numeric, string or boolean) value - the actual value of the metric units - measurement units, defined in the taxonomy object - a resource the attribute relates to (storageaccount, module, cluster) instance - a reference to the resource instance, e.g. storage account name scale - a scale of values (minimum and maximum) when applicable The infrastructure attributes are associated with resources managed by Fybrik: FybrikStorageAccount, FybrikModule and cluster (defined in the cluster-metadata config map). The valid values for the attribute object field are storageaccount , module and cluster , respectively. For example, the following attribute defines the storage cost of the \"account-theshire\" storage account. { \"attribute\": \"storage-cost\", \"description\": \"theshire object store\", \"value\": \"90\", \"type\": \"numeric\", \"units\": \"US Dollar per TB per month\", \"object\": \"storageaccount\", \"instance\": \"account-theshire\" }","title":"How to define infrastructure attributes"},{"location":"concepts/config-policies/#add-a-new-attribute-definition-to-the-taxonomy","text":"See https://github.com/fybrik/fybrik/blob/master/samples/taxonomy/example/infrastructure/attributepair.yaml for an example how to define an attribute and the corresponding measurement units.","title":"Add a new attribute definition to the taxonomy"},{"location":"concepts/config-policies/#usage-of-infrastructure-attributes-in-policies","text":"An infrastructure attribute can be used as the property value in configuration policies. For example, the following policy restricts the storage account selection using the storage-cost infrastructure attribute: # restrict storage costs to a maximum of $95 when copying the data config[{\"capability\": \"copy\", \"decision\": decision}] { input.request.usage == \"copy\" input.request.dataset.geography != input.workload.cluster.metadata.region account_restrict := {\"property\": \"storage-cost\", \"range\": {\"max\": 95}} policy := {\"ID\": \"copy-restrict-storage\", \"description\":\"Use cheaper storage\", \"version\": \"0.1\"} decision := {\"policy\": policy, \"restrictions\": {\"storageaccounts\": [account_restrict]}} }","title":"Usage of infrastructure attributes in policies"},{"location":"concepts/connectors/","text":"Connectors The project currently has two extension mechanisms, namely connectors and modules. This page describes what connectors are and what connectors are installed using the default Fybrik installation. What are connectors? Connectors are Open API services that the Fybrik control plane uses to connect to external systems. Specifically, the control plane needs connectors to a data catalog and a data governance policy manager. These connector services are deployed alongside the control plane. Can I write my own connectors? Yes. Fybrik provides some default connectors described in this page but anyone can develop their own connectors. A connector needs to implement one or more of the interfaces described in the API documentation , depending on the connector type. Note that a single Kubernetes service can implement all interfaces if the system it connects to supports the required functionality, but it can also be different services. In addition, to benefit from the control plane security feature ensure that the Pods of your connector: Have a fybrik.io/componentType: connector label Have a sidecar.istio.io/inject: \"true\" annotation Connector types Data catalog Fybrik assumes the use of an enterprise data catalog. For example, to reference a required data asset in a FybrikApplication resource, you provide a link to the asset in the catalog. The catalog provides metadata about the asset such as security tags. It also provides connection information to describe how to connect to the data source to consume the data. Fybrik uses the metadata provided by the catalog both to enable seamless connectivity to the data and as input to making data governance policy decisions. The data user is not concerned with any of it and just selects the data that it needs regardless of where the data resides. Fybrik is not a data catalog. Instead, it links to existing data catalogs using connectors. The default installation of Fybrik installs Katalog , a built-in data catalog using Kubernetes custom resources used for evaluation. A connector to ODPi Egeria is also available. Credential management The connector might need to read credentials stored in HashiCorp Vault. The parameters to login to vault and to read secret are as follows: address: Vault address authPath: Path to kubernetes auth method used to login to Vault role: connector role used to login to Vault secretPath: Path of the secret holding the credentials in Vault The parameters should be known to the connector upon startup time except from the vault secret path ( SecretPath ) which is passed as a parameter in each call to the connector usually under Credentials name. An example for Vault Login API call which uses the Vault parameters is as follows: $ curl -v -X POST <address>/<authPath> -H \"Content-Type: application/json\" --data '{\"jwt\": <connector service account token>, \"role\": <role>}' An example for Vault Read Secret API call which uses the Vault parameters is as follows: $ curl --header \"X-Vault-Token: ...\" -X GET https://<address>/<secretPath> More about kubenertes auth method and vault roles can be found in Vault documentation . Policy manager Data governance policies are defined externally in the data governance manager of choice. Enforcing data governance policies requires a Policy Decision Point (PDP) that dictates what enforcement actions need to take place. Fybrik supports a wide and extendable set of enforcement actions to perform on data read, copy, (future) write or delete. These include transformation of data, verification of the data, and various restrictions on the external activity of an application that can access the data. A PDP returns a list of enforcement actions given a set of policies and specific context about the application and the data it uses. Fybrik includes a PDP that is powered by Open Policy Agent (OPA). However, the PDP can also use external policy managers via connectors, to cover some or even all policy types.","title":"Connectors"},{"location":"concepts/connectors/#connectors","text":"The project currently has two extension mechanisms, namely connectors and modules. This page describes what connectors are and what connectors are installed using the default Fybrik installation.","title":"Connectors"},{"location":"concepts/connectors/#what-are-connectors","text":"Connectors are Open API services that the Fybrik control plane uses to connect to external systems. Specifically, the control plane needs connectors to a data catalog and a data governance policy manager. These connector services are deployed alongside the control plane.","title":"What are connectors?"},{"location":"concepts/connectors/#can-i-write-my-own-connectors","text":"Yes. Fybrik provides some default connectors described in this page but anyone can develop their own connectors. A connector needs to implement one or more of the interfaces described in the API documentation , depending on the connector type. Note that a single Kubernetes service can implement all interfaces if the system it connects to supports the required functionality, but it can also be different services. In addition, to benefit from the control plane security feature ensure that the Pods of your connector: Have a fybrik.io/componentType: connector label Have a sidecar.istio.io/inject: \"true\" annotation","title":"Can I write my own connectors?"},{"location":"concepts/connectors/#connector-types","text":"","title":"Connector types"},{"location":"concepts/connectors/#data-catalog","text":"Fybrik assumes the use of an enterprise data catalog. For example, to reference a required data asset in a FybrikApplication resource, you provide a link to the asset in the catalog. The catalog provides metadata about the asset such as security tags. It also provides connection information to describe how to connect to the data source to consume the data. Fybrik uses the metadata provided by the catalog both to enable seamless connectivity to the data and as input to making data governance policy decisions. The data user is not concerned with any of it and just selects the data that it needs regardless of where the data resides. Fybrik is not a data catalog. Instead, it links to existing data catalogs using connectors. The default installation of Fybrik installs Katalog , a built-in data catalog using Kubernetes custom resources used for evaluation. A connector to ODPi Egeria is also available.","title":"Data catalog"},{"location":"concepts/connectors/#credential-management","text":"The connector might need to read credentials stored in HashiCorp Vault. The parameters to login to vault and to read secret are as follows: address: Vault address authPath: Path to kubernetes auth method used to login to Vault role: connector role used to login to Vault secretPath: Path of the secret holding the credentials in Vault The parameters should be known to the connector upon startup time except from the vault secret path ( SecretPath ) which is passed as a parameter in each call to the connector usually under Credentials name. An example for Vault Login API call which uses the Vault parameters is as follows: $ curl -v -X POST <address>/<authPath> -H \"Content-Type: application/json\" --data '{\"jwt\": <connector service account token>, \"role\": <role>}' An example for Vault Read Secret API call which uses the Vault parameters is as follows: $ curl --header \"X-Vault-Token: ...\" -X GET https://<address>/<secretPath> More about kubenertes auth method and vault roles can be found in Vault documentation .","title":"Credential management"},{"location":"concepts/connectors/#policy-manager","text":"Data governance policies are defined externally in the data governance manager of choice. Enforcing data governance policies requires a Policy Decision Point (PDP) that dictates what enforcement actions need to take place. Fybrik supports a wide and extendable set of enforcement actions to perform on data read, copy, (future) write or delete. These include transformation of data, verification of the data, and various restrictions on the external activity of an application that can access the data. A PDP returns a list of enforcement actions given a set of policies and specific context about the application and the data it uses. Fybrik includes a PDP that is powered by Open Policy Agent (OPA). However, the PDP can also use external policy managers via connectors, to cover some or even all policy types.","title":"Policy manager"},{"location":"concepts/introduction/","text":"Introduction Fybrik is a cloud native platform to unify data access and governance, enabling business agility while securing enterprise data. By providing access and use of data only via the platform, Fybrik brings together access and governance for data, greatly reducing risk of data loss. Fybrik allows: Data users to use data in a self-service model without manual processes, without needing to confer with data stewards, and without dealing with credentials. Use common tools and frameworks for reading from and exporting data to data lakes or data warehouses. Data stewards to control data usage by applications. Use the organization's policy manager and data catalog of choice and let Fybrik automatically enforce data governance policies, whether they be based on laws, industry standards or enterprise policies. Data operators to automate data lifecycle management removing the need for manual processes and custom jobs created by data operators, providing them with config policies to optimize the data flows orchestrated by fybrik. How does it work? The inputs to Fybrik are declarative definitions with separation of aspects: Data stewards input definitions related to data governance and security Data users input definitions related to data usage in the business logic of their applications Data operators input definitions related to infrastructure and available resources Upon creation or change of any definition, Fybrik compiles together relevant inputs into a plotter describing the flow of data between the application and the data sources/destinations (data plane). The plotter augments the application workload and data sources with additional services and functions packed as pluggable modules. This creates a data path that: Integrates business logic with non-functional data centric requirements such as enabling data access regardless of its physical location, caching, lineage tracking, etc. Enforces governance relating to the data and its lifecycle; including limiting what data the business logic can access, performing transformations as needed, controlling what the business logic can export and to where Makes data available in locations where it is needed. Thus, in a multi cluster scenario it may copy data from one location to another, something known as an implicit copy. The implicit copy is deleted when no longer needed. Fybrik is an open solution that can be extended to work with a wide range of tools and data stores. For example, the injectable modules and the connectors to external systems (e.g., to a data catalog) can all be third party. The logic used by fybrik to generate the data planes is customizable. An organization can determine how best its infrastructure should be leveraged via config policies Applications Fybrik considers applications as first level entities. Before running a workload, an application needs to be registered to a Fybrik control plane by applying a FybrikApplication resource. This is the declarative definition provided by the data user. The registration provides context about the application such as the purpose for which it's running, the data assets that it accesses, and a selector to identify the workload. Additional context such as geo-location is extracted from the platform. The actions taken by Fybrik are based on policies and the context of the application. Specifically, Fybrik does not consider end-users of an application. It is the responsibility of the application to implement mechanisms such as end user authentication if required, e.g. using Istio authorization with JWT . There are specific situations in which there is no workload associated with a FybrikApplication resource. Examples of these are requests to ingest data into a governed environment, or (future) requests to clean up data in the governed environment based on data governance policies. Security While Fybrik handles enforcement of data governance policies, if one could access the data not through the platform then we lose control over data usage. For this reason, Fybrik does not let user applications ever observe data access credentials, neither for externally created data assets nor for data assets created by the Fybrik control plane and applications running in it. Instead, modules run in the data path to handle access to data, including passing the data access credentials to upstream data stores. Security is preserved by authorizing the applications based on their Pod identities. Multicluster Fybrik supports data paths that access data stores that are external to the cluster such as cloud managed object stores or databases as well as data stores within the cluster such as databases running in Kubernetes. All applications and modules however will run within a cluster that has Fybrik installed. Multi-cloud and hybrid cloud scenarios are supported out of the box by running Fybrik in multiple Kubernetes clusters and configuring the manager to use a multi-cluster coordination mechanism such as Razee. This enables cases such as running, for example, transformations on-prem while creating an implicit copy of an on-prem SoR table to a public cloud storage system.","title":"Introduction"},{"location":"concepts/introduction/#introduction","text":"Fybrik is a cloud native platform to unify data access and governance, enabling business agility while securing enterprise data. By providing access and use of data only via the platform, Fybrik brings together access and governance for data, greatly reducing risk of data loss. Fybrik allows: Data users to use data in a self-service model without manual processes, without needing to confer with data stewards, and without dealing with credentials. Use common tools and frameworks for reading from and exporting data to data lakes or data warehouses. Data stewards to control data usage by applications. Use the organization's policy manager and data catalog of choice and let Fybrik automatically enforce data governance policies, whether they be based on laws, industry standards or enterprise policies. Data operators to automate data lifecycle management removing the need for manual processes and custom jobs created by data operators, providing them with config policies to optimize the data flows orchestrated by fybrik.","title":"Introduction"},{"location":"concepts/introduction/#how-does-it-work","text":"The inputs to Fybrik are declarative definitions with separation of aspects: Data stewards input definitions related to data governance and security Data users input definitions related to data usage in the business logic of their applications Data operators input definitions related to infrastructure and available resources Upon creation or change of any definition, Fybrik compiles together relevant inputs into a plotter describing the flow of data between the application and the data sources/destinations (data plane). The plotter augments the application workload and data sources with additional services and functions packed as pluggable modules. This creates a data path that: Integrates business logic with non-functional data centric requirements such as enabling data access regardless of its physical location, caching, lineage tracking, etc. Enforces governance relating to the data and its lifecycle; including limiting what data the business logic can access, performing transformations as needed, controlling what the business logic can export and to where Makes data available in locations where it is needed. Thus, in a multi cluster scenario it may copy data from one location to another, something known as an implicit copy. The implicit copy is deleted when no longer needed. Fybrik is an open solution that can be extended to work with a wide range of tools and data stores. For example, the injectable modules and the connectors to external systems (e.g., to a data catalog) can all be third party. The logic used by fybrik to generate the data planes is customizable. An organization can determine how best its infrastructure should be leveraged via config policies","title":"How does it work?"},{"location":"concepts/introduction/#applications","text":"Fybrik considers applications as first level entities. Before running a workload, an application needs to be registered to a Fybrik control plane by applying a FybrikApplication resource. This is the declarative definition provided by the data user. The registration provides context about the application such as the purpose for which it's running, the data assets that it accesses, and a selector to identify the workload. Additional context such as geo-location is extracted from the platform. The actions taken by Fybrik are based on policies and the context of the application. Specifically, Fybrik does not consider end-users of an application. It is the responsibility of the application to implement mechanisms such as end user authentication if required, e.g. using Istio authorization with JWT . There are specific situations in which there is no workload associated with a FybrikApplication resource. Examples of these are requests to ingest data into a governed environment, or (future) requests to clean up data in the governed environment based on data governance policies.","title":"Applications"},{"location":"concepts/introduction/#security","text":"While Fybrik handles enforcement of data governance policies, if one could access the data not through the platform then we lose control over data usage. For this reason, Fybrik does not let user applications ever observe data access credentials, neither for externally created data assets nor for data assets created by the Fybrik control plane and applications running in it. Instead, modules run in the data path to handle access to data, including passing the data access credentials to upstream data stores. Security is preserved by authorizing the applications based on their Pod identities.","title":"Security"},{"location":"concepts/introduction/#multicluster","text":"Fybrik supports data paths that access data stores that are external to the cluster such as cloud managed object stores or databases as well as data stores within the cluster such as databases running in Kubernetes. All applications and modules however will run within a cluster that has Fybrik installed. Multi-cloud and hybrid cloud scenarios are supported out of the box by running Fybrik in multiple Kubernetes clusters and configuring the manager to use a multi-cluster coordination mechanism such as Razee. This enables cases such as running, for example, transformations on-prem while creating an implicit copy of an on-prem SoR table to a public cloud storage system.","title":"Multicluster"},{"location":"concepts/modules/","text":"Modules The project currently has two extension mechanisms, namely connectors and modules. This page describes what modules are and how they are leveraged by the control plane to build the data plane flow. What are modules? As described in the architecture page, the control plane generates a description of a data plane based on policies and application requirements. This is known as a blueprint, and includes components that are deployed by the control plane to fulfill different data-centric requirements. For example, a component that can mask data can be used to enforce a data masking policy, or a component that copies data may be used to create a local data copy to meet performance requirements, etc. Modules are the way to describe such data plane components and make them available to the control plane. A module is packaged as a Helm chart that the control plane can install to a workload's data plane. To make a module available to the control plane it must be registered by applying a FybrikModule custom resource. The functionality described by the module may be deployed (a) per workload, or (b) it may be composed of one or more components that run independent of the workload and its associated control plane. In the case of (a), the control plane handles the deployment of the functional component. In the case of (b) where the functionality of the module runs independently and handles requests from multiple workloads, a client module is what is deployed by the control plane. This client module passes parameters to the external component(s) and monitors the status and results of the requests to the external component(s). The following diagram shows an example with an Arrow Flight module that is fully deployed by the control plane and a second module where the client is deployed by the control plane but the ETL component providing the functionality has been independently deployed and supports multiple workloads. Components that make up a module There are several parts to a module: Optional external component(s): deployed and managed independently of Fybrik. Module Workload : the workload that runs once the Helm chart is installed by the control plane. Can be a client to the external component(s) or be independent. Module Helm Chart : the package containing the module workload that the control plane installs as part of a data plane. FybrikModule YAML : describes the functional capabilities, supported interfaces, and has links to the Module Helm chart. Registering a module To make the control plane aware of the module so that it can be included in appropriate workload data flows, the administrator must apply the FybrikModule YAML in the fybrik-system namespace. This makes the control plane aware of the existence of the module. Note that it does not check that the module's helm chart exists. For example, the following registers the arrow-flight-module : kubectl apply -f https://raw.githubusercontent.com/fybrik/arrow-flight-module/master/module.yaml -n fybrik-system When is a module used? There are three main data flows in which modules may be used: * Read - preparing data to be read and/or actually reading the data * Write - writing a new data set or appending data to an existing data set * Copy - for performing an implicit data copy on behalf of the application. The decision to do an implicit copy is made by the control plane, typically for performance or governance reasons. A module may be used in one or more of these flows, as is indicated in the module's yaml file. Control plane choice of modules A user workload description FybrikApplicaton includes a list of the data sets required, the technologies that will be used to access them, the access type (e.g. read, copy), information about the location and reason for the use of the data. This information together with input from data and enterprise policies , determine which modules are chosen by the control plane and where they are deployed. Available modules The table below lists the currently available modules: Name Description FybrikModule Prerequisite arrow-flight-module reading and writing datasets while performing data transformations https://raw.githubusercontent.com/fybrik/arrow-flight-module/master/module.yaml airbyte-module reading datasets from data sources supported by the Airbyte tool https://raw.githubusercontent.com/fybrik/airbyte-module/main/module.yaml delete-module deletes s3 objects https://raw.githubusercontent.com/fybrik/delete-module/main/module.yaml implicit-copy copies data between any two supported data stores, for example S3 and Kafka, and applies transformations. https://raw.githubusercontent.com/fybrik/data-movement-operator/master/modules/implicit-copy-batch-module.yaml https://raw.githubusercontent.com/fybrik/data-movement-operator/master/modules/implicit-copy-stream-module.yaml - Datashim deployment. - FybrikStorageAccount resource deployed in the control plane namespace to hold the details of the storage which is used by the module for coping the data. Contributing Read Module Development for details on the components that make up a module and how to create a module.","title":"Modules"},{"location":"concepts/modules/#modules","text":"The project currently has two extension mechanisms, namely connectors and modules. This page describes what modules are and how they are leveraged by the control plane to build the data plane flow.","title":"Modules"},{"location":"concepts/modules/#what-are-modules","text":"As described in the architecture page, the control plane generates a description of a data plane based on policies and application requirements. This is known as a blueprint, and includes components that are deployed by the control plane to fulfill different data-centric requirements. For example, a component that can mask data can be used to enforce a data masking policy, or a component that copies data may be used to create a local data copy to meet performance requirements, etc. Modules are the way to describe such data plane components and make them available to the control plane. A module is packaged as a Helm chart that the control plane can install to a workload's data plane. To make a module available to the control plane it must be registered by applying a FybrikModule custom resource. The functionality described by the module may be deployed (a) per workload, or (b) it may be composed of one or more components that run independent of the workload and its associated control plane. In the case of (a), the control plane handles the deployment of the functional component. In the case of (b) where the functionality of the module runs independently and handles requests from multiple workloads, a client module is what is deployed by the control plane. This client module passes parameters to the external component(s) and monitors the status and results of the requests to the external component(s). The following diagram shows an example with an Arrow Flight module that is fully deployed by the control plane and a second module where the client is deployed by the control plane but the ETL component providing the functionality has been independently deployed and supports multiple workloads.","title":"What are modules?"},{"location":"concepts/modules/#components-that-make-up-a-module","text":"There are several parts to a module: Optional external component(s): deployed and managed independently of Fybrik. Module Workload : the workload that runs once the Helm chart is installed by the control plane. Can be a client to the external component(s) or be independent. Module Helm Chart : the package containing the module workload that the control plane installs as part of a data plane. FybrikModule YAML : describes the functional capabilities, supported interfaces, and has links to the Module Helm chart.","title":"Components that make up a module"},{"location":"concepts/modules/#registering-a-module","text":"To make the control plane aware of the module so that it can be included in appropriate workload data flows, the administrator must apply the FybrikModule YAML in the fybrik-system namespace. This makes the control plane aware of the existence of the module. Note that it does not check that the module's helm chart exists. For example, the following registers the arrow-flight-module : kubectl apply -f https://raw.githubusercontent.com/fybrik/arrow-flight-module/master/module.yaml -n fybrik-system","title":"Registering a module"},{"location":"concepts/modules/#when-is-a-module-used","text":"There are three main data flows in which modules may be used: * Read - preparing data to be read and/or actually reading the data * Write - writing a new data set or appending data to an existing data set * Copy - for performing an implicit data copy on behalf of the application. The decision to do an implicit copy is made by the control plane, typically for performance or governance reasons. A module may be used in one or more of these flows, as is indicated in the module's yaml file.","title":"When is a module used?"},{"location":"concepts/modules/#control-plane-choice-of-modules","text":"A user workload description FybrikApplicaton includes a list of the data sets required, the technologies that will be used to access them, the access type (e.g. read, copy), information about the location and reason for the use of the data. This information together with input from data and enterprise policies , determine which modules are chosen by the control plane and where they are deployed.","title":"Control plane choice of modules"},{"location":"concepts/modules/#available-modules","text":"The table below lists the currently available modules: Name Description FybrikModule Prerequisite arrow-flight-module reading and writing datasets while performing data transformations https://raw.githubusercontent.com/fybrik/arrow-flight-module/master/module.yaml airbyte-module reading datasets from data sources supported by the Airbyte tool https://raw.githubusercontent.com/fybrik/airbyte-module/main/module.yaml delete-module deletes s3 objects https://raw.githubusercontent.com/fybrik/delete-module/main/module.yaml implicit-copy copies data between any two supported data stores, for example S3 and Kafka, and applies transformations. https://raw.githubusercontent.com/fybrik/data-movement-operator/master/modules/implicit-copy-batch-module.yaml https://raw.githubusercontent.com/fybrik/data-movement-operator/master/modules/implicit-copy-stream-module.yaml - Datashim deployment. - FybrikStorageAccount resource deployed in the control plane namespace to hold the details of the storage which is used by the module for coping the data.","title":"Available modules"},{"location":"concepts/modules/#contributing","text":"Read Module Development for details on the components that make up a module and how to create a module.","title":"Contributing"},{"location":"concepts/taxonomy/","text":"Taxonomy Fybrik interacts with multiple external components, such as the data catalog, data governance policy manager, and modules. In order for fybrik to orchestrate the data plane of a given workload it is essential that all the components involved use common terms. For example, if a data governance policy refers to a particular transform it is crucial that the module implementing that transform refer to it in the same way, or for fybrik to be able to map between the disparate terms. A taxonomy defines the terms and related values that need to be commonly understood and supported across the components in the system: FybrikApplication yaml - information provided about the workload and the datasets Fybrik manager (FybrikApplication controller) Data catalog Data Governance Policy Manager Config Policy Manager FybrikModules Issues Addressed by Taxonomy The taxonomy addresses the following: Redundancy: No need for the same structures and values to be hardcoded in multiple places, such as in the fybrik manager and in the plugins. Validation: Validates structures and values passed between components. Dynamic Updates: New terms and new values can be added, removed and updated dynamically. For example, one can add new enforcement actions, new connection types, new purposes, etc without needing to redeploy fybrik. Taxonomy Contributors Different actors and components define the contents of different parts of the taxonomy. The following table describes the taxonomy and which component most logically owns each part of it. Taxonomy Contributing Component Actor Example Values catalog Data Catalog Data Steward data stores, formats, metadata application Policy Manager, Data Catalog Governance Officer roles, intents module Modules Module Developer capabilities, transforms If, for example, a Data Governance Officer writes a policy that limits the use of sensitive data for marketing, then the possible valid intents such as marketing would be defined by him in the Data Policy Manager. These values must be added to fybrik's taxonomy, either manually or via an automated feed, so that fybrik can validate the intent provided in a FybrikApplication yaml when a user's workload requests data. As new capabilities, transforms, data types, and protocols are made available via FybrikModules fybrik's module taxonomy must be updated. Once updated these capabilities are available for use by other components, such as the Data Catalog and Data Governance Policy manager should they choose to leverage them. Default taxonomies are provided by fybrik, and are meant as a starting point on which to expand. Validation Points Fybrik validates the structures and values it receives from all external components. For interface components (FybrikApplication and FybrikModule), validation occurs when the resource is created, updated or deleted. How validation errors are received depends on whether fybrik is deployed with webhooks or not. If webhooks are deployed, errors are received from the kubernetes command (ex: kubectl apply ) and no resource is created. If webhooks are not deployed, validation is done in the resource's controller. If there is an error, the resource is created but its status will contain the error. (Note: These resources will need to manually be removed by the person creating them.) Summary The taxonomy mechanism enables independent components to work together, without the need for version updates and redeployment as capabilities change.","title":"Taxonomy"},{"location":"concepts/taxonomy/#taxonomy","text":"Fybrik interacts with multiple external components, such as the data catalog, data governance policy manager, and modules. In order for fybrik to orchestrate the data plane of a given workload it is essential that all the components involved use common terms. For example, if a data governance policy refers to a particular transform it is crucial that the module implementing that transform refer to it in the same way, or for fybrik to be able to map between the disparate terms. A taxonomy defines the terms and related values that need to be commonly understood and supported across the components in the system: FybrikApplication yaml - information provided about the workload and the datasets Fybrik manager (FybrikApplication controller) Data catalog Data Governance Policy Manager Config Policy Manager FybrikModules","title":"Taxonomy"},{"location":"concepts/taxonomy/#issues-addressed-by-taxonomy","text":"The taxonomy addresses the following: Redundancy: No need for the same structures and values to be hardcoded in multiple places, such as in the fybrik manager and in the plugins. Validation: Validates structures and values passed between components. Dynamic Updates: New terms and new values can be added, removed and updated dynamically. For example, one can add new enforcement actions, new connection types, new purposes, etc without needing to redeploy fybrik.","title":"Issues Addressed by Taxonomy"},{"location":"concepts/taxonomy/#taxonomy-contributors","text":"Different actors and components define the contents of different parts of the taxonomy. The following table describes the taxonomy and which component most logically owns each part of it. Taxonomy Contributing Component Actor Example Values catalog Data Catalog Data Steward data stores, formats, metadata application Policy Manager, Data Catalog Governance Officer roles, intents module Modules Module Developer capabilities, transforms If, for example, a Data Governance Officer writes a policy that limits the use of sensitive data for marketing, then the possible valid intents such as marketing would be defined by him in the Data Policy Manager. These values must be added to fybrik's taxonomy, either manually or via an automated feed, so that fybrik can validate the intent provided in a FybrikApplication yaml when a user's workload requests data. As new capabilities, transforms, data types, and protocols are made available via FybrikModules fybrik's module taxonomy must be updated. Once updated these capabilities are available for use by other components, such as the Data Catalog and Data Governance Policy manager should they choose to leverage them. Default taxonomies are provided by fybrik, and are meant as a starting point on which to expand.","title":"Taxonomy Contributors"},{"location":"concepts/taxonomy/#validation-points","text":"Fybrik validates the structures and values it receives from all external components. For interface components (FybrikApplication and FybrikModule), validation occurs when the resource is created, updated or deleted. How validation errors are received depends on whether fybrik is deployed with webhooks or not. If webhooks are deployed, errors are received from the kubernetes command (ex: kubectl apply ) and no resource is created. If webhooks are not deployed, validation is done in the resource's controller. If there is an error, the resource is created but its status will contain the error. (Note: These resources will need to manually be removed by the person creating them.)","title":"Validation Points"},{"location":"concepts/taxonomy/#summary","text":"The taxonomy mechanism enables independent components to work together, without the need for version updates and redeployment as capabilities change.","title":"Summary"},{"location":"concepts/vault_plugins/","text":"HashiCorp Vault plugins HashiCorp Vault plugins are standalone applications that Vault server executes to enable third-party secret engines and auth methods. After their enablement during Vault server initialization, the plugins can be used as a regular auth or secrets backends. This project uses secrets plugins to retrieve dataset credentials by the running modules . The plugins retrieve the credentials from where they are stored, for example, data catalog or in kubernetes secret. Vault-plugin-secrets-kubernetes-reader plugin is an example of Vault custom secret plugin which retrieves dataset credentials stored in a kubernetes secret. Additional secret plugins can be developed to retrieve credentials additional location. This tutorial can serve as a good starting point to learn about Vault plugin development. Before you begin Ensure that you have the Vault v1.9.x to execute Vault CLI commands. The following steps are for configuring a new secret plug-in for Fybrik: Login into Vault Enable the plugin during Vault server initialization in a specific path. An example of that can be found in helm chart values.yaml file in the project where Vault-plugin-secrets-kubernetes-reader plugin is enabled in kubernetes-secrets path: vault secrets enable -path = kubernetes-secrets vault-plugin-secrets-kubernetes-reader Add Vault policy to allow the modules to access secrets using the plugin. Following is an example of a policy which gives permission to read secrets in Vault path kubernetes-secrets : vault policy write \"allow-all-dataset-creds\" - <<EOF path \"kubernetes-secrets/*\" { capabilities = [\"read\"] } EOF Have the CatalogDatasetInfo structure from the data catalog response contain the Vault secret path which should be used to retrieve the credentials for a given asset. When the Vault plugin is used to retrieve the credentials the parameters to the plugin should follow the plugin usage instructions. This path will later be passed on to the modules . For example, when the credentials are stored in kubernetes secret as is done in the Katalog built-in data catalog; the Vault-plugin-secrets-kubernetes-reader plugin can be used to retrieve the credentials. In this case two parameters should be passed: paysim-csv which is the kubernetes secret name that holds the credentials and fybrik-notebook-sample is the secret namespace, both are known to the katalog when constructing the path. The following snippet shows CatalogDatasetInfo structure with Vault secret path in CredentialsInfo field. connectors.CatalogDatasetInfo { DatasetId: fybrik-notebook-sample/paysim-csv, Details: & connectors.DatasetDetails { Name: fybrik-notebook-sample/paysim-csv, Geo: theshire, DataStore: fybrik-notebook-sample/paysim-csv, CredentialsInfo: & connectors.CredentialsInfo { VaultSecretPath: \"/v1/kubernetes-secrets/paysim-csv?namespace=fybrik-notebook-sample\" } , } , } Update the modules to use the Vault related values to retrieve dataset credentias during their runtime execution. The values contain secretPath field with the plugin path as described in the previous step. The following snippet, taken from hello-world-module values.yaml file, contains an example of such values. vault: # Address is Vault address address: http://vault.fybrik-system:8200 # AuthPath is the path to auth method used to login to Vault authPath: /v1/auth/kubernetes/login # Role is the Vault role used for retrieving the credentials role: module # SecretPath is the path of the secret holding the Credentials in Vault secretPath: /v1/kubernetes-secrets/paysim-csv?namespace = fybrik-notebook-sample","title":"HashiCorp Vault plugins"},{"location":"concepts/vault_plugins/#hashicorp-vault-plugins","text":"HashiCorp Vault plugins are standalone applications that Vault server executes to enable third-party secret engines and auth methods. After their enablement during Vault server initialization, the plugins can be used as a regular auth or secrets backends. This project uses secrets plugins to retrieve dataset credentials by the running modules . The plugins retrieve the credentials from where they are stored, for example, data catalog or in kubernetes secret. Vault-plugin-secrets-kubernetes-reader plugin is an example of Vault custom secret plugin which retrieves dataset credentials stored in a kubernetes secret. Additional secret plugins can be developed to retrieve credentials additional location. This tutorial can serve as a good starting point to learn about Vault plugin development.","title":"HashiCorp Vault plugins"},{"location":"concepts/vault_plugins/#before-you-begin","text":"Ensure that you have the Vault v1.9.x to execute Vault CLI commands. The following steps are for configuring a new secret plug-in for Fybrik: Login into Vault Enable the plugin during Vault server initialization in a specific path. An example of that can be found in helm chart values.yaml file in the project where Vault-plugin-secrets-kubernetes-reader plugin is enabled in kubernetes-secrets path: vault secrets enable -path = kubernetes-secrets vault-plugin-secrets-kubernetes-reader Add Vault policy to allow the modules to access secrets using the plugin. Following is an example of a policy which gives permission to read secrets in Vault path kubernetes-secrets : vault policy write \"allow-all-dataset-creds\" - <<EOF path \"kubernetes-secrets/*\" { capabilities = [\"read\"] } EOF Have the CatalogDatasetInfo structure from the data catalog response contain the Vault secret path which should be used to retrieve the credentials for a given asset. When the Vault plugin is used to retrieve the credentials the parameters to the plugin should follow the plugin usage instructions. This path will later be passed on to the modules . For example, when the credentials are stored in kubernetes secret as is done in the Katalog built-in data catalog; the Vault-plugin-secrets-kubernetes-reader plugin can be used to retrieve the credentials. In this case two parameters should be passed: paysim-csv which is the kubernetes secret name that holds the credentials and fybrik-notebook-sample is the secret namespace, both are known to the katalog when constructing the path. The following snippet shows CatalogDatasetInfo structure with Vault secret path in CredentialsInfo field. connectors.CatalogDatasetInfo { DatasetId: fybrik-notebook-sample/paysim-csv, Details: & connectors.DatasetDetails { Name: fybrik-notebook-sample/paysim-csv, Geo: theshire, DataStore: fybrik-notebook-sample/paysim-csv, CredentialsInfo: & connectors.CredentialsInfo { VaultSecretPath: \"/v1/kubernetes-secrets/paysim-csv?namespace=fybrik-notebook-sample\" } , } , } Update the modules to use the Vault related values to retrieve dataset credentias during their runtime execution. The values contain secretPath field with the plugin path as described in the previous step. The following snippet, taken from hello-world-module values.yaml file, contains an example of such values. vault: # Address is Vault address address: http://vault.fybrik-system:8200 # AuthPath is the path to auth method used to login to Vault authPath: /v1/auth/kubernetes/login # Role is the Vault role used for retrieving the credentials role: module # SecretPath is the path of the secret holding the Credentials in Vault secretPath: /v1/kubernetes-secrets/paysim-csv?namespace = fybrik-notebook-sample","title":"Before you begin"},{"location":"contribute/","text":"Contribute Fybrik is open for contributions and welcomes anyone who wishes to contribute and take part in our journey towards success. This section contains information and guidelines to help you contribute more easily to the project. We would love for you to get involved Join our community in GitHub Discussions","title":"About"},{"location":"contribute/#contribute","text":"Fybrik is open for contributions and welcomes anyone who wishes to contribute and take part in our journey towards success. This section contains information and guidelines to help you contribute more easily to the project. We would love for you to get involved Join our community in GitHub Discussions","title":"Contribute"},{"location":"contribute/build-test/","text":"Build and Test Build the project images make docker-build Run unit tests make test Some tests for controllers are written in a fashion that they can be run on a simulated environment using envtest or on an already existing Kubernetes cluster (or local kind cluster). The default is to use envtest. In order to run the tests in a local cluster the following environment variables can be set: NO_SIMULATED_PROGRESS = true USE_EXISTING_CLUSTER = true make -C manager test Please be aware that the controller is running locally in this case! If a controller is already deployed onto the cluster then the tests can be run with the command below. This will ensure that the tests are only creating custom resources on the cluster and checking their status: USE_EXISTING_CONTROLLER = true NO_SIMULATED_PROGRESS = true USE_EXISTING_CLUSTER = true make -C manager test Environment variables description Environment variable Default Description USE_EXISTING_CLUSTER false This variable controls if an existing K8s cluster should be used or not. If not envtest will spin up an artificial environment that includes a local etcd setup. NO_SIMULATED_PROGRESS false This variable can be used by tests that can manually simulate progress of e.g. jobs or pods. e.g. the simulated test environment from testEnv does not progress pods etc while when testing against an external Kubernetes cluster this will actually run pods. USE_EXISTING_CONTROLLER false This variable controls if a controller should be set up and run by this test suite or if an external one should be used. E.g. in integration tests running against an existing setup a controller is already existing in the Kubernetes cluster and should not be started by the test as two controllers competing may influence the test. Running integration tests With the following you will then setup a kind cluster with the local registry, build and push current docker images and finally run the integration tests on it: make run-integration-tests It is sometimes useful to call the integration test commands step by step, e.g., if you want to only repeat a specific step which failed without having to rerun the entire sequence. You can find the commands of the run-integration-tests target in the Makefile . You can run make kind-cleanup to delete the created clusters when you're done. Building in a multi cluster environment As Fybrik can run in a multi-cluster environment there is also a test environment that can be used that simulates this scenario. Using kind one can spin up two separate kubernetes clusters with different contexts and develop and test in these. Two kind clusters that share the same kind-registry can be set up using: make kind-setup-multi You can run make kind-cleanup to delete the created clusters when you're done.","title":"Build and Test"},{"location":"contribute/build-test/#build-and-test","text":"","title":"Build and Test"},{"location":"contribute/build-test/#build-the-project-images","text":"make docker-build","title":"Build the project images"},{"location":"contribute/build-test/#run-unit-tests","text":"make test Some tests for controllers are written in a fashion that they can be run on a simulated environment using envtest or on an already existing Kubernetes cluster (or local kind cluster). The default is to use envtest. In order to run the tests in a local cluster the following environment variables can be set: NO_SIMULATED_PROGRESS = true USE_EXISTING_CLUSTER = true make -C manager test Please be aware that the controller is running locally in this case! If a controller is already deployed onto the cluster then the tests can be run with the command below. This will ensure that the tests are only creating custom resources on the cluster and checking their status: USE_EXISTING_CONTROLLER = true NO_SIMULATED_PROGRESS = true USE_EXISTING_CLUSTER = true make -C manager test","title":"Run unit tests"},{"location":"contribute/build-test/#environment-variables-description","text":"Environment variable Default Description USE_EXISTING_CLUSTER false This variable controls if an existing K8s cluster should be used or not. If not envtest will spin up an artificial environment that includes a local etcd setup. NO_SIMULATED_PROGRESS false This variable can be used by tests that can manually simulate progress of e.g. jobs or pods. e.g. the simulated test environment from testEnv does not progress pods etc while when testing against an external Kubernetes cluster this will actually run pods. USE_EXISTING_CONTROLLER false This variable controls if a controller should be set up and run by this test suite or if an external one should be used. E.g. in integration tests running against an existing setup a controller is already existing in the Kubernetes cluster and should not be started by the test as two controllers competing may influence the test.","title":"Environment variables description"},{"location":"contribute/build-test/#running-integration-tests","text":"With the following you will then setup a kind cluster with the local registry, build and push current docker images and finally run the integration tests on it: make run-integration-tests It is sometimes useful to call the integration test commands step by step, e.g., if you want to only repeat a specific step which failed without having to rerun the entire sequence. You can find the commands of the run-integration-tests target in the Makefile . You can run make kind-cleanup to delete the created clusters when you're done.","title":"Running integration tests"},{"location":"contribute/build-test/#building-in-a-multi-cluster-environment","text":"As Fybrik can run in a multi-cluster environment there is also a test environment that can be used that simulates this scenario. Using kind one can spin up two separate kubernetes clusters with different contexts and develop and test in these. Two kind clusters that share the same kind-registry can be set up using: make kind-setup-multi You can run make kind-cleanup to delete the created clusters when you're done.","title":"Building in a multi cluster environment"},{"location":"contribute/environment/","text":"Development Environment This page describes what you need to install as a developer and contributor to this project, for setting up a development environment. Operating system Linux and Mac OS operating systems are officially supported. Windows users should consider using Windows Subsystem for Linux 2 (WSL 2), a remote Linux machine, or any other solution such as a virtual machine. Dependencies Install the following on your machine: go 1.17 Docker make jq unzip Mac only : brew install coreutils (installs the timeout command) Then, run the following command to install additional dependencies: make install-tools This installs additional dependencies to hack/tools/bin . The make targets (e.g., make test ) are configured to use the binaries from hack/tools/bin . However, you may want to add some of these tools to your system PATH for direct usage from your terminal (e.g., for using kubectl ). Please note: For fybrik version 0.5.x and lower, Helm version greater than 3.3 but less than 3.7 is required when contributing. On the other side, for fybrik v0.6.x, Helm v3.7 or above is required. Editors The project is predominantly written in Go, so we recommend Visual Studio Code for its good Go support. Alternatively you can select from Editors Docker hub rate limits As docker hub introduced rate limits on docker image downloads this may affect development using the local kind setup. One option to fix the limit is to use a docker hub login for downloading the images. The environment will run a docker registry as a proxy for all public images. This registry runs in a docker container next to the kind clusters. export DOCKERHUB_USERNAME = 'your docker hub username' export DOCKERHUB_PASSWORD = 'your password' Optional Code Improvement and Verification Tools Fybrik repositories use different linters to validate and improve code. golangci-lint works as a Go linters aggregator, which includes a lot of linter such as staticcheck , revive , goimports and more. You can check its configuration in the .golangci.yml files. Integrating golangci-lint with VS Code Change the default lint to golanci-lint in VS Code: Install golangci-lint: https://golangci-lint.run/usage/install/ Open VS Code setting.json : Open the Command Palette: Ctrl+Shift+P In the dropdown search box, search for \"Open Settings (JSON)\" Open setting.json Add to setting.json the following: \"go.lintTool\":\"golangci-lint\", \"go.lintFlags\": [ \"--fast\", \"--allow-parallel-runners\" ] Golangci-lint automatically discovers .golangci.yml in the working project, you don't need to configure it in VS Code settings. To integrate with other IDEs: https://golangci-lint.run/usage/integrations/ If you wish to run golangci-lint on cmd, run in the desired directory: golangci-lint run --fast Pre-commit pre-commit is an optional tool that inspect the snapshot that's about to be committed according to the configured hooks, in our case, golangci-lint . Pre-commit configuration is in .pre-commit-config.yaml How to use: Install pre-commit: https://pre-commit.com/ In the repository, run: pre-commit install Now, pre-commit will automatically validate all your commits. To run commits without pre-commit validation add the --no-verify flag to git commit .","title":"Development Environment"},{"location":"contribute/environment/#development-environment","text":"This page describes what you need to install as a developer and contributor to this project, for setting up a development environment.","title":"Development Environment"},{"location":"contribute/environment/#operating-system","text":"Linux and Mac OS operating systems are officially supported. Windows users should consider using Windows Subsystem for Linux 2 (WSL 2), a remote Linux machine, or any other solution such as a virtual machine.","title":"Operating system"},{"location":"contribute/environment/#dependencies","text":"Install the following on your machine: go 1.17 Docker make jq unzip Mac only : brew install coreutils (installs the timeout command) Then, run the following command to install additional dependencies: make install-tools This installs additional dependencies to hack/tools/bin . The make targets (e.g., make test ) are configured to use the binaries from hack/tools/bin . However, you may want to add some of these tools to your system PATH for direct usage from your terminal (e.g., for using kubectl ). Please note: For fybrik version 0.5.x and lower, Helm version greater than 3.3 but less than 3.7 is required when contributing. On the other side, for fybrik v0.6.x, Helm v3.7 or above is required.","title":"Dependencies"},{"location":"contribute/environment/#editors","text":"The project is predominantly written in Go, so we recommend Visual Studio Code for its good Go support. Alternatively you can select from Editors","title":"Editors"},{"location":"contribute/environment/#docker-hub-rate-limits","text":"As docker hub introduced rate limits on docker image downloads this may affect development using the local kind setup. One option to fix the limit is to use a docker hub login for downloading the images. The environment will run a docker registry as a proxy for all public images. This registry runs in a docker container next to the kind clusters. export DOCKERHUB_USERNAME = 'your docker hub username' export DOCKERHUB_PASSWORD = 'your password'","title":"Docker hub rate limits"},{"location":"contribute/environment/#optional-code-improvement-and-verification-tools","text":"Fybrik repositories use different linters to validate and improve code. golangci-lint works as a Go linters aggregator, which includes a lot of linter such as staticcheck , revive , goimports and more. You can check its configuration in the .golangci.yml files.","title":"Optional Code Improvement and Verification Tools"},{"location":"contribute/environment/#integrating-golangci-lint-with-vs-code","text":"Change the default lint to golanci-lint in VS Code: Install golangci-lint: https://golangci-lint.run/usage/install/ Open VS Code setting.json : Open the Command Palette: Ctrl+Shift+P In the dropdown search box, search for \"Open Settings (JSON)\" Open setting.json Add to setting.json the following: \"go.lintTool\":\"golangci-lint\", \"go.lintFlags\": [ \"--fast\", \"--allow-parallel-runners\" ] Golangci-lint automatically discovers .golangci.yml in the working project, you don't need to configure it in VS Code settings. To integrate with other IDEs: https://golangci-lint.run/usage/integrations/ If you wish to run golangci-lint on cmd, run in the desired directory: golangci-lint run --fast","title":"Integrating golangci-lint with VS Code"},{"location":"contribute/environment/#pre-commit","text":"pre-commit is an optional tool that inspect the snapshot that's about to be committed according to the configured hooks, in our case, golangci-lint . Pre-commit configuration is in .pre-commit-config.yaml How to use: Install pre-commit: https://pre-commit.com/ In the repository, run: pre-commit install Now, pre-commit will automatically validate all your commits. To run commits without pre-commit validation add the --no-verify flag to git commit .","title":"Pre-commit"},{"location":"contribute/flow/","text":"GitHub Workflow This page describes the GitHub workflow that contributors should follow. Issues and pull requests Contributing to Fybrik is done following the GitHub workflow of Pull Requests. You should usually open a pull request in the following situations: Start work on a contribution that was that you\u2019ve already discussed in an issue. Submit trivial fixes (for example, a typo, a broken link or an obvious error). A pull request doesn\u2019t have to represent finished work. It\u2019s usually better to open a draft pull request early on, so others can watch or give feedback on your progress. Here\u2019s how to submit a pull request: Fork the main repository Clone the forked repository locally . Connect your local to the original \u201cupstream\u201d repository by adding it as a remote. git clone git@github.com: $( git config user.name ) /fybrik.git git remote add upstream https://github.com/fybrik/fybrik.git git remote set-url --push upstream no_push Pull in changes from \u201cupstream\u201d often so that you stay up to date so that when you submit your pull request, merge conflicts will be less likely. git fetch upstream master git checkout master git merge upstream/master git push origin master Create a branch for your edits from master. Note that you should never add edits to the master branch itself. git checkout -b <branch name> Make commits of logical units , ensuring that commit messages are in the proper format . Push your changes to the created branch in your fork of the repository. Open a pull request to the original repository. Reference any relevant issues or supporting documentation in your PR (for example, \u201cCloses #37.\u201d) As always, you must follow code style , ensure that all tests pass , and add any new tests as appropriate. Thanks for your contribution! Normalize the code To ensure the code is formatted uniformly we use various linters which are invoked using make verify Format of the Commit Message The project follows a rough convention for commit messages that is designed to answer two questions: what changed and why. The subject line should feature the what and the body of the commit should describe the why. Every commit must also include a DCO Sign Off at the end of the commit message. By doing this you state that you certify the Developer Certificate of Origin . This can be automated by adding the -s flag to git commit . You can also mass sign-off a whole PR with git rebase --signoff master . Example commit message: scripts: add the test-cluster command this uses tmux to setup a test cluster that you can easily kill and start for debugging. Fixes #38 Signed-off-by: Legal Name <your.email@example.com> The format can be described more formally as follows: <subsystem>: <what changed> <BLANK LINE> <why this change was made> <BLANK LINE> <footer> <BLANK LINE> <signoff> The first line is the subject and should be no longer than 70 characters, the second line is always blank, and other lines should be wrapped at 80 characters. This allows the message to be easier to read on GitHub as well as in various git tools.","title":"GitHub Workflow"},{"location":"contribute/flow/#github-workflow","text":"This page describes the GitHub workflow that contributors should follow.","title":"GitHub Workflow"},{"location":"contribute/flow/#issues-and-pull-requests","text":"Contributing to Fybrik is done following the GitHub workflow of Pull Requests. You should usually open a pull request in the following situations: Start work on a contribution that was that you\u2019ve already discussed in an issue. Submit trivial fixes (for example, a typo, a broken link or an obvious error). A pull request doesn\u2019t have to represent finished work. It\u2019s usually better to open a draft pull request early on, so others can watch or give feedback on your progress. Here\u2019s how to submit a pull request: Fork the main repository Clone the forked repository locally . Connect your local to the original \u201cupstream\u201d repository by adding it as a remote. git clone git@github.com: $( git config user.name ) /fybrik.git git remote add upstream https://github.com/fybrik/fybrik.git git remote set-url --push upstream no_push Pull in changes from \u201cupstream\u201d often so that you stay up to date so that when you submit your pull request, merge conflicts will be less likely. git fetch upstream master git checkout master git merge upstream/master git push origin master Create a branch for your edits from master. Note that you should never add edits to the master branch itself. git checkout -b <branch name> Make commits of logical units , ensuring that commit messages are in the proper format . Push your changes to the created branch in your fork of the repository. Open a pull request to the original repository. Reference any relevant issues or supporting documentation in your PR (for example, \u201cCloses #37.\u201d) As always, you must follow code style , ensure that all tests pass , and add any new tests as appropriate. Thanks for your contribution!","title":"Issues and pull requests"},{"location":"contribute/flow/#normalize-the-code","text":"To ensure the code is formatted uniformly we use various linters which are invoked using make verify","title":"Normalize the code"},{"location":"contribute/flow/#format-of-the-commit-message","text":"The project follows a rough convention for commit messages that is designed to answer two questions: what changed and why. The subject line should feature the what and the body of the commit should describe the why. Every commit must also include a DCO Sign Off at the end of the commit message. By doing this you state that you certify the Developer Certificate of Origin . This can be automated by adding the -s flag to git commit . You can also mass sign-off a whole PR with git rebase --signoff master . Example commit message: scripts: add the test-cluster command this uses tmux to setup a test cluster that you can easily kill and start for debugging. Fixes #38 Signed-off-by: Legal Name <your.email@example.com> The format can be described more formally as follows: <subsystem>: <what changed> <BLANK LINE> <why this change was made> <BLANK LINE> <footer> <BLANK LINE> <signoff> The first line is the subject and should be no longer than 70 characters, the second line is always blank, and other lines should be wrapped at 80 characters. This allows the message to be easier to read on GitHub as well as in various git tools.","title":"Format of the Commit Message"},{"location":"contribute/logging/","text":"Logging This page describes the information that your code should provide in all log entries it generates, and some tools fybrik provides to ensure consistency across components. Background Log entries should be written to stdout and stderr. Fybrik does not collect nor aggregate logs. This may be done by external tools. (ex: logstash, fluentd, etc.) A globally unique identifier for each FybrikApplication instance is passed to all control plane and data plane components to be included in log entries. This enables corrrelation of log entries across different logs and clusters for the specific instance, even if the name of the FybrikApplication is reused over time. Log Entry Contents All fybrik components, whether control plane or data plane components, should write log entries to stdout and stderr in json format. The contents of the log entries are detailed in fybrik.io/pkg/logging/logging.go. The fybrik control plane uses zerolog for its golang components, and provides a library of fybrik specific helper functions to be used with it. Examples of how to use zerolog: https://github.com/rs/zerolog/blob/master/log_example_test.go TBD - fybrik logging helper functions for python and java. Log Entry Verbosity The choice of a log level should take into account in which environments the logged information is relevant: production, testing, or development. Although the administrator can configure the verbosity as desired, the following are typical configurations for the different environments. All environments Errors should always be logged, and preferably with as much information as possible. To this end, the function LogStructure in in pkg/logging/logging.go converts golang structures to json for inclussion in the log. Please note that panic and fatal should be used sparingly. panic (zerolog.PanicLevel, 5) - Errors that prevent the component from operating correctly and handling requests Ex: fybrik control plane did not deploy correctly Ex: Data plane component crashed and cannot handle requests fatal (zerolog.FatalLevel, 4) - Errors that prevent the component from successfully completing a particular task Ex: fybrikapplication controller cannot generate a plotter Ex: Arrow/Flight server used to read data cannot access data store error (zerolog.ErrorLevel, 3) - Errors that are not fatal nor panic, but that the user / request initiator is made aware of (typical production setting for stable solution) Ex: Dataset requested in fybrikapplication.spec is not allowed to be used Ex: Query to Arrow/Flight server used to read data returns an error because of incorrect dataset ID warn (zerolog.WarnLevel, 2) - Errors not shared with the user / request initiator, typically from which the component recovers on its own Production All of the previous plus: - info (zerolog.InfoLevel, 1) - High level health information that makes it clear the overall status, but without much detail (highest level used in production) Testing All of the previous plus: - debug (zerolog.DebugLevel, 0) - Additional information needed to help identify problems (typically used during testing) Development All of the previous plus: - trace (zerolog.TraceLevel, -1) - For tracing step by step flow of control (typically used during development) JSON Logging Standard Format All Fybrik components should generate logging information in a standard format. This information will be used by different actors for different purposes, so as much relevant information as possible needs to be captured in a consistent format. We list all mandatory and optional fields to be used by all Fybrik components. In addition to the fields we list, Fybrik components may include extra fields as needed. Mandatory Fields The fields in this section are typically generated by the logging libraries. level - log level (\u2018panic\u2019, \u2018fatal\u2019, \u2018error\u2019, \u2018warn\u2019, \u2018info\u2019, \u2018debug\u2019, or \u2018trace\u2019) time - timestamp of the log event. Timestamps should be in ISO8601 format with time offset from UTC or timezone. Example: \u20182022-02-16T10:46:21+02:00\u2019 caller - the code line which generated the error (file name + line number). Example: manager/main.go:319 Optional Fields app.fybrik.io/app-uuid - unique identifier for kubernetes FybrikApplication, used to correlate log messages across components for a particular FybrikApplication instance. It is also unique over time so one may differentiate between FybrikApplication instances with the same name created at different times message - string message for the log entry. Either this field or message_id must be included message_id - unique identifier indicating the message string that should be used. This is used instead of a message string for messages that need to support internationalization, such as those that go to users funcName - method or function in which the error occurred DataSetID - unique identifier for the data set ForUser - True if this should be shared with the end user in fybrikapplication status or events. False otherwise ForArchive - True if this should be archived long term. For example, if it contains full contents of FybrikApplication and its status and should be stored for auditing purposes cluster - cluster name on which the process generating the entry ran component - name of the component generating the log entry action - current operation being called. For example, \u201ccreate_catalog\u201d or \u201cupdate_asset\u201d response_time - response time of the current operation in milliseconds. Can be used in monitoring dashboards such as Kibana error \u2013 the error code or message returned to the fybrik component upon an unsuccessful action. Additional context should usually be provided in the accompanying message field Environment Variables LOGGING_VERBOSITY - should be set to one of the levels described in the previous section. PRETTY_LOGGING - If true log entries are in human readable format. If false, they are in json. Should only be true during development, since json is preferred to enable easy parsing by aggregator tools. Logging of Structures Fybrik provides a helper function called LogStructure in pkg/logging/logging.go for writing Go structures in json format to the log. It supports different verbosity levels, and thus can be used in production, testing and development environments.","title":"Logging"},{"location":"contribute/logging/#logging","text":"This page describes the information that your code should provide in all log entries it generates, and some tools fybrik provides to ensure consistency across components.","title":"Logging"},{"location":"contribute/logging/#background","text":"Log entries should be written to stdout and stderr. Fybrik does not collect nor aggregate logs. This may be done by external tools. (ex: logstash, fluentd, etc.) A globally unique identifier for each FybrikApplication instance is passed to all control plane and data plane components to be included in log entries. This enables corrrelation of log entries across different logs and clusters for the specific instance, even if the name of the FybrikApplication is reused over time.","title":"Background"},{"location":"contribute/logging/#log-entry-contents","text":"All fybrik components, whether control plane or data plane components, should write log entries to stdout and stderr in json format. The contents of the log entries are detailed in fybrik.io/pkg/logging/logging.go. The fybrik control plane uses zerolog for its golang components, and provides a library of fybrik specific helper functions to be used with it. Examples of how to use zerolog: https://github.com/rs/zerolog/blob/master/log_example_test.go TBD - fybrik logging helper functions for python and java.","title":"Log Entry Contents"},{"location":"contribute/logging/#log-entry-verbosity","text":"The choice of a log level should take into account in which environments the logged information is relevant: production, testing, or development. Although the administrator can configure the verbosity as desired, the following are typical configurations for the different environments.","title":"Log Entry Verbosity"},{"location":"contribute/logging/#all-environments","text":"Errors should always be logged, and preferably with as much information as possible. To this end, the function LogStructure in in pkg/logging/logging.go converts golang structures to json for inclussion in the log. Please note that panic and fatal should be used sparingly. panic (zerolog.PanicLevel, 5) - Errors that prevent the component from operating correctly and handling requests Ex: fybrik control plane did not deploy correctly Ex: Data plane component crashed and cannot handle requests fatal (zerolog.FatalLevel, 4) - Errors that prevent the component from successfully completing a particular task Ex: fybrikapplication controller cannot generate a plotter Ex: Arrow/Flight server used to read data cannot access data store error (zerolog.ErrorLevel, 3) - Errors that are not fatal nor panic, but that the user / request initiator is made aware of (typical production setting for stable solution) Ex: Dataset requested in fybrikapplication.spec is not allowed to be used Ex: Query to Arrow/Flight server used to read data returns an error because of incorrect dataset ID warn (zerolog.WarnLevel, 2) - Errors not shared with the user / request initiator, typically from which the component recovers on its own","title":"All environments"},{"location":"contribute/logging/#production","text":"All of the previous plus: - info (zerolog.InfoLevel, 1) - High level health information that makes it clear the overall status, but without much detail (highest level used in production)","title":"Production"},{"location":"contribute/logging/#testing","text":"All of the previous plus: - debug (zerolog.DebugLevel, 0) - Additional information needed to help identify problems (typically used during testing)","title":"Testing"},{"location":"contribute/logging/#development","text":"All of the previous plus: - trace (zerolog.TraceLevel, -1) - For tracing step by step flow of control (typically used during development)","title":"Development"},{"location":"contribute/logging/#json-logging-standard-format","text":"All Fybrik components should generate logging information in a standard format. This information will be used by different actors for different purposes, so as much relevant information as possible needs to be captured in a consistent format. We list all mandatory and optional fields to be used by all Fybrik components. In addition to the fields we list, Fybrik components may include extra fields as needed.","title":"JSON Logging Standard Format"},{"location":"contribute/logging/#mandatory-fields","text":"The fields in this section are typically generated by the logging libraries. level - log level (\u2018panic\u2019, \u2018fatal\u2019, \u2018error\u2019, \u2018warn\u2019, \u2018info\u2019, \u2018debug\u2019, or \u2018trace\u2019) time - timestamp of the log event. Timestamps should be in ISO8601 format with time offset from UTC or timezone. Example: \u20182022-02-16T10:46:21+02:00\u2019 caller - the code line which generated the error (file name + line number). Example: manager/main.go:319","title":"Mandatory Fields"},{"location":"contribute/logging/#optional-fields","text":"app.fybrik.io/app-uuid - unique identifier for kubernetes FybrikApplication, used to correlate log messages across components for a particular FybrikApplication instance. It is also unique over time so one may differentiate between FybrikApplication instances with the same name created at different times message - string message for the log entry. Either this field or message_id must be included message_id - unique identifier indicating the message string that should be used. This is used instead of a message string for messages that need to support internationalization, such as those that go to users funcName - method or function in which the error occurred DataSetID - unique identifier for the data set ForUser - True if this should be shared with the end user in fybrikapplication status or events. False otherwise ForArchive - True if this should be archived long term. For example, if it contains full contents of FybrikApplication and its status and should be stored for auditing purposes cluster - cluster name on which the process generating the entry ran component - name of the component generating the log entry action - current operation being called. For example, \u201ccreate_catalog\u201d or \u201cupdate_asset\u201d response_time - response time of the current operation in milliseconds. Can be used in monitoring dashboards such as Kibana error \u2013 the error code or message returned to the fybrik component upon an unsuccessful action. Additional context should usually be provided in the accompanying message field","title":"Optional Fields"},{"location":"contribute/logging/#environment-variables","text":"LOGGING_VERBOSITY - should be set to one of the levels described in the previous section. PRETTY_LOGGING - If true log entries are in human readable format. If false, they are in json. Should only be true during development, since json is preferred to enable easy parsing by aggregator tools.","title":"Environment Variables"},{"location":"contribute/logging/#logging-of-structures","text":"Fybrik provides a helper function called LogStructure in pkg/logging/logging.go for writing Go structures in json format to the log. It supports different verbosity levels, and thus can be used in production, testing and development environments.","title":"Logging of Structures"},{"location":"contribute/modules/","text":"Module Development This page describes what must be provided when contributing a module . Steps for creating a module Implement the logic of the module you are contributing. The implementation can either be directly in the Module Workload or in an external component. If the logic is in an external component, then the module workload should act as a client - i.e. receiving paramaters from the control plane and passing them to the external component. Create and publish the Module Helm Chart that will be used by the control plane to deploy the module workload, update it, and delete it as necessary. Create the FybrikModule YAML which describes the capabilities of the module workload, in which flows it should be considered for inclusion, its supported interfaces, and the link to the module helm chart. Test the new module These steps are described in the following sections in more detail, so that you can create your own modules for use by Fybrik. Note that a new module is maintained in its own git repository, separate from the fybrik repository. Module Workload The module workload is associated with a specific user workload and is deployed by the control plane. It may implement the logic required itself, or it may be a client interface to an external component. The former will have module type \"server\" and the latter \"config\". There is also a third type of module workload known as a plugin. It provides a standard interface by which another module may invoke its capabilities. For example, you may have a module that reads data but doesn't know how to do data transforms. Rather than implementing transforms in the module workload code, it can call the plugin to do the transforms. The control plane deploys the relevant transform plugin as well as the read module. Credential management Modules that access or write data need credentials in order to access the data store. The credentials are retrieved from HashiCorp Vault . The parameters to login to vault and to read secret are passed as part of the arguments to the module Helm chart. An example for Vault Login API call which uses the Vault parameters is as follows: $ curl -v -X POST <address>/<authPath> -H \"Content-Type: application/json\" --data '{\"jwt\": <module service account token>, \"role\": <role>}' An example for Vault Read Secret API call which uses the Vault parameters is as follows: $ curl --header \"X-Vault-Token: ...\" -X GET https://<address>/<secretPath> Fybrik repository contains a Python Vault package that modules can use to retrieve the credentials. Module Helm Chart For any module chosen by the control plane to be part of the data path, the control plane needs to be able to install/remove/upgrade an instance of the module. Fybrik uses Helm to provide this functionality. Follow the Helm getting started guide if you are unfamiliar with Helm. Note that Helm 3.7 or above is required. The names of the Kubernetes resources deployed by the module helm chart must contain the release name to avoid resource conflicts. A Kubernetes service resource which is used to access the module must have a name equal to the release name (this service name is also used in the optional spec.capabilities.api.endpoint.hostname field). Because the chart is installed by the control plane, the input values to the chart will contain the following information: .Values.assets - a list of asset arguments such as datastores, transformations, etc. .Values.selector - application selector .Values.context - application context .Values.labels - labels specified in FybrikApplication .Values.uuid - a unique id of FybrikApplication An example of values passed to a module(values.sample.yaml): labels: app.fybrik.io/app-name: my-notebook-read namespace: fybrik-notebook-sample uuid: 12345678 context: intent: \"Fraud Detection\" selector: matchLabels: app: my-notebook assets: - args: - connection: name: s3 s3: bucket: fybrik-test-bucket endpoint: s3.eu-gb.cloud-object-storage.appdomain.cloud object_key: test1.parquet format: parquet vault: read: address: http://vault.fybrik-system:8200 authPath: /v1/auth/kubernetes/login role: module secretPath: /v1/kubernetes-secrets/data-creds?namespace=fybrik-notebook-sample assetID: \"test1\" capability: read transformations: - name: \"RedactAction\" RedactAction: columns: - col1 - col2 If the module workload needs to return information to the user, that information should be written to the NOTES.txt of the helm chart. For a full example see the Arrow Flight Module chart . Publishing the Helm Chart Once your Helm chart is ready, you need to push it to a OCI-based registry such as ghcr.io . This allows the control plane of Fybrik to later pull the chart whenever it needs to be installed. You can use the hack/make-rules/helm.mk Makefile, or manually push the chart as described in the link : helm registry login -u <username> <registry> helm package <chart folder> -d <local-chart-path> helm push <local-chart-path> oci://<registry>/<path> FybrikModule YAML FybrikModule is a kubernetes Custom Resource Definition (custom resource) which describes to the control plane the functionality provided by the module. The FybrikModule custom resource has no controller. The specification of the FybrikModule Kubernetes custom resource is available in the API documentation . The YAML file begins with standard Kubernetes metadata followed by the FybrikModule specification: apiVersion : app.fybrik.io/v1alpha1 # always this value kind : FybrikModule # always this value metadata : name : \"<module name>\" # the name of your new module labels : name : \"<module name>\" # the name of your new module version : \"<semantic version>\" namespace : fybrik-system # control plane namespace. Always fybrik-system spec : ... The child fields of spec are described next. spec.chart This is a link to a the Helm chart stored in the image registry . This is similar to how a Kubernetes Pod references a container image. See Module Helm chart for more details. spec: chart: name: \"<helm chart link>\" # e.g.: ghcr.io/username/chartname:chartversion values: image.tag: v0.0.1 spec.statusIndicators Used for tracking the status of the module in terms of success or failure. In many cases this can be omitted and the status will be detected automatically. if the Helm chart includes standard Kubernetes resources such as Deployment and Service, then the status is automatically detected. If however Custom Resource Definitions are used, then the status may not be automatically detected and statusIndicators should be specified. statusIndicators : - kind : \"<module name>\" successCondition : \"<condition>\" # ex: status.status == SUCCEEDED failureCondition : \"<condition>\" # ex: status.status == FAILED errorMessage : \"<field path>\" # ex: status.error spec.dependencies A dependency has a type and a name . Currently dependencies of type module are supported, indicating that another module must also be installed for this module to work. dependencies : - type : module #currently the only option is a dependency on another module deployed by the control plane name : <dependent module name> spec.type The type field may be one of the following vaues: 1)service - Indicates that module workload implements the modules logic, and is deployed by the fybrik control plane. 2) config - In this case the logic is performed by a component deployed externally, i.e. not by the fybrik control plane. Such components can be assumed to support multiple workloads. 3) plugin (FUTURE) - This type of module enables a sub-set of often used capabilities to be implemented once and re-used by any module that supports plugins of the declared type. spec.pluginType (Future Functionality) The types of plugins supported by this module. Example: vault, fybrik-wasm ... spec.capabilities Each module may support one or more capabilities. Currently there are four capabilities: read for enabling an application to read data or prepare data for being read, write for enabling an application to write data, and copy for performing an implicit data copy on behalf of the application, and transform for altering data based on governance policies. A module provides one or more of these capabilities. capabilities.capability Indicates which of the types of capabilities this instance describes. capability : # Indicate the capabilities for which the control plane should consider using this module - read # optional - write # optional - copy # optional - transform # optional capability.scope The capability provided by the module may work on one of several different scopes: workload - deployed once by fybrik and available for use by the data planes of all the datasets asset - deployed by fybrik for each dataset cluster - deployed outside of fybrik and can be used by multiple fybbrik workloads in a given cluster scope : <scope of the capability> # cluster, workload, asset capabilites.supportedInterfaces Lists the supported data services from which the module can read data (sources) and to which it can write (sinks). There can be multiple sources and sinks. For each, a protocol and format are provided. protocol field can take a value such as kafka , s3 , db2 , fybrik-arrow-flight , etc. format field can take a value such as avro , parquet , json , or csv . Note that a module that targets copy flows will omit the api field and contain just source and sink , a module that only supports reading data assets will omit the sink field and only contain api and source capabilites.api describes the api exposed by the module to the user's workload for the particular capability. protocol field can take a value such as kafka , s3 , db2 , fybrik-arrow-flight , etc dataformat field can take a value such as parquet , csv , avro , etc endpoint field describes the endpoint exposed the module capabilites.api.endpoint describes the endpoint from a networking perspective: hostname field is the hostname to be used when accessing the module. Equals the release name. Can be omitted. port field is the port of the service exposed by the module. scheme field can take a value such as http , https , grpc , grpc+tls , jdbc:oracle:thin:@ , etc An example for a module that copies data from a db2 database table to an s3 bucket in parquet format. capabilities : - capability : copy supportedInterfaces : - source : protocol : db2 sink : protocol : s3 dataformat : parquet An example for a module that has an API for reading data, and supports reading both parquet and csv formats from s3. capabilities : - capability : read api : protocol : fybrik-arrow-flight endpoint : port : 80 scheme : grpc supportedInterfaces : - source : protocol : s3 dataformat : parquet - flow : read source : protocol : s3 dataformat : csv capabilites.actions are taken from a defined Enforcement Actions Taxonomy a module that does not perform any transformation on the data may omit the capabilities.actions field. The following is an example of how a module would declare that it knows how to redact, remove or encrypt data. Additional properties may be associated with each action. capabilities : - read : actions : - name : \"RedactAction\" - name : \"RemoveAction\" - name : \"EncryptAction\" Full Examples The following are examples of YAMLs from fully implemented modules: An example YAML for a module that copies from db2 to s3 and includes transformation actions And an example arrow flight read module YAML, also with transformation support Getting Started In order to help module developers get started there are two example \"hello world\" modules: * Hello world module * Hello world read module An example of a fully functional module is the [arrow flight module][https://github.com/fybrik/arrow-flight-module] Test Register the module to make the control plane aware of it. Create an FybrikApplication YAML for a user workload, ensuring that the data set and other parameters included in it, together with the governance policies defined in the policy manager, will result in your module being chosen based on the control plane logic . Apply the FybrikApplication YAML. View the FybrikApplication status . Run the user workload and review the results to check if they are what is expected.","title":"Module Development"},{"location":"contribute/modules/#module-development","text":"This page describes what must be provided when contributing a module .","title":"Module Development"},{"location":"contribute/modules/#steps-for-creating-a-module","text":"Implement the logic of the module you are contributing. The implementation can either be directly in the Module Workload or in an external component. If the logic is in an external component, then the module workload should act as a client - i.e. receiving paramaters from the control plane and passing them to the external component. Create and publish the Module Helm Chart that will be used by the control plane to deploy the module workload, update it, and delete it as necessary. Create the FybrikModule YAML which describes the capabilities of the module workload, in which flows it should be considered for inclusion, its supported interfaces, and the link to the module helm chart. Test the new module These steps are described in the following sections in more detail, so that you can create your own modules for use by Fybrik. Note that a new module is maintained in its own git repository, separate from the fybrik repository.","title":"Steps for creating a module"},{"location":"contribute/modules/#module-workload","text":"The module workload is associated with a specific user workload and is deployed by the control plane. It may implement the logic required itself, or it may be a client interface to an external component. The former will have module type \"server\" and the latter \"config\". There is also a third type of module workload known as a plugin. It provides a standard interface by which another module may invoke its capabilities. For example, you may have a module that reads data but doesn't know how to do data transforms. Rather than implementing transforms in the module workload code, it can call the plugin to do the transforms. The control plane deploys the relevant transform plugin as well as the read module.","title":"Module Workload"},{"location":"contribute/modules/#credential-management","text":"Modules that access or write data need credentials in order to access the data store. The credentials are retrieved from HashiCorp Vault . The parameters to login to vault and to read secret are passed as part of the arguments to the module Helm chart. An example for Vault Login API call which uses the Vault parameters is as follows: $ curl -v -X POST <address>/<authPath> -H \"Content-Type: application/json\" --data '{\"jwt\": <module service account token>, \"role\": <role>}' An example for Vault Read Secret API call which uses the Vault parameters is as follows: $ curl --header \"X-Vault-Token: ...\" -X GET https://<address>/<secretPath> Fybrik repository contains a Python Vault package that modules can use to retrieve the credentials.","title":"Credential management"},{"location":"contribute/modules/#module-helm-chart","text":"For any module chosen by the control plane to be part of the data path, the control plane needs to be able to install/remove/upgrade an instance of the module. Fybrik uses Helm to provide this functionality. Follow the Helm getting started guide if you are unfamiliar with Helm. Note that Helm 3.7 or above is required. The names of the Kubernetes resources deployed by the module helm chart must contain the release name to avoid resource conflicts. A Kubernetes service resource which is used to access the module must have a name equal to the release name (this service name is also used in the optional spec.capabilities.api.endpoint.hostname field). Because the chart is installed by the control plane, the input values to the chart will contain the following information: .Values.assets - a list of asset arguments such as datastores, transformations, etc. .Values.selector - application selector .Values.context - application context .Values.labels - labels specified in FybrikApplication .Values.uuid - a unique id of FybrikApplication An example of values passed to a module(values.sample.yaml): labels: app.fybrik.io/app-name: my-notebook-read namespace: fybrik-notebook-sample uuid: 12345678 context: intent: \"Fraud Detection\" selector: matchLabels: app: my-notebook assets: - args: - connection: name: s3 s3: bucket: fybrik-test-bucket endpoint: s3.eu-gb.cloud-object-storage.appdomain.cloud object_key: test1.parquet format: parquet vault: read: address: http://vault.fybrik-system:8200 authPath: /v1/auth/kubernetes/login role: module secretPath: /v1/kubernetes-secrets/data-creds?namespace=fybrik-notebook-sample assetID: \"test1\" capability: read transformations: - name: \"RedactAction\" RedactAction: columns: - col1 - col2 If the module workload needs to return information to the user, that information should be written to the NOTES.txt of the helm chart. For a full example see the Arrow Flight Module chart .","title":"Module Helm Chart"},{"location":"contribute/modules/#publishing-the-helm-chart","text":"Once your Helm chart is ready, you need to push it to a OCI-based registry such as ghcr.io . This allows the control plane of Fybrik to later pull the chart whenever it needs to be installed. You can use the hack/make-rules/helm.mk Makefile, or manually push the chart as described in the link : helm registry login -u <username> <registry> helm package <chart folder> -d <local-chart-path> helm push <local-chart-path> oci://<registry>/<path>","title":"Publishing the Helm Chart"},{"location":"contribute/modules/#fybrikmodule-yaml","text":"FybrikModule is a kubernetes Custom Resource Definition (custom resource) which describes to the control plane the functionality provided by the module. The FybrikModule custom resource has no controller. The specification of the FybrikModule Kubernetes custom resource is available in the API documentation . The YAML file begins with standard Kubernetes metadata followed by the FybrikModule specification: apiVersion : app.fybrik.io/v1alpha1 # always this value kind : FybrikModule # always this value metadata : name : \"<module name>\" # the name of your new module labels : name : \"<module name>\" # the name of your new module version : \"<semantic version>\" namespace : fybrik-system # control plane namespace. Always fybrik-system spec : ... The child fields of spec are described next.","title":"FybrikModule YAML"},{"location":"contribute/modules/#specchart","text":"This is a link to a the Helm chart stored in the image registry . This is similar to how a Kubernetes Pod references a container image. See Module Helm chart for more details. spec: chart: name: \"<helm chart link>\" # e.g.: ghcr.io/username/chartname:chartversion values: image.tag: v0.0.1","title":"spec.chart"},{"location":"contribute/modules/#specstatusindicators","text":"Used for tracking the status of the module in terms of success or failure. In many cases this can be omitted and the status will be detected automatically. if the Helm chart includes standard Kubernetes resources such as Deployment and Service, then the status is automatically detected. If however Custom Resource Definitions are used, then the status may not be automatically detected and statusIndicators should be specified. statusIndicators : - kind : \"<module name>\" successCondition : \"<condition>\" # ex: status.status == SUCCEEDED failureCondition : \"<condition>\" # ex: status.status == FAILED errorMessage : \"<field path>\" # ex: status.error","title":"spec.statusIndicators"},{"location":"contribute/modules/#specdependencies","text":"A dependency has a type and a name . Currently dependencies of type module are supported, indicating that another module must also be installed for this module to work. dependencies : - type : module #currently the only option is a dependency on another module deployed by the control plane name : <dependent module name>","title":"spec.dependencies"},{"location":"contribute/modules/#spectype","text":"The type field may be one of the following vaues: 1)service - Indicates that module workload implements the modules logic, and is deployed by the fybrik control plane. 2) config - In this case the logic is performed by a component deployed externally, i.e. not by the fybrik control plane. Such components can be assumed to support multiple workloads. 3) plugin (FUTURE) - This type of module enables a sub-set of often used capabilities to be implemented once and re-used by any module that supports plugins of the declared type.","title":"spec.type"},{"location":"contribute/modules/#specplugintype","text":"(Future Functionality) The types of plugins supported by this module. Example: vault, fybrik-wasm ...","title":"spec.pluginType"},{"location":"contribute/modules/#speccapabilities","text":"Each module may support one or more capabilities. Currently there are four capabilities: read for enabling an application to read data or prepare data for being read, write for enabling an application to write data, and copy for performing an implicit data copy on behalf of the application, and transform for altering data based on governance policies. A module provides one or more of these capabilities. capabilities.capability Indicates which of the types of capabilities this instance describes. capability : # Indicate the capabilities for which the control plane should consider using this module - read # optional - write # optional - copy # optional - transform # optional capability.scope The capability provided by the module may work on one of several different scopes: workload - deployed once by fybrik and available for use by the data planes of all the datasets asset - deployed by fybrik for each dataset cluster - deployed outside of fybrik and can be used by multiple fybbrik workloads in a given cluster scope : <scope of the capability> # cluster, workload, asset capabilites.supportedInterfaces Lists the supported data services from which the module can read data (sources) and to which it can write (sinks). There can be multiple sources and sinks. For each, a protocol and format are provided. protocol field can take a value such as kafka , s3 , db2 , fybrik-arrow-flight , etc. format field can take a value such as avro , parquet , json , or csv . Note that a module that targets copy flows will omit the api field and contain just source and sink , a module that only supports reading data assets will omit the sink field and only contain api and source capabilites.api describes the api exposed by the module to the user's workload for the particular capability. protocol field can take a value such as kafka , s3 , db2 , fybrik-arrow-flight , etc dataformat field can take a value such as parquet , csv , avro , etc endpoint field describes the endpoint exposed the module capabilites.api.endpoint describes the endpoint from a networking perspective: hostname field is the hostname to be used when accessing the module. Equals the release name. Can be omitted. port field is the port of the service exposed by the module. scheme field can take a value such as http , https , grpc , grpc+tls , jdbc:oracle:thin:@ , etc An example for a module that copies data from a db2 database table to an s3 bucket in parquet format. capabilities : - capability : copy supportedInterfaces : - source : protocol : db2 sink : protocol : s3 dataformat : parquet An example for a module that has an API for reading data, and supports reading both parquet and csv formats from s3. capabilities : - capability : read api : protocol : fybrik-arrow-flight endpoint : port : 80 scheme : grpc supportedInterfaces : - source : protocol : s3 dataformat : parquet - flow : read source : protocol : s3 dataformat : csv capabilites.actions are taken from a defined Enforcement Actions Taxonomy a module that does not perform any transformation on the data may omit the capabilities.actions field. The following is an example of how a module would declare that it knows how to redact, remove or encrypt data. Additional properties may be associated with each action. capabilities : - read : actions : - name : \"RedactAction\" - name : \"RemoveAction\" - name : \"EncryptAction\"","title":"spec.capabilities"},{"location":"contribute/modules/#full-examples","text":"The following are examples of YAMLs from fully implemented modules: An example YAML for a module that copies from db2 to s3 and includes transformation actions And an example arrow flight read module YAML, also with transformation support","title":"Full Examples"},{"location":"contribute/modules/#getting-started","text":"In order to help module developers get started there are two example \"hello world\" modules: * Hello world module * Hello world read module An example of a fully functional module is the [arrow flight module][https://github.com/fybrik/arrow-flight-module]","title":"Getting Started"},{"location":"contribute/modules/#test","text":"Register the module to make the control plane aware of it. Create an FybrikApplication YAML for a user workload, ensuring that the data set and other parameters included in it, together with the governance policies defined in the policy manager, will result in your module being chosen based on the control plane logic . Apply the FybrikApplication YAML. View the FybrikApplication status . Run the user workload and review the results to check if they are what is expected.","title":"Test"},{"location":"contribute/openapi/","text":"Using OpenAPI Generator to Generate Code for a Data Catalog Connector (in the Go Language) The Fybrik repository contains specification files that detail the data catalog connector API. These files include datacatalog.spec.yaml and taxonomy.json . They detail all the fields that the data catalog connector should expect for each of the supported operations: createAsset, getAssetInfo, deleteAsset, and updateAsset. The OpenAPI generator is a tool that can be used to generate the skeleton code for REST servers in a variety of programming languages, given a specification file (written in adherence to the OpenAPI standard ). In our case, we used the OpenAPI generator to generate skeleton code for a Fybrik data catalog connector server, in the go programming language. As expected, this skeleton code does not provide any functionality, since the specification file details only the API, not the functionality. Also, the behavior of the actual connector code must surely depend on the data catalog chosen to organize the Fyrbik assets. Here is a snippet of the Makefile used to automatically generate code: generate-code: git clone https://github.com/fybrik/fybrik/ cd fybrik && git checkout ${FYBRIK_VERSION} docker run --rm \\ -v ${PWD}:/local \\ -u \"${USER_ID}:${GROUP_ID}\" \\ openapitools/openapi-generator-cli generate -g go-server \\ --git-user-id=${GIT_USER_ID} \\ --git-repo-id=${GIT_REPO_ID} \\ --config=/local/openapi-configs/go-server-config.yaml \\ -o /local/api \\ -i /local/fybrik/connectors/api/datacatalog.spec.yaml rm -Rf fybrik One thing that the automatically generated code is very good at (besides providing us with the REST server framework) is checking the validity of the REST requests. The server code automatically rejects any requests whose body does not adhere to the specification. For instance, if any fields are missing from the request body, the server code would fail the request and return the relevant error code and error message. We found one main problem with the auto-generated code, and it had to do with the Connection structure. The Connection structure has one mandatory field, called name . According to the taxonomy, there could be additional properties, and there are no limitations on their names and types. The problem we encountered had to do with the additional properties. Here is an example of the connection information for a typical asset: asset.yaml . The connection information includes the mandatory name field, but also an additional property s3 with a few subfields. We found that assets with such connection information were rejected by the server with the following error: \"json: unknown field \\\"s3\\\"\" . This problem was easily overcome when we commented out the d.DisallowUnknownFields() line in the CreateAsset method. However, we encountered a more difficult problem when it turned out that these \"unknown fields\" were ignored and were not kept in the CreateAssetRequest objects. As a result, the unknown fields could not have been sent to the data catalog, and were removed from the asset metadata. There are several ways to overcome this problem. The best way we found so far was to avoid using the CreateAssetRequest struct automatically generated by the OpenAPI generator. Instead, we used a CreateAssetRequest struct defined as part of the Fybrik code . The connection subfield of the Fybrik CreateAssetRequest struct has a different definition , which includes both the name field and additional properties, as required.","title":"Using OpenAPI Generator"},{"location":"contribute/openapi/#using-openapi-generator-to-generate-code-for-a-data-catalog-connector","text":"(in the Go Language) The Fybrik repository contains specification files that detail the data catalog connector API. These files include datacatalog.spec.yaml and taxonomy.json . They detail all the fields that the data catalog connector should expect for each of the supported operations: createAsset, getAssetInfo, deleteAsset, and updateAsset. The OpenAPI generator is a tool that can be used to generate the skeleton code for REST servers in a variety of programming languages, given a specification file (written in adherence to the OpenAPI standard ). In our case, we used the OpenAPI generator to generate skeleton code for a Fybrik data catalog connector server, in the go programming language. As expected, this skeleton code does not provide any functionality, since the specification file details only the API, not the functionality. Also, the behavior of the actual connector code must surely depend on the data catalog chosen to organize the Fyrbik assets. Here is a snippet of the Makefile used to automatically generate code: generate-code: git clone https://github.com/fybrik/fybrik/ cd fybrik && git checkout ${FYBRIK_VERSION} docker run --rm \\ -v ${PWD}:/local \\ -u \"${USER_ID}:${GROUP_ID}\" \\ openapitools/openapi-generator-cli generate -g go-server \\ --git-user-id=${GIT_USER_ID} \\ --git-repo-id=${GIT_REPO_ID} \\ --config=/local/openapi-configs/go-server-config.yaml \\ -o /local/api \\ -i /local/fybrik/connectors/api/datacatalog.spec.yaml rm -Rf fybrik One thing that the automatically generated code is very good at (besides providing us with the REST server framework) is checking the validity of the REST requests. The server code automatically rejects any requests whose body does not adhere to the specification. For instance, if any fields are missing from the request body, the server code would fail the request and return the relevant error code and error message. We found one main problem with the auto-generated code, and it had to do with the Connection structure. The Connection structure has one mandatory field, called name . According to the taxonomy, there could be additional properties, and there are no limitations on their names and types. The problem we encountered had to do with the additional properties. Here is an example of the connection information for a typical asset: asset.yaml . The connection information includes the mandatory name field, but also an additional property s3 with a few subfields. We found that assets with such connection information were rejected by the server with the following error: \"json: unknown field \\\"s3\\\"\" . This problem was easily overcome when we commented out the d.DisallowUnknownFields() line in the CreateAsset method. However, we encountered a more difficult problem when it turned out that these \"unknown fields\" were ignored and were not kept in the CreateAssetRequest objects. As a result, the unknown fields could not have been sent to the data catalog, and were removed from the asset metadata. There are several ways to overcome this problem. The best way we found so far was to avoid using the CreateAssetRequest struct automatically generated by the OpenAPI generator. Instead, we used a CreateAssetRequest struct defined as part of the Fybrik code . The connection subfield of the Fybrik CreateAssetRequest struct has a different definition , which includes both the name field and additional properties, as required.","title":"Using OpenAPI Generator to Generate Code for a Data Catalog Connector"},{"location":"contribute/documentation/","text":"Contribute Documentation The content of this website is the documentation of the project. The documentation is managed in /site/docs as markdown files. MkDocs and the Material theme are used to generate the website from these markdown files. Reference pages are auto generated from the source code. Therefore, if you change Kubernetes Custom Resource Definitions or the connectors API then you must add reasonable documentation comments. The rest of the documentation pages are written manually. Contributing to the documentation is therefore similar to code contribution and follows the same process of using pull requests. However, when writing documentation you must also follow the formatting and style guidelines.","title":"Contribute Documentation"},{"location":"contribute/documentation/#contribute-documentation","text":"The content of this website is the documentation of the project. The documentation is managed in /site/docs as markdown files. MkDocs and the Material theme are used to generate the website from these markdown files. Reference pages are auto generated from the source code. Therefore, if you change Kubernetes Custom Resource Definitions or the connectors API then you must add reasonable documentation comments. The rest of the documentation pages are written manually. Contributing to the documentation is therefore similar to code contribution and follows the same process of using pull requests. However, when writing documentation you must also follow the formatting and style guidelines.","title":"Contribute Documentation"},{"location":"contribute/documentation/formatting/","text":"Formatting Standards This page shows the formatting standards for the Fybrik documentation. Link to other pages using relative links When linking between pages in the documentation you can simply use the regular Markdown linking syntax, including the relative path to the Markdown document you wish to link to. For example: Please see the [ project license ]( license.md ) for further details. If the target documentation file is in another directory you'll need to make sure to include any relative directory path in the link: Please see the [ project license ]( ../about/license.md ) for further details. Prefer SVG format for diagrams Place image files in the docs/static directory. Use regular Markdown syntax for images. For example: ![](../static/myimage.svg) To make localization easier and enhance accessibility, the preferred image format is SVG. We recommend to use draw.io for creating images and diagrams. Use Export as to save your image in SVG format. Keep the Include a copy of my diagram option checked to allow later loading the SVG in draw.io and be sure to check Embed images if you diagram includes any. If your diagram depicts a process, try to avoid adding the descriptions of the steps to the diagram. Instead, only add the numbers of the steps to the diagram and add the descriptions of the steps as a numbered list in the document. Ensure that the numbers on the list match the numbers on your diagram. This approach helps make diagrams easier to understand and the content more accessible. Do not wrap lines Never wrap lines after a fixed number of characters or in a middle of a senstence. Instead, configure your editor to soft-wrap when editing documentation. Do Don't This is a long line. This is a long line. Use angle brackets for placeholders Use angle brackets for placeholders in commands or code samples. Tell the reader what the placeholder represents. For example: Display information about a pod: $ kubectl describe pod <pod-name> Where <pod-name> is the name of one of your pods. Use bold to emphasize user interface elements Do Don't Click Fork . Click \"Fork\". Select Other . Select 'Other'. Use bold to emphasize important text Use bold to emphasize text that is particularly important. Avoid overusing bold as it reduces its impact and readability. Do Don't Examples of bad configurations: Examples of bad configurations : The maximum length of the name field is 256 characters . The maximum length of the name field is 256 characters . Don't use capitalization for emphasis Only use the original capitalization found in the code or configuration files when referencing those values directly. Use back-ticks `` around the referenced value to make the connection explicit. For example, use IsolationPolicy , not Isolation Policy or isolation policy . If you are not referencing values or code directly, use normal sentence capitalization, for example, \"The isolation policy configuration takes place in a YAML file.\" Use italics to emphasize new terms Do Don't A cluster is a set of nodes ... A \"cluster\" is a set of nodes ... These components form the control plane . These components form the control plane . Use back-ticks around file names, directories, and paths Do Don't Open the foo.yaml file. Open the foo.yaml file. Go to the /content/docs/architecture directory. Go to the /content/docs/architecture directory. Open the /data/args.yaml file. Open the /data/args.yaml file. Use back-ticks around inline code and commands Do Don't The foo run command creates a Deployment . The \"foo run\" command creates a Deployment . For declarative management, use foo apply . For declarative management, use \"foo apply\". Use code-blocks for commands you intend readers to execute. Only use inline code and commands to mention specific labels, flags, values, functions, objects, variables, modules, or commands. Use back-ticks around object field names Do Don't Set the value of the ports field in the configuration file. Set the value of the \"ports\" field in the configuration file. The value of the rule field is a Rule object. The value of the \"rule\" field is a Rule object.","title":"Formatting Standards"},{"location":"contribute/documentation/formatting/#formatting-standards","text":"This page shows the formatting standards for the Fybrik documentation.","title":"Formatting Standards"},{"location":"contribute/documentation/formatting/#link-to-other-pages-using-relative-links","text":"When linking between pages in the documentation you can simply use the regular Markdown linking syntax, including the relative path to the Markdown document you wish to link to. For example: Please see the [ project license ]( license.md ) for further details. If the target documentation file is in another directory you'll need to make sure to include any relative directory path in the link: Please see the [ project license ]( ../about/license.md ) for further details.","title":"Link to other pages using relative links"},{"location":"contribute/documentation/formatting/#prefer-svg-format-for-diagrams","text":"Place image files in the docs/static directory. Use regular Markdown syntax for images. For example: ![](../static/myimage.svg) To make localization easier and enhance accessibility, the preferred image format is SVG. We recommend to use draw.io for creating images and diagrams. Use Export as to save your image in SVG format. Keep the Include a copy of my diagram option checked to allow later loading the SVG in draw.io and be sure to check Embed images if you diagram includes any. If your diagram depicts a process, try to avoid adding the descriptions of the steps to the diagram. Instead, only add the numbers of the steps to the diagram and add the descriptions of the steps as a numbered list in the document. Ensure that the numbers on the list match the numbers on your diagram. This approach helps make diagrams easier to understand and the content more accessible.","title":"Prefer SVG format for diagrams"},{"location":"contribute/documentation/formatting/#do-not-wrap-lines","text":"Never wrap lines after a fixed number of characters or in a middle of a senstence. Instead, configure your editor to soft-wrap when editing documentation. Do Don't This is a long line. This is a long line.","title":"Do not wrap lines"},{"location":"contribute/documentation/formatting/#use-angle-brackets-for-placeholders","text":"Use angle brackets for placeholders in commands or code samples. Tell the reader what the placeholder represents. For example: Display information about a pod: $ kubectl describe pod <pod-name> Where <pod-name> is the name of one of your pods.","title":"Use angle brackets for placeholders"},{"location":"contribute/documentation/formatting/#use-bold-to-emphasize-user-interface-elements","text":"Do Don't Click Fork . Click \"Fork\". Select Other . Select 'Other'.","title":"Use bold to emphasize user interface elements"},{"location":"contribute/documentation/formatting/#use-bold-to-emphasize-important-text","text":"Use bold to emphasize text that is particularly important. Avoid overusing bold as it reduces its impact and readability. Do Don't Examples of bad configurations: Examples of bad configurations : The maximum length of the name field is 256 characters . The maximum length of the name field is 256 characters .","title":"Use bold to emphasize important text"},{"location":"contribute/documentation/formatting/#dont-use-capitalization-for-emphasis","text":"Only use the original capitalization found in the code or configuration files when referencing those values directly. Use back-ticks `` around the referenced value to make the connection explicit. For example, use IsolationPolicy , not Isolation Policy or isolation policy . If you are not referencing values or code directly, use normal sentence capitalization, for example, \"The isolation policy configuration takes place in a YAML file.\"","title":"Don't use capitalization for emphasis"},{"location":"contribute/documentation/formatting/#use-italics-to-emphasize-new-terms","text":"Do Don't A cluster is a set of nodes ... A \"cluster\" is a set of nodes ... These components form the control plane . These components form the control plane .","title":"Use italics to emphasize new terms"},{"location":"contribute/documentation/formatting/#use-back-ticks-around-file-names-directories-and-paths","text":"Do Don't Open the foo.yaml file. Open the foo.yaml file. Go to the /content/docs/architecture directory. Go to the /content/docs/architecture directory. Open the /data/args.yaml file. Open the /data/args.yaml file.","title":"Use back-ticks around file names, directories, and paths"},{"location":"contribute/documentation/formatting/#use-back-ticks-around-inline-code-and-commands","text":"Do Don't The foo run command creates a Deployment . The \"foo run\" command creates a Deployment . For declarative management, use foo apply . For declarative management, use \"foo apply\". Use code-blocks for commands you intend readers to execute. Only use inline code and commands to mention specific labels, flags, values, functions, objects, variables, modules, or commands.","title":"Use back-ticks around inline code and commands"},{"location":"contribute/documentation/formatting/#use-back-ticks-around-object-field-names","text":"Do Don't Set the value of the ports field in the configuration file. Set the value of the \"ports\" field in the configuration file. The value of the rule field is a Rule object. The value of the \"rule\" field is a Rule object.","title":"Use back-ticks around object field names"},{"location":"contribute/documentation/style/","text":"Style Guide This page provides basic style guidance for keeping the documentation of Fybrik clear and understandable . Choose the right title Use a short, keyword-rich title that captures the intent of the document and draws the reader in. Ensure that the title clearly and concisely conveys the content or subject matter and is meaningful to a global audience. The text for the title of the document must use title case. Capitalize the first letter of every word except conjunctions and prepositions. Do Don't # Security Architecture # Security architecture # Code of Conduct # Code Of Conduct Use sentence case for headings Use sentence case for the headings in your document. Only capitalize the first word of the heading, except for proper nouns or acronyms. Do Don't Configuring rate limits Configuring Rate Limits Using Envoy for ingress Using envoy for ingress Using HTTPS Using https Use present tense Do Don't This command starts a proxy. This command will start a proxy. Exception: Use future or past tense if it is required to convey the correct meaning. This exception is extremely rare and should be avoided. Use active voice Do Don't You can explore the API using a browser. The API can be explored using a browser. The YAML file specifies the replica count. The replica count is specified in the YAML file. Use simple and direct language Use simple and direct language. Avoid using unnecessary phrases, such as saying \"please.\" Do Don't To create a ReplicaSet , ... In order to create a ReplicaSet , ... See the configuration file. Please see the configuration file. View the Pods. With this next command, we'll view the Pods. Prefer shorter words over longer alternatives Do Don't This tool helps scaling up pods. This tool facilitates scaling up pods. Pilot uses the purpose field to ... Pilot utilizes the purpose field to ... Address the reader as \"you\" Do Don't You can create a Deployment by ... We'll create a Deployment by ... In the preceding output, you can see... In the preceding output, we can see ... Avoid using \"we\" Using \"we\" in a sentence can be confusing, because the reader might not know whether they're part of the \"we\" you're describing. Do Don't Version 1.4 includes ... In version 1.4, we have added ... Fybrik provides a new feature for ... We provide a new feature ... This page teaches you how to use pods. In this page, we are going to learn about pods. Avoid jargon and idioms Some readers speak English as a second language. Avoid jargon and idioms to help make their understanding easier. Do Don't Internally, ... Under the hood, ... Create a new cluster. Turn up a new cluster. Initially, ... Out of the box, ... Avoid statements that will soon be out of date Avoid using wording that becomes outdated quickly like \"currently\" and \"new\". A feature that is new today is not new for long. Do Don't In version 1.4, ... In the current version, ... The Federation feature provides ... The new Federation feature provides ... Avoid statements about the future Avoid making promises or giving hints about the future. If you need to talk about a feature in development, add a clear indication under the front matter that identifies the information accordingly: Warning This page describes a feature that is not yet released The only exceptions to this rule are design or architecture documents that can describe a vision. However, you must clearly distiquish between implemented features and a vision. Create useful links There are good hyperlinks, and bad hyperlinks. The common practice of calling links here or click here are examples of bad hyperlinks. Check out this excellent article explaining what makes a good hyperlink and try to keep these guidelines in mind when creating or reviewing site content.","title":"Style Guide"},{"location":"contribute/documentation/style/#style-guide","text":"This page provides basic style guidance for keeping the documentation of Fybrik clear and understandable .","title":"Style Guide"},{"location":"contribute/documentation/style/#choose-the-right-title","text":"Use a short, keyword-rich title that captures the intent of the document and draws the reader in. Ensure that the title clearly and concisely conveys the content or subject matter and is meaningful to a global audience. The text for the title of the document must use title case. Capitalize the first letter of every word except conjunctions and prepositions. Do Don't # Security Architecture # Security architecture # Code of Conduct # Code Of Conduct","title":"Choose the right title"},{"location":"contribute/documentation/style/#use-sentence-case-for-headings","text":"Use sentence case for the headings in your document. Only capitalize the first word of the heading, except for proper nouns or acronyms. Do Don't Configuring rate limits Configuring Rate Limits Using Envoy for ingress Using envoy for ingress Using HTTPS Using https","title":"Use sentence case for headings"},{"location":"contribute/documentation/style/#use-present-tense","text":"Do Don't This command starts a proxy. This command will start a proxy. Exception: Use future or past tense if it is required to convey the correct meaning. This exception is extremely rare and should be avoided.","title":"Use present tense"},{"location":"contribute/documentation/style/#use-active-voice","text":"Do Don't You can explore the API using a browser. The API can be explored using a browser. The YAML file specifies the replica count. The replica count is specified in the YAML file.","title":"Use active voice"},{"location":"contribute/documentation/style/#use-simple-and-direct-language","text":"Use simple and direct language. Avoid using unnecessary phrases, such as saying \"please.\" Do Don't To create a ReplicaSet , ... In order to create a ReplicaSet , ... See the configuration file. Please see the configuration file. View the Pods. With this next command, we'll view the Pods.","title":"Use simple and direct language"},{"location":"contribute/documentation/style/#prefer-shorter-words-over-longer-alternatives","text":"Do Don't This tool helps scaling up pods. This tool facilitates scaling up pods. Pilot uses the purpose field to ... Pilot utilizes the purpose field to ...","title":"Prefer shorter words over longer alternatives"},{"location":"contribute/documentation/style/#address-the-reader-as-you","text":"Do Don't You can create a Deployment by ... We'll create a Deployment by ... In the preceding output, you can see... In the preceding output, we can see ...","title":"Address the reader as \"you\""},{"location":"contribute/documentation/style/#avoid-using-we","text":"Using \"we\" in a sentence can be confusing, because the reader might not know whether they're part of the \"we\" you're describing. Do Don't Version 1.4 includes ... In version 1.4, we have added ... Fybrik provides a new feature for ... We provide a new feature ... This page teaches you how to use pods. In this page, we are going to learn about pods.","title":"Avoid using \"we\""},{"location":"contribute/documentation/style/#avoid-jargon-and-idioms","text":"Some readers speak English as a second language. Avoid jargon and idioms to help make their understanding easier. Do Don't Internally, ... Under the hood, ... Create a new cluster. Turn up a new cluster. Initially, ... Out of the box, ...","title":"Avoid jargon and idioms"},{"location":"contribute/documentation/style/#avoid-statements-that-will-soon-be-out-of-date","text":"Avoid using wording that becomes outdated quickly like \"currently\" and \"new\". A feature that is new today is not new for long. Do Don't In version 1.4, ... In the current version, ... The Federation feature provides ... The new Federation feature provides ...","title":"Avoid statements that will soon be out of date"},{"location":"contribute/documentation/style/#avoid-statements-about-the-future","text":"Avoid making promises or giving hints about the future. If you need to talk about a feature in development, add a clear indication under the front matter that identifies the information accordingly: Warning This page describes a feature that is not yet released The only exceptions to this rule are design or architecture documents that can describe a vision. However, you must clearly distiquish between implemented features and a vision.","title":"Avoid statements about the future"},{"location":"contribute/documentation/style/#create-useful-links","text":"There are good hyperlinks, and bad hyperlinks. The common practice of calling links here or click here are examples of bad hyperlinks. Check out this excellent article explaining what makes a good hyperlink and try to keep these guidelines in mind when creating or reviewing site content.","title":"Create useful links"},{"location":"get-started/quickstart/","text":"Quick Start Guide Follow this guide to install Fybrik using default parameters that are suitable for experimentation on a single cluster. Before you begin Ensure that you have the following: Helm 3.3 or greater must be installed and configured on your machine. Kubectl 1.20 or newer must be installed on your machine. Access to a Kubernetes cluster such as Kind as a cluster administrator. Add required Helm repositories helm repo add jetstack https://charts.jetstack.io helm repo add hashicorp https://helm.releases.hashicorp.com helm repo add fybrik-charts https://fybrik.github.io/charts helm repo update Install cert-manager Fybrik requires cert-manager to be installed to your cluster 1 . Many clusters already include cert-manager. Check if cert-manager namespace exists in your cluster and only run the following if it doesn't exist: helm install cert-manager jetstack/cert-manager \\ --namespace cert-manager \\ --version v1.6.2 \\ --create-namespace \\ --set installCRDs = true \\ --wait --timeout 120s Install Hashicorp Vault and plugins Hashicorp Vault and a secrets-kubernetes-reader plugin are used by Fybrik for credential management. Install latest development version from GitHub The published Helm charts are only available for released versions. To install the dev version install the charts from the source code. For example: Kubernetes OpenShift git clone https://github.com/fybrik/fybrik.git cd fybrik helm dependency update charts/vault helm install vault charts/vault --create-namespace -n fybrik-system \\ --set \"vault.injector.enabled=false\" \\ --set \"vault.server.dev.enabled=true\" \\ --values charts/vault/env/dev/vault-single-cluster-values.yaml kubectl wait --for = condition = ready --all pod -n fybrik-system --timeout = 120s git clone https://github.com/fybrik/fybrik.git cd fybrik helm dependency update charts/vault helm install vault charts/vault --create-namespace -n fybrik-system \\ --set \"vault.global.openshift=true\" \\ --set \"vault.injector.enabled=false\" \\ --set \"vault.server.dev.enabled=true\" \\ --values charts/vault/env/dev/vault-single-cluster-values.yaml kubectl wait --for = condition = ready --all pod -n fybrik-system --timeout = 120s Run the following to install vault and the plugin in development mode: Kubernetes OpenShift helm install vault fybrik-charts/vault --create-namespace -n fybrik-system \\ --set \"vault.injector.enabled=false\" \\ --set \"vault.server.dev.enabled=true\" \\ --values https://raw.githubusercontent.com/fybrik/fybrik/master/charts/vault/env/dev/vault-single-cluster-values.yaml kubectl wait --for = condition = ready --all pod -n fybrik-system --timeout = 120s helm install vault fybrik-charts/vault --create-namespace -n fybrik-system \\ --set \"vault.global.openshift=true\" \\ --set \"vault.injector.enabled=false\" \\ --set \"vault.server.dev.enabled=true\" \\ --values https://raw.githubusercontent.com/fybrik/fybrik/master/charts/vault/env/dev/vault-single-cluster-values.yaml kubectl wait --for = condition = ready --all pod -n fybrik-system --timeout = 120s Install control plane Install latest development version from GitHub The published Helm charts are only available for released versions. To install the dev version install the charts from the source code. For example: git clone https://github.com/fybrik/fybrik.git cd fybrik helm install fybrik-crd charts/fybrik-crd -n fybrik-system --wait helm install fybrik charts/fybrik --set global.tag = master -n fybrik-system --wait The control plane includes a manager service that connects to a data catalog and to a policy manager. Install the Fybrik release with a built-in data catalog and with Open Policy Agent as the policy manager: helm install fybrik-crd fybrik-charts/fybrik-crd -n fybrik-system --version master --wait helm install fybrik fybrik-charts/fybrik -n fybrik-system --version master --wait Install modules Install latest development version from GitHub To apply the latest development version of arrow-flight-module: kubectl apply -f https://raw.githubusercontent.com/fybrik/arrow-flight-module/master/module.yaml -n fybrik-system Modules are plugins that the control plane deploys whenever required. The arrow flight module enables reading data through Apache Arrow Flight API. Install the latest 2 release of arrow-flight-module: kubectl apply -f https://github.com/fybrik/arrow-flight-module/releases/latest/download/module.yaml -n fybrik-system Fybrik version 0.6.0 and lower should use cert-manager 1.2.0 \u21a9 Refer to the documentation of arrow-flight-module for other versions \u21a9","title":"Quickstart"},{"location":"get-started/quickstart/#quick-start-guide","text":"Follow this guide to install Fybrik using default parameters that are suitable for experimentation on a single cluster.","title":"Quick Start Guide"},{"location":"get-started/quickstart/#before-you-begin","text":"Ensure that you have the following: Helm 3.3 or greater must be installed and configured on your machine. Kubectl 1.20 or newer must be installed on your machine. Access to a Kubernetes cluster such as Kind as a cluster administrator.","title":"Before you begin"},{"location":"get-started/quickstart/#add-required-helm-repositories","text":"helm repo add jetstack https://charts.jetstack.io helm repo add hashicorp https://helm.releases.hashicorp.com helm repo add fybrik-charts https://fybrik.github.io/charts helm repo update","title":"Add required Helm repositories"},{"location":"get-started/quickstart/#install-cert-manager","text":"Fybrik requires cert-manager to be installed to your cluster 1 . Many clusters already include cert-manager. Check if cert-manager namespace exists in your cluster and only run the following if it doesn't exist: helm install cert-manager jetstack/cert-manager \\ --namespace cert-manager \\ --version v1.6.2 \\ --create-namespace \\ --set installCRDs = true \\ --wait --timeout 120s","title":"Install cert-manager"},{"location":"get-started/quickstart/#install-hashicorp-vault-and-plugins","text":"Hashicorp Vault and a secrets-kubernetes-reader plugin are used by Fybrik for credential management. Install latest development version from GitHub The published Helm charts are only available for released versions. To install the dev version install the charts from the source code. For example: Kubernetes OpenShift git clone https://github.com/fybrik/fybrik.git cd fybrik helm dependency update charts/vault helm install vault charts/vault --create-namespace -n fybrik-system \\ --set \"vault.injector.enabled=false\" \\ --set \"vault.server.dev.enabled=true\" \\ --values charts/vault/env/dev/vault-single-cluster-values.yaml kubectl wait --for = condition = ready --all pod -n fybrik-system --timeout = 120s git clone https://github.com/fybrik/fybrik.git cd fybrik helm dependency update charts/vault helm install vault charts/vault --create-namespace -n fybrik-system \\ --set \"vault.global.openshift=true\" \\ --set \"vault.injector.enabled=false\" \\ --set \"vault.server.dev.enabled=true\" \\ --values charts/vault/env/dev/vault-single-cluster-values.yaml kubectl wait --for = condition = ready --all pod -n fybrik-system --timeout = 120s Run the following to install vault and the plugin in development mode: Kubernetes OpenShift helm install vault fybrik-charts/vault --create-namespace -n fybrik-system \\ --set \"vault.injector.enabled=false\" \\ --set \"vault.server.dev.enabled=true\" \\ --values https://raw.githubusercontent.com/fybrik/fybrik/master/charts/vault/env/dev/vault-single-cluster-values.yaml kubectl wait --for = condition = ready --all pod -n fybrik-system --timeout = 120s helm install vault fybrik-charts/vault --create-namespace -n fybrik-system \\ --set \"vault.global.openshift=true\" \\ --set \"vault.injector.enabled=false\" \\ --set \"vault.server.dev.enabled=true\" \\ --values https://raw.githubusercontent.com/fybrik/fybrik/master/charts/vault/env/dev/vault-single-cluster-values.yaml kubectl wait --for = condition = ready --all pod -n fybrik-system --timeout = 120s","title":"Install Hashicorp Vault and plugins"},{"location":"get-started/quickstart/#install-control-plane","text":"Install latest development version from GitHub The published Helm charts are only available for released versions. To install the dev version install the charts from the source code. For example: git clone https://github.com/fybrik/fybrik.git cd fybrik helm install fybrik-crd charts/fybrik-crd -n fybrik-system --wait helm install fybrik charts/fybrik --set global.tag = master -n fybrik-system --wait The control plane includes a manager service that connects to a data catalog and to a policy manager. Install the Fybrik release with a built-in data catalog and with Open Policy Agent as the policy manager: helm install fybrik-crd fybrik-charts/fybrik-crd -n fybrik-system --version master --wait helm install fybrik fybrik-charts/fybrik -n fybrik-system --version master --wait","title":"Install control plane"},{"location":"get-started/quickstart/#install-modules","text":"Install latest development version from GitHub To apply the latest development version of arrow-flight-module: kubectl apply -f https://raw.githubusercontent.com/fybrik/arrow-flight-module/master/module.yaml -n fybrik-system Modules are plugins that the control plane deploys whenever required. The arrow flight module enables reading data through Apache Arrow Flight API. Install the latest 2 release of arrow-flight-module: kubectl apply -f https://github.com/fybrik/arrow-flight-module/releases/latest/download/module.yaml -n fybrik-system Fybrik version 0.6.0 and lower should use cert-manager 1.2.0 \u21a9 Refer to the documentation of arrow-flight-module for other versions \u21a9","title":"Install modules"},{"location":"reference/connectors/","text":"Protocol Documentation Top credentials.proto Credentials Field Type Label Description access_key string access credential for the bucket where the asset is stored secret_key string username string password string api_key string api key assigned to the bucket in which the asset is stored resource_instance_id string resource instance id for the bucket Top data_catalog_request.proto CatalogDatasetRequest Field Type Label Description credential_path string link to vault plugin for reading k8s secret with user credentials dataset_id string identifier of asset - always needed. JSON expected. Interpreted by the Connector, can contain any additional information as part of JSON Top data_catalog_response.proto CatalogDatasetInfo Field Type Label Description dataset_id string details DatasetDetails Top data_catalog_service.proto DataCatalogService Method Name Request Type Response Type Description GetDatasetInfo CatalogDatasetRequest CatalogDatasetInfo RegisterDatasetInfo RegisterAssetRequest RegisterAssetResponse Top dataset_details.proto CredentialsInfo Field Type Label Description vault_secret_path string the path to Vault secret which is used to retrive the dataset credentials from the catalog. DataComponentMetadata Field Type Label Description component_type string e.g., column named_metadata DataComponentMetadata.NamedMetadataEntry repeated Named terms, that exist in Catalog toxonomy and the values for these terms for columns we will have \"SchemaDetails\" key, that will include technical schema details for this column TODO: Consider create special field for schema outside of metadata tags string repeated Tags - can be any free text added to a component (no taxonomy) DataComponentMetadata.NamedMetadataEntry Field Type Label Description key string value string DataStore Field Type Label Description type DataStore.DataStoreType name string for auditing and readability. Can be same as location type or can have more info if availble from catalog db2 Db2DataStore oneof location { // should have been oneof but for technical rasons, a problem to translate it to JSON, we remove the oneof for now should have been local, db2, s3 without \"location\" but had a problem to compile it in proto - collision with proto name DataLocationDb2 s3 S3DataStore kafka KafkaDataStore DatasetDetails Field Type Label Description name string name in Catalog data_owner string information on the owner of data asset - can have different formats for different catalogs data_store DataStore All info about the data store data_format string geo string geography location where data resides (if this information available) metadata DatasetMetadata LocationType locationType = 10; //publicCloud/privateCloud etc. Should be filled later when we understand better if we have a closed set of values and how they are used. credentials_info CredentialsInfo information about how to retrive dataset credentials from the catalog. DatasetMetadata Field Type Label Description dataset_named_metadata DatasetMetadata.DatasetNamedMetadataEntry repeated dataset_tags string repeated Tags - can be any free text added to a component (no taxonomy) components_metadata DatasetMetadata.ComponentsMetadataEntry repeated metadata for each component in asset. In tabular data each column is a component, then we will have: column name -> column metadata DatasetMetadata.ComponentsMetadataEntry Field Type Label Description key string value DataComponentMetadata DatasetMetadata.DatasetNamedMetadataEntry Field Type Label Description key string value string Db2DataStore Field Type Label Description url string database string table string reformat to SCHEMA.TABLE struct port string ssl string Note that bool value if set to \"false\" does not appear in the struct at all KafkaDataStore Field Type Label Description topic_name string bootstrap_servers string schema_registry string key_deserializer string value_deserializer string security_protocol string sasl_mechanism string ssl_truststore string ssl_truststore_password string S3DataStore Field Type Label Description endpoint string bucket string object_key string can be object name or the prefix for dataset region string WKC does not return it, it will stay empty in our case!!! DataStore.DataStoreType Name Number Description UNKNOWN 0 LOCAL 1 S3 2 DB2 3 KAFKA 4 Top policy_manager_request.proto AccessOperation Field Type Label Description type AccessOperation.AccessType destination string Destination for transfer or write. ApplicationContext Field Type Label Description credential_path string link to vault plugin for reading k8s secret with user credentials app_info ApplicationDetails datasets DatasetContext repeated general_operations AccessOperation repeated ApplicationDetails Field Type Label Description processing_geography string properties ApplicationDetails.PropertiesEntry repeated ApplicationDetails.PropertiesEntry Field Type Label Description key string value string DatasetContext Field Type Label Description dataset DatasetIdentifier operation AccessOperation DatasetIdentifier Field Type Label Description dataset_id string identifier of asset - always needed. JSON expected. Interpreted by the Connector, can contain any additional information as part of JSON AccessOperation.AccessType Name Number Description UNKNOWN 0 READ 1 COPY 2 WRITE 3 Top policy_manager_response.proto ComponentVersion Field Type Label Description name string id string version string DatasetDecision Field Type Label Description dataset DatasetIdentifier decisions OperationDecision repeated EnforcementAction Field Type Label Description name string id string level EnforcementAction.EnforcementActionLevel args EnforcementAction.ArgsEntry repeated EnforcementAction.ArgsEntry Field Type Label Description key string value string OperationDecision Field Type Label Description operation AccessOperation enforcement_actions EnforcementAction repeated used_policies Policy repeated PoliciesDecisions Field Type Label Description component_versions ComponentVersion repeated dataset_decisions DatasetDecision repeated one per dataset general_decisions OperationDecision repeated Policy Field Type Label Description id string name string description string type string hierarchy string repeated EnforcementAction.EnforcementActionLevel Name Number Description UNKNOWN 0 DATASET 1 COLUMN 2 ROW 3 CELL 4 Top policy_manager_service.proto PolicyManagerService Method Name Request Type Response Type Description GetPoliciesDecisions ApplicationContext PoliciesDecisions Top register_asset_request.proto RegisterAssetRequest Field Type Label Description creds Credentials dataset_details DatasetDetails destination_catalog_id string credential_path string link to vault plugin for reading k8s secret with user credentials Top register_asset_response.proto RegisterAssetResponse Field Type Label Description asset_id string Returns the id of the new asset registered in a catalog Scalar Value Types .proto Type Notes C++ Java Python Go C# PHP Ruby double double double float float64 double float Float float float float float float32 float float Float int32 Uses variable-length encoding. Inefficient for encoding negative numbers \u2013 if your field is likely to have negative values, use sint32 instead. int32 int int int32 int integer Bignum or Fixnum (as required) int64 Uses variable-length encoding. Inefficient for encoding negative numbers \u2013 if your field is likely to have negative values, use sint64 instead. int64 long int/long int64 long integer/string Bignum uint32 Uses variable-length encoding. uint32 int int/long uint32 uint integer Bignum or Fixnum (as required) uint64 Uses variable-length encoding. uint64 long int/long uint64 ulong integer/string Bignum or Fixnum (as required) sint32 Uses variable-length encoding. Signed int value. These more efficiently encode negative numbers than regular int32s. int32 int int int32 int integer Bignum or Fixnum (as required) sint64 Uses variable-length encoding. Signed int value. These more efficiently encode negative numbers than regular int64s. int64 long int/long int64 long integer/string Bignum fixed32 Always four bytes. More efficient than uint32 if values are often greater than 2^28. uint32 int int uint32 uint integer Bignum or Fixnum (as required) fixed64 Always eight bytes. More efficient than uint64 if values are often greater than 2^56. uint64 long int/long uint64 ulong integer/string Bignum sfixed32 Always four bytes. int32 int int int32 int integer Bignum or Fixnum (as required) sfixed64 Always eight bytes. int64 long int/long int64 long integer/string Bignum bool bool boolean boolean bool bool boolean TrueClass/FalseClass string A string must always contain UTF-8 encoded or 7-bit ASCII text. string String str/unicode string string string String (UTF-8) bytes May contain any arbitrary sequence of bytes. string ByteString str []byte ByteString string String (ASCII-8BIT)","title":"Protocol Documentation"},{"location":"reference/connectors/#protocol-documentation","text":"Top","title":"Protocol Documentation"},{"location":"reference/connectors/#credentialsproto","text":"","title":"credentials.proto"},{"location":"reference/connectors/#credentials","text":"Field Type Label Description access_key string access credential for the bucket where the asset is stored secret_key string username string password string api_key string api key assigned to the bucket in which the asset is stored resource_instance_id string resource instance id for the bucket Top","title":"Credentials"},{"location":"reference/connectors/#data_catalog_requestproto","text":"","title":"data_catalog_request.proto"},{"location":"reference/connectors/#catalogdatasetrequest","text":"Field Type Label Description credential_path string link to vault plugin for reading k8s secret with user credentials dataset_id string identifier of asset - always needed. JSON expected. Interpreted by the Connector, can contain any additional information as part of JSON Top","title":"CatalogDatasetRequest"},{"location":"reference/connectors/#data_catalog_responseproto","text":"","title":"data_catalog_response.proto"},{"location":"reference/connectors/#catalogdatasetinfo","text":"Field Type Label Description dataset_id string details DatasetDetails Top","title":"CatalogDatasetInfo"},{"location":"reference/connectors/#data_catalog_serviceproto","text":"","title":"data_catalog_service.proto"},{"location":"reference/connectors/#datacatalogservice","text":"Method Name Request Type Response Type Description GetDatasetInfo CatalogDatasetRequest CatalogDatasetInfo RegisterDatasetInfo RegisterAssetRequest RegisterAssetResponse Top","title":"DataCatalogService"},{"location":"reference/connectors/#dataset_detailsproto","text":"","title":"dataset_details.proto"},{"location":"reference/connectors/#credentialsinfo","text":"Field Type Label Description vault_secret_path string the path to Vault secret which is used to retrive the dataset credentials from the catalog.","title":"CredentialsInfo"},{"location":"reference/connectors/#datacomponentmetadata","text":"Field Type Label Description component_type string e.g., column named_metadata DataComponentMetadata.NamedMetadataEntry repeated Named terms, that exist in Catalog toxonomy and the values for these terms for columns we will have \"SchemaDetails\" key, that will include technical schema details for this column TODO: Consider create special field for schema outside of metadata tags string repeated Tags - can be any free text added to a component (no taxonomy)","title":"DataComponentMetadata"},{"location":"reference/connectors/#datacomponentmetadatanamedmetadataentry","text":"Field Type Label Description key string value string","title":"DataComponentMetadata.NamedMetadataEntry"},{"location":"reference/connectors/#datastore","text":"Field Type Label Description type DataStore.DataStoreType name string for auditing and readability. Can be same as location type or can have more info if availble from catalog db2 Db2DataStore oneof location { // should have been oneof but for technical rasons, a problem to translate it to JSON, we remove the oneof for now should have been local, db2, s3 without \"location\" but had a problem to compile it in proto - collision with proto name DataLocationDb2 s3 S3DataStore kafka KafkaDataStore","title":"DataStore"},{"location":"reference/connectors/#datasetdetails","text":"Field Type Label Description name string name in Catalog data_owner string information on the owner of data asset - can have different formats for different catalogs data_store DataStore All info about the data store data_format string geo string geography location where data resides (if this information available) metadata DatasetMetadata LocationType locationType = 10; //publicCloud/privateCloud etc. Should be filled later when we understand better if we have a closed set of values and how they are used. credentials_info CredentialsInfo information about how to retrive dataset credentials from the catalog.","title":"DatasetDetails"},{"location":"reference/connectors/#datasetmetadata","text":"Field Type Label Description dataset_named_metadata DatasetMetadata.DatasetNamedMetadataEntry repeated dataset_tags string repeated Tags - can be any free text added to a component (no taxonomy) components_metadata DatasetMetadata.ComponentsMetadataEntry repeated metadata for each component in asset. In tabular data each column is a component, then we will have: column name -> column metadata","title":"DatasetMetadata"},{"location":"reference/connectors/#datasetmetadatacomponentsmetadataentry","text":"Field Type Label Description key string value DataComponentMetadata","title":"DatasetMetadata.ComponentsMetadataEntry"},{"location":"reference/connectors/#datasetmetadatadatasetnamedmetadataentry","text":"Field Type Label Description key string value string","title":"DatasetMetadata.DatasetNamedMetadataEntry"},{"location":"reference/connectors/#db2datastore","text":"Field Type Label Description url string database string table string reformat to SCHEMA.TABLE struct port string ssl string Note that bool value if set to \"false\" does not appear in the struct at all","title":"Db2DataStore"},{"location":"reference/connectors/#kafkadatastore","text":"Field Type Label Description topic_name string bootstrap_servers string schema_registry string key_deserializer string value_deserializer string security_protocol string sasl_mechanism string ssl_truststore string ssl_truststore_password string","title":"KafkaDataStore"},{"location":"reference/connectors/#s3datastore","text":"Field Type Label Description endpoint string bucket string object_key string can be object name or the prefix for dataset region string WKC does not return it, it will stay empty in our case!!!","title":"S3DataStore"},{"location":"reference/connectors/#datastoredatastoretype","text":"Name Number Description UNKNOWN 0 LOCAL 1 S3 2 DB2 3 KAFKA 4 Top","title":"DataStore.DataStoreType"},{"location":"reference/connectors/#policy_manager_requestproto","text":"","title":"policy_manager_request.proto"},{"location":"reference/connectors/#accessoperation","text":"Field Type Label Description type AccessOperation.AccessType destination string Destination for transfer or write.","title":"AccessOperation"},{"location":"reference/connectors/#applicationcontext","text":"Field Type Label Description credential_path string link to vault plugin for reading k8s secret with user credentials app_info ApplicationDetails datasets DatasetContext repeated general_operations AccessOperation repeated","title":"ApplicationContext"},{"location":"reference/connectors/#applicationdetails","text":"Field Type Label Description processing_geography string properties ApplicationDetails.PropertiesEntry repeated","title":"ApplicationDetails"},{"location":"reference/connectors/#applicationdetailspropertiesentry","text":"Field Type Label Description key string value string","title":"ApplicationDetails.PropertiesEntry"},{"location":"reference/connectors/#datasetcontext","text":"Field Type Label Description dataset DatasetIdentifier operation AccessOperation","title":"DatasetContext"},{"location":"reference/connectors/#datasetidentifier","text":"Field Type Label Description dataset_id string identifier of asset - always needed. JSON expected. Interpreted by the Connector, can contain any additional information as part of JSON","title":"DatasetIdentifier"},{"location":"reference/connectors/#accessoperationaccesstype","text":"Name Number Description UNKNOWN 0 READ 1 COPY 2 WRITE 3 Top","title":"AccessOperation.AccessType"},{"location":"reference/connectors/#policy_manager_responseproto","text":"","title":"policy_manager_response.proto"},{"location":"reference/connectors/#componentversion","text":"Field Type Label Description name string id string version string","title":"ComponentVersion"},{"location":"reference/connectors/#datasetdecision","text":"Field Type Label Description dataset DatasetIdentifier decisions OperationDecision repeated","title":"DatasetDecision"},{"location":"reference/connectors/#enforcementaction","text":"Field Type Label Description name string id string level EnforcementAction.EnforcementActionLevel args EnforcementAction.ArgsEntry repeated","title":"EnforcementAction"},{"location":"reference/connectors/#enforcementactionargsentry","text":"Field Type Label Description key string value string","title":"EnforcementAction.ArgsEntry"},{"location":"reference/connectors/#operationdecision","text":"Field Type Label Description operation AccessOperation enforcement_actions EnforcementAction repeated used_policies Policy repeated","title":"OperationDecision"},{"location":"reference/connectors/#policiesdecisions","text":"Field Type Label Description component_versions ComponentVersion repeated dataset_decisions DatasetDecision repeated one per dataset general_decisions OperationDecision repeated","title":"PoliciesDecisions"},{"location":"reference/connectors/#policy","text":"Field Type Label Description id string name string description string type string hierarchy string repeated","title":"Policy"},{"location":"reference/connectors/#enforcementactionenforcementactionlevel","text":"Name Number Description UNKNOWN 0 DATASET 1 COLUMN 2 ROW 3 CELL 4 Top","title":"EnforcementAction.EnforcementActionLevel"},{"location":"reference/connectors/#policy_manager_serviceproto","text":"","title":"policy_manager_service.proto"},{"location":"reference/connectors/#policymanagerservice","text":"Method Name Request Type Response Type Description GetPoliciesDecisions ApplicationContext PoliciesDecisions Top","title":"PolicyManagerService"},{"location":"reference/connectors/#register_asset_requestproto","text":"","title":"register_asset_request.proto"},{"location":"reference/connectors/#registerassetrequest","text":"Field Type Label Description creds Credentials dataset_details DatasetDetails destination_catalog_id string credential_path string link to vault plugin for reading k8s secret with user credentials Top","title":"RegisterAssetRequest"},{"location":"reference/connectors/#register_asset_responseproto","text":"","title":"register_asset_response.proto"},{"location":"reference/connectors/#registerassetresponse","text":"Field Type Label Description asset_id string Returns the id of the new asset registered in a catalog","title":"RegisterAssetResponse"},{"location":"reference/connectors/#scalar-value-types","text":".proto Type Notes C++ Java Python Go C# PHP Ruby double double double float float64 double float Float float float float float float32 float float Float int32 Uses variable-length encoding. Inefficient for encoding negative numbers \u2013 if your field is likely to have negative values, use sint32 instead. int32 int int int32 int integer Bignum or Fixnum (as required) int64 Uses variable-length encoding. Inefficient for encoding negative numbers \u2013 if your field is likely to have negative values, use sint64 instead. int64 long int/long int64 long integer/string Bignum uint32 Uses variable-length encoding. uint32 int int/long uint32 uint integer Bignum or Fixnum (as required) uint64 Uses variable-length encoding. uint64 long int/long uint64 ulong integer/string Bignum or Fixnum (as required) sint32 Uses variable-length encoding. Signed int value. These more efficiently encode negative numbers than regular int32s. int32 int int int32 int integer Bignum or Fixnum (as required) sint64 Uses variable-length encoding. Signed int value. These more efficiently encode negative numbers than regular int64s. int64 long int/long int64 long integer/string Bignum fixed32 Always four bytes. More efficient than uint32 if values are often greater than 2^28. uint32 int int uint32 uint integer Bignum or Fixnum (as required) fixed64 Always eight bytes. More efficient than uint64 if values are often greater than 2^56. uint64 long int/long uint64 ulong integer/string Bignum sfixed32 Always four bytes. int32 int int int32 int integer Bignum or Fixnum (as required) sfixed64 Always eight bytes. int64 long int/long int64 long integer/string Bignum bool bool boolean boolean bool bool boolean TrueClass/FalseClass string A string must always contain UTF-8 encoded or 7-bit ASCII text. string String str/unicode string string string String (UTF-8) bytes May contain any arbitrary sequence of bytes. string ByteString str []byte ByteString string String (ASCII-8BIT)","title":"Scalar Value Types"},{"location":"reference/crds/","text":"API Reference Packages: app.fybrik.io/v1alpha1 katalog.fybrik.io/v1alpha1 app.fybrik.io/v1alpha1 Resource Types: Blueprint FybrikApplication FybrikModule FybrikStorageAccount Plotter Blueprint \u21a9 Parent Blueprint is the Schema for the blueprints API Name Type Description Required apiVersion string app.fybrik.io/v1alpha1 true kind string Blueprint true metadata object Refer to the Kubernetes API documentation for the fields of the `metadata` field. true spec object BlueprintSpec defines the desired state of Blueprint, which defines the components of the workload's data path that run in a particular cluster. In a single cluster environment there is one blueprint per workload (FybrikApplication). In a multi-cluster environment there is one Blueprint per cluster per workload (FybrikApplication). true status object BlueprintStatus defines the observed state of Blueprint This includes readiness, error message, and indicators for the Kubernetes resources owned by the Blueprint for cleanup and status monitoring false Blueprint.spec \u21a9 Parent BlueprintSpec defines the desired state of Blueprint, which defines the components of the workload's data path that run in a particular cluster. In a single cluster environment there is one blueprint per workload (FybrikApplication). In a multi-cluster environment there is one Blueprint per cluster per workload (FybrikApplication). Name Type Description Required cluster string Cluster indicates the cluster on which the Blueprint runs true modules map[string]object Modules is a map which contains modules that indicate the data path components that run in this cluster The map key is moduleInstanceName which is the unique name for the deployed instance related to this workload true modulesNamespace string ModulesNamespace is the namespace where modules should be allocated true application object ApplicationContext is a context of the origin FybrikApplication (labels, properties, etc.) false Blueprint.spec.modules[key] \u21a9 Parent BlueprintModule is a copy of a FybrikModule Custom Resource. It contains the information necessary to instantiate a datapath component, including the parameters relevant for the particular workload. Name Type Description Required chart object Chart contains the location of the helm chart with info detailing how to deploy true name string Name of the FybrikModule on which this is based true arguments object Arguments are the input parameters for a specific instance of a module. false assetIds []string assetIDs indicate the assets processed by this module. Included so we can track asset status as well as module status in the future. false Blueprint.spec.modules[key].chart \u21a9 Parent Chart contains the location of the helm chart with info detailing how to deploy Name Type Description Required name string Name of helm chart true chartPullSecret string Name of secret containing helm registry credentials false values map[string]string Values to pass to helm chart installation false Blueprint.spec.modules[key].arguments \u21a9 Parent Arguments are the input parameters for a specific instance of a module. Name Type Description Required assets []object Assets define asset related arguments, such as data source, transformations, etc. false Blueprint.spec.modules[key].arguments.assets[index] \u21a9 Parent AssetContext defines the input parameters for modules that access an asset Name Type Description Required assetID string AssetID identifies the asset to be used for accessing the data when it is ready It is copied from the FybrikApplication resource true capability string Capability of the module true args []object List of datastores associated with the asset false transformations []object Transformations are different types of processing that may be done to the data as it is copied. false Blueprint.spec.modules[key].arguments.assets[index].args[index] \u21a9 Parent DataStore contains the details for accesing the data that are sent by catalog connectors Credentials for accesing the data are stored in Vault, in the location represented by Vault property. Name Type Description Required connection object Connection has the relevant details for accesing the data (url, table, ssl, etc.) true format string Format represents data format (e.g. parquet) as received from catalog connectors false vault map[string]object Holds details for retrieving credentials by the modules from Vault store. It is a map so that different credentials can be stored for the different DataFlow operations. false Blueprint.spec.modules[key].arguments.assets[index].args[index].connection \u21a9 Parent Connection has the relevant details for accesing the data (url, table, ssl, etc.) Name Type Description Required name string true Blueprint.spec.modules[key].arguments.assets[index].args[index].vault[key] \u21a9 Parent Holds details for retrieving credentials from Vault store. Name Type Description Required address string Address is Vault address true authPath string AuthPath is the path to auth method i.e. kubernetes true role string Role is the Vault role used for retrieving the credentials true secretPath string SecretPath is the path of the secret holding the Credentials in Vault true Blueprint.spec.modules[key].arguments.assets[index].transformations[index] \u21a9 Parent Name Type Description Required name string true Blueprint.spec.application \u21a9 Parent ApplicationContext is a context of the origin FybrikApplication (labels, properties, etc.) Name Type Description Required context object Application context such as intent, role, etc. false selector object Application selector is used to identify the user workload. It is obtained from FybrikApplication spec. false Blueprint.spec.application.selector \u21a9 Parent Application selector is used to identify the user workload. It is obtained from FybrikApplication spec. Name Type Description Required matchExpressions []object matchExpressions is a list of label selector requirements. The requirements are ANDed. false matchLabels map[string]string matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels map is equivalent to an element of matchExpressions, whose key field is \"key\", the operator is \"In\", and the values array contains only \"value\". The requirements are ANDed. false Blueprint.spec.application.selector.matchExpressions[index] \u21a9 Parent A label selector requirement is a selector that contains values, a key, and an operator that relates the key and values. Name Type Description Required key string key is the label key that the selector applies to. true operator string operator represents a key's relationship to a set of values. Valid operators are In, NotIn, Exists and DoesNotExist. true values []string values is an array of string values. If the operator is In or NotIn, the values array must be non-empty. If the operator is Exists or DoesNotExist, the values array must be empty. This array is replaced during a strategic merge patch. false Blueprint.status \u21a9 Parent BlueprintStatus defines the observed state of Blueprint This includes readiness, error message, and indicators for the Kubernetes resources owned by the Blueprint for cleanup and status monitoring Name Type Description Required modules map[string]object ModulesState is a map which holds the status of each module its key is the moduleInstanceName which is the unique name for the deployed instance related to this workload false observedGeneration integer ObservedGeneration is taken from the Blueprint metadata. This is used to determine during reconcile whether reconcile was called because the desired state changed, or whether status of the allocated resources should be checked. Format : int64 false observedState object ObservedState includes information to be reported back to the FybrikApplication resource It includes readiness and error indications, as well as user instructions false releases map[string]integer Releases map each release to the observed generation of the blueprint containing this release. At the end of reconcile, each release should be mapped to the latest blueprint version or be uninstalled. false Blueprint.status.modules[key] \u21a9 Parent ObservedState represents a part of the generated Blueprint/Plotter resource status that allows update of FybrikApplication status Name Type Description Required error string Error indicates that there has been an error to orchestrate the modules and provides the error message false ready boolean Ready represents that the modules have been orchestrated successfully and the data is ready for usage false Blueprint.status.observedState \u21a9 Parent ObservedState includes information to be reported back to the FybrikApplication resource It includes readiness and error indications, as well as user instructions Name Type Description Required error string Error indicates that there has been an error to orchestrate the modules and provides the error message false ready boolean Ready represents that the modules have been orchestrated successfully and the data is ready for usage false FybrikApplication \u21a9 Parent FybrikApplication provides information about the application whose data is being operated on, the nature of the processing, and the data sets chosen for processing by the application. The FybrikApplication controller obtains instructions regarding any governance related changes that must be performed on the data, identifies the modules capable of performing such changes, and finally generates the Plotter which defines the secure runtime environment and all the components in it. This runtime environment provides the application with access to the data requested in a secure manner and without having to provide any credentials for the data sets. The credentials are obtained automatically by the manager from the credential management system. Name Type Description Required apiVersion string app.fybrik.io/v1alpha1 true kind string FybrikApplication true metadata object Refer to the Kubernetes API documentation for the fields of the `metadata` field. true spec object FybrikApplicationSpec defines data flows needed by the application, the purpose and other contextual information about the application. true status object FybrikApplicationStatus defines the observed state of FybrikApplication. false FybrikApplication.spec \u21a9 Parent FybrikApplicationSpec defines data flows needed by the application, the purpose and other contextual information about the application. Name Type Description Required appInfo object AppInfo contains information describing the reasons for the processing that will be done by the application. true data []object Data contains the identifiers of the data to be used by the Data Scientist's application, and the protocol used to access it and the format expected. true secretRef string SecretRef points to the secret that holds credentials for each system the user has been authenticated with. The secret is deployed in FybrikApplication namespace. false selector object Selector enables to connect the resource to the application Application labels should match the labels in the selector. false FybrikApplication.spec.data[index] \u21a9 Parent DataContext indicates data set being processed by the workload and includes information about the data format and technologies used to access the data. Name Type Description Required dataSetID string DataSetID is a unique identifier of the dataset chosen from the data catalog. For data catalogs that support multiple sub-catalogs, it includes the catalog id and the dataset id. When writing a new dataset it is the name provided by the user or workload generating it. true requirements object Requirements from the system true flow enum Flows indicates what is being done with the particular dataset - ex: read, write, copy (ingest), delete This is optional for the purpose of backward compatibility. If nothing is provided, read is assumed. Enum : read, write, delete, copy false FybrikApplication.spec.data[index].requirements \u21a9 Parent Requirements from the system Name Type Description Required flowParams object FlowParams include the requirements for particular data flows false interface object Interface indicates the protocol and format expected by the data user false FybrikApplication.spec.data[index].requirements.flowParams \u21a9 Parent FlowParams include the requirements for particular data flows Name Type Description Required catalog string Catalog indicates that the data asset must be cataloged, and in which catalog to register it false isNewDataSet boolean IsNewDataSet if true indicates that the DataContext.DataSetID is user provided and not a full catalog / dataset ID. Relevant when writing. A unique ID from the catalog will be provided in the FybrikApplication Status after a new catalog entry is created. false metadata object Source asset metadata like asset name, owner, geography, etc Relevant when writing new asset. false storageEstimate integer Storage estimate indicates the estimated amount of storage in MB, GB, TB required when writing new data. Format : int64 false FybrikApplication.spec.data[index].requirements.flowParams.metadata \u21a9 Parent Source asset metadata like asset name, owner, geography, etc Relevant when writing new asset. Name Type Description Required columns []object Columns associated with the asset false geography string Geography of the resource false name string Name of the resource false owner string Owner of the resource false tags object Tags associated with the asset false FybrikApplication.spec.data[index].requirements.flowParams.metadata.columns[index] \u21a9 Parent ResourceColumn represents a column in a tabular resource Name Type Description Required name string Name of the column true tags object Tags associated with the column false FybrikApplication.spec.data[index].requirements.interface \u21a9 Parent Interface indicates the protocol and format expected by the data user Name Type Description Required protocol string Protocol defines the interface protocol used for data transactions true dataformat string DataFormat defines the data format type false FybrikApplication.spec.selector \u21a9 Parent Selector enables to connect the resource to the application Application labels should match the labels in the selector. Name Type Description Required workloadSelector object WorkloadSelector enables to connect the resource to the application Application labels should match the labels in the selector. true clusterName string Cluster name false FybrikApplication.spec.selector.workloadSelector \u21a9 Parent WorkloadSelector enables to connect the resource to the application Application labels should match the labels in the selector. Name Type Description Required matchExpressions []object matchExpressions is a list of label selector requirements. The requirements are ANDed. false matchLabels map[string]string matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels map is equivalent to an element of matchExpressions, whose key field is \"key\", the operator is \"In\", and the values array contains only \"value\". The requirements are ANDed. false FybrikApplication.spec.selector.workloadSelector.matchExpressions[index] \u21a9 Parent A label selector requirement is a selector that contains values, a key, and an operator that relates the key and values. Name Type Description Required key string key is the label key that the selector applies to. true operator string operator represents a key's relationship to a set of values. Valid operators are In, NotIn, Exists and DoesNotExist. true values []string values is an array of string values. If the operator is In or NotIn, the values array must be non-empty. If the operator is Exists or DoesNotExist, the values array must be empty. This array is replaced during a strategic merge patch. false FybrikApplication.status \u21a9 Parent FybrikApplicationStatus defines the observed state of FybrikApplication. Name Type Description Required assetStates map[string]object AssetStates provides a status per asset false errorMessage string ErrorMessage indicates that an error has happened during the reconcile, unrelated to a specific asset false generated object Generated resource identifier false observedGeneration integer ObservedGeneration is taken from the FybrikApplication metadata. This is used to determine during reconcile whether reconcile was called because the desired state changed, or whether the Blueprint status changed. Format : int64 false provisionedStorage map[string]object ProvisionedStorage maps a dataset (identified by AssetID) to the new provisioned bucket. It allows FybrikApplication controller to manage buckets in case the spec has been modified, an error has occurred, or a delete event has been received. ProvisionedStorage has the information required to register the dataset once the owned plotter resource is ready false ready boolean Ready is true if all specified assets are either ready to be used or are denied access. false validApplication string ValidApplication indicates whether the FybrikApplication is valid given the defined taxonomy false validatedGeneration integer ValidatedGeneration is the version of the FyrbikApplication that has been validated with the taxonomy defined. Format : int64 false FybrikApplication.status.assetStates[key] \u21a9 Parent AssetState defines the observed state of an asset Name Type Description Required catalogedAsset string CatalogedAsset provides a new asset identifier after being registered in the enterprise catalog false conditions []object Conditions indicate the asset state (Ready, Deny, Error) false endpoint object Endpoint provides the endpoint spec from which the asset will be served to the application false FybrikApplication.status.assetStates[key].conditions[index] \u21a9 Parent Condition describes the state of a FybrikApplication at a certain point. Name Type Description Required type string Type of the condition true message string Message contains the details of the current condition false observedGeneration integer ObservedGeneration is the version of the resource for which the condition has been evaluated Format : int64 false status enum Status of the condition, one of (`True`, `False`, `Unknown`). Enum : True, False, Unknown Default : Unknown false FybrikApplication.status.assetStates[key].endpoint \u21a9 Parent Endpoint provides the endpoint spec from which the asset will be served to the application Name Type Description Required name string true FybrikApplication.status.generated \u21a9 Parent Generated resource identifier Name Type Description Required appVersion integer Version of FybrikApplication that has generated this resource Format : int64 true kind string Kind of the resource (Blueprint, Plotter) true name string Name of the resource true namespace string Namespace of the resource true FybrikApplication.status.provisionedStorage[key] \u21a9 Parent DatasetDetails holds details of the provisioned storage Name Type Description Required datasetRef string Reference to a Dataset resource containing the request to provision storage false details object Dataset information false resourceMetadata object Resource Metadata false secretRef object Reference to a secret where the credentials are stored false FybrikApplication.status.provisionedStorage[key].details \u21a9 Parent Dataset information Name Type Description Required connection object Connection has the relevant details for accesing the data (url, table, ssl, etc.) true format string Format represents data format (e.g. parquet) as received from catalog connectors false vault map[string]object Holds details for retrieving credentials by the modules from Vault store. It is a map so that different credentials can be stored for the different DataFlow operations. false FybrikApplication.status.provisionedStorage[key].details.connection \u21a9 Parent Connection has the relevant details for accesing the data (url, table, ssl, etc.) Name Type Description Required name string true FybrikApplication.status.provisionedStorage[key].details.vault[key] \u21a9 Parent Holds details for retrieving credentials from Vault store. Name Type Description Required address string Address is Vault address true authPath string AuthPath is the path to auth method i.e. kubernetes true role string Role is the Vault role used for retrieving the credentials true secretPath string SecretPath is the path of the secret holding the Credentials in Vault true FybrikApplication.status.provisionedStorage[key].resourceMetadata \u21a9 Parent Resource Metadata Name Type Description Required columns []object Columns associated with the asset false geography string Geography of the resource false name string Name of the resource false owner string Owner of the resource false tags object Tags associated with the asset false FybrikApplication.status.provisionedStorage[key].resourceMetadata.columns[index] \u21a9 Parent ResourceColumn represents a column in a tabular resource Name Type Description Required name string Name of the column true tags object Tags associated with the column false FybrikApplication.status.provisionedStorage[key].secretRef \u21a9 Parent Reference to a secret where the credentials are stored Name Type Description Required name string Secret name true namespace string Secret Namespace true FybrikModule \u21a9 Parent FybrikModule is a description of an injectable component. the parameters it requires, as well as the specification of how to instantiate such a component. It is used as metadata only. There is no status nor reconciliation. Name Type Description Required apiVersion string app.fybrik.io/v1alpha1 true kind string FybrikModule true metadata object Refer to the Kubernetes API documentation for the fields of the `metadata` field. true spec object FybrikModuleSpec contains the info common to all modules, which are one of the components that process, load, write, audit, monitor the data used by the data scientist's application. true status object FybrikModuleStatus defines the observed state of FybrikModule. false FybrikModule.spec \u21a9 Parent FybrikModuleSpec contains the info common to all modules, which are one of the components that process, load, write, audit, monitor the data used by the data scientist's application. Name Type Description Required capabilities []object Capabilities declares what this module knows how to do and the types of data it knows how to handle The key to the map is a CapabilityType string true chart object Reference to a Helm chart that allows deployment of the resources required for this module true type string May be one of service, config or plugin Service: Means that the control plane deploys the component that performs the capability Config: Another pre-installed service performs the capability and the module deployed configures it for the particular workload or dataset Plugin: Indicates that this module performs a capability as part of another service or module rather than as a stand-alone module true dependencies []object Other components that must be installed in order for this module to work false description string An explanation of what this module does false pluginType string Plugin type indicates the plugin technology used to invoke the capabilities Ex: vault, fybrik-wasm... Should be provided if type is plugin false statusIndicators []object StatusIndicators allow to check status of a non-standard resource that can not be computed by helm/kstatus false FybrikModule.spec.capabilities[index] \u21a9 Parent Capability declares what this module knows how to do and the types of data it knows how to handle Name Type Description Required capability string Capability declares what this module knows how to do - ex: read, write, transform... true actions []object Actions are the data transformations that the module supports false api object API indicates to the application how to access the capabilities provided by the module false plugins []object Plugins enable the module to add libraries to perform actions rather than implementing them by itself false scope enum Scope indicates at what level the capability is used: workload, asset, cluster If not indicated it is assumed to be asset Enum : asset, workload, cluster false supportedInterfaces []object Copy should have one or more instances in the list, and its content should have source and sink Read should have one or more instances in the list, each with source populated Write should have one or more instances in the list, each with sink populated This field may not be required if not handling data false FybrikModule.spec.capabilities[index].actions[index] \u21a9 Parent Name Type Description Required name string Unique name of an action supported by the module true FybrikModule.spec.capabilities[index].api \u21a9 Parent API indicates to the application how to access the capabilities provided by the module Name Type Description Required connection object Connection information true dataFormat string Data format false FybrikModule.spec.capabilities[index].api.connection \u21a9 Parent Connection information Name Type Description Required name string true FybrikModule.spec.capabilities[index].plugins[index] \u21a9 Parent Name Type Description Required dataFormat string DataFormat indicates the format of data the plugin knows how to process true pluginType string PluginType indicates the technology used for the module and the plugin to interact The values supported should come from the module taxonomy Examples of such mechanisms are vault plugins, wasm, etc true FybrikModule.spec.capabilities[index].supportedInterfaces[index] \u21a9 Parent ModuleInOut specifies the protocol and format of the data input and output by the module - if any Name Type Description Required sink object Sink specifies the output data protocol and format false source object Source specifies the input data protocol and format false FybrikModule.spec.capabilities[index].supportedInterfaces[index].sink \u21a9 Parent Sink specifies the output data protocol and format Name Type Description Required protocol string Protocol defines the interface protocol used for data transactions true dataformat string DataFormat defines the data format type false FybrikModule.spec.capabilities[index].supportedInterfaces[index].source \u21a9 Parent Source specifies the input data protocol and format Name Type Description Required protocol string Protocol defines the interface protocol used for data transactions true dataformat string DataFormat defines the data format type false FybrikModule.spec.chart \u21a9 Parent Reference to a Helm chart that allows deployment of the resources required for this module Name Type Description Required name string Name of helm chart true chartPullSecret string Name of secret containing helm registry credentials false values map[string]string Values to pass to helm chart installation false FybrikModule.spec.dependencies[index] \u21a9 Parent Dependency details another component on which this module relies - i.e. a pre-requisit Name Type Description Required name string Name is the name of the dependent component true type enum Type provides information used in determining how to instantiate the component Enum : module, connector, feature true FybrikModule.spec.statusIndicators[index] \u21a9 Parent ResourceStatusIndicator is used to determine the status of an orchestrated resource Name Type Description Required kind string Kind provides information about the resource kind true successCondition string SuccessCondition specifies a condition that indicates that the resource is ready It uses kubernetes label selection syntax (https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/) true errorMessage string ErrorMessage specifies the resource field to check for an error, e.g. status.errorMsg false failureCondition string FailureCondition specifies a condition that indicates the resource failure It uses kubernetes label selection syntax (https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/) false FybrikModule.status \u21a9 Parent FybrikModuleStatus defines the observed state of FybrikModule. Name Type Description Required conditions []object Conditions indicate the module states with respect to validation false FybrikModule.status.conditions[index] \u21a9 Parent Condition describes the state of a FybrikApplication at a certain point. Name Type Description Required type string Type of the condition true message string Message contains the details of the current condition false observedGeneration integer ObservedGeneration is the version of the resource for which the condition has been evaluated Format : int64 false status enum Status of the condition, one of (`True`, `False`, `Unknown`). Enum : True, False, Unknown Default : Unknown false FybrikStorageAccount \u21a9 Parent FybrikStorageAccount defines a storage account used for copying data. Only S3 based storage is supported. It contains endpoint, region and a reference to the credentials a Owner of the asset is responsible to store the credentials Name Type Description Required apiVersion string app.fybrik.io/v1alpha1 true kind string FybrikStorageAccount true metadata object Refer to the Kubernetes API documentation for the fields of the `metadata` field. true spec object FybrikStorageAccountSpec defines the desired state of FybrikStorageAccount true status object FybrikStorageAccountStatus defines the observed state of FybrikStorageAccount false FybrikStorageAccount.spec \u21a9 Parent FybrikStorageAccountSpec defines the desired state of FybrikStorageAccount Name Type Description Required endpoint string Endpoint for accessing the data true id string Identification of a storage account true region string Storage region true secretRef string A name of k8s secret deployed in the control plane. This secret includes secretKey and accessKey credentials for S3 bucket true Plotter \u21a9 Parent Plotter is the Schema for the plotters API Name Type Description Required apiVersion string app.fybrik.io/v1alpha1 true kind string Plotter true metadata object Refer to the Kubernetes API documentation for the fields of the `metadata` field. true spec object PlotterSpec defines the desired state of Plotter, which is applied in a multi-clustered environment. Plotter declares what needs to be installed and where (as blueprints running on remote clusters) which provides the Data Scientist's application with secure and governed access to the data requested in the FybrikApplication. true status object PlotterStatus defines the observed state of Plotter This includes readiness, error message, and indicators received from blueprint resources owned by the Plotter for cleanup and status monitoring false Plotter.spec \u21a9 Parent PlotterSpec defines the desired state of Plotter, which is applied in a multi-clustered environment. Plotter declares what needs to be installed and where (as blueprints running on remote clusters) which provides the Data Scientist's application with secure and governed access to the data requested in the FybrikApplication. Name Type Description Required assets map[string]object Assets is a map holding information about the assets The key is the assetID true flows []object true modulesNamespace string ModulesNamespace is the namespace where modules should be allocated true templates map[string]object Templates is a map holding the templates used in this plotter steps The key is the template name true appInfo object Application context to be transferred to the modules false appSelector object Selector enables to connect the resource to the application Application labels should match the labels in the selector. For some flows the selector may not be used. false Plotter.spec.assets[key] \u21a9 Parent AssetDetails is a list of assets used in the fybrikapplication. In addition to assets declared in fybrikapplication, AssetDetails list also contains assets that are allocated by the control-plane in order to serve fybrikapplication Name Type Description Required assetDetails object DataStore contains the details for accesing the data that are sent by catalog connectors Credentials for accesing the data are stored in Vault, in the location represented by Vault property. true advertisedAssetId string AdvertisedAssetID links this asset to asset from fybrikapplication and is used by user facing services false Plotter.spec.assets[key].assetDetails \u21a9 Parent DataStore contains the details for accesing the data that are sent by catalog connectors Credentials for accesing the data are stored in Vault, in the location represented by Vault property. Name Type Description Required connection object Connection has the relevant details for accesing the data (url, table, ssl, etc.) true format string Format represents data format (e.g. parquet) as received from catalog connectors false vault map[string]object Holds details for retrieving credentials by the modules from Vault store. It is a map so that different credentials can be stored for the different DataFlow operations. false Plotter.spec.assets[key].assetDetails.connection \u21a9 Parent Connection has the relevant details for accesing the data (url, table, ssl, etc.) Name Type Description Required name string true Plotter.spec.assets[key].assetDetails.vault[key] \u21a9 Parent Holds details for retrieving credentials from Vault store. Name Type Description Required address string Address is Vault address true authPath string AuthPath is the path to auth method i.e. kubernetes true role string Role is the Vault role used for retrieving the credentials true secretPath string SecretPath is the path of the secret holding the Credentials in Vault true Plotter.spec.flows[index] \u21a9 Parent Flows is the list of data flows driven from fybrikapplication: Each element in the list holds the flow of the data requested in fybrikapplication. Name Type Description Required assetId string AssetID indicates the data set being used in this data flow true flowType enum Type of the flow (e.g. read) Enum : read, write, delete, copy true name string Name of the flow true subFlows []object true Plotter.spec.flows[index].subFlows[index] \u21a9 Parent Subflows is a list of data flows which are originated from the same data asset but are triggered differently (e.g., one upon init trigger and one upon workload trigger) Name Type Description Required flowType enum Type of the flow (e.g. read) Enum : read, write, delete, copy true name string Name of the SubFlow true steps [][]object Steps defines a series of sequential/parallel data flow steps The first dimension represents parallel data flows. The second sequential components within the same parallel data flow. true triggers []enum Triggers true Plotter.spec.flows[index].subFlows[index].steps[index][index] \u21a9 Parent DataFlowStep contains details on a single data flow step Name Type Description Required cluster string Name of the cluster this step is executed on true name string Name of the step true template string Template is the name of the template to execute the step The full details of the template can be extracted from Plotter.spec.templates list field. true parameters object Step parameters TODO why not flatten the parameters into this data flow step false Plotter.spec.flows[index].subFlows[index].steps[index][index].parameters \u21a9 Parent Step parameters TODO why not flatten the parameters into this data flow step Name Type Description Required action []object Actions are the data transformations that the module supports false api object ResourceDetails includes asset connection details false args []object false Plotter.spec.flows[index].subFlows[index].steps[index][index].parameters.action[index] \u21a9 Parent Name Type Description Required name string true Plotter.spec.flows[index].subFlows[index].steps[index][index].parameters.api \u21a9 Parent ResourceDetails includes asset connection details Name Type Description Required connection object Connection information true dataFormat string Data format false Plotter.spec.flows[index].subFlows[index].steps[index][index].parameters.api.connection \u21a9 Parent Connection information Name Type Description Required name string true Plotter.spec.flows[index].subFlows[index].steps[index][index].parameters.args[index] \u21a9 Parent StepArgument describes a step: it could be assetID or an endpoint of another step Name Type Description Required api object API holds information for accessing a module instance false assetId string AssetID identifies the source asset of this step false Plotter.spec.flows[index].subFlows[index].steps[index][index].parameters.args[index].api \u21a9 Parent API holds information for accessing a module instance Name Type Description Required connection object Connection information true dataFormat string Data format false Plotter.spec.flows[index].subFlows[index].steps[index][index].parameters.args[index].api.connection \u21a9 Parent Connection information Name Type Description Required name string true Plotter.spec.templates[key] \u21a9 Parent Template contains basic information about the required modules to serve the fybrikapplication e.g., the module helm chart name. Name Type Description Required modules []object Modules is a list of dependent modules. e.g., if a plugin module is used then the service module is used in should appear first in the modules list of the same template. If the modules list contains more than one module, the first module in the list is referred to as the \"primary module\" of which all the parameters to this template are sent to. true name string Name of the template false Plotter.spec.templates[key].modules[index] \u21a9 Parent ModuleInfo is a copy of FybrikModule Custom Resource. It contains information to instantiate resource of type FybrikModule. Name Type Description Required capability string Module capability true chart object Chart contains the information needed to use helm to install the capability true name string Name of the module true type string May be one of service, config or plugin Service: Means that the control plane deploys the component that performs the capability Config: Another pre-installed service performs the capability and the module deployed configures it for the particular workload or dataset Plugin: Indicates that this module performs a capability as part of another service or module rather than as a stand-alone module true scope enum Scope indicates at what level the capability is used: workload, asset, cluster If not indicated it is assumed to be asset Enum : asset, workload, cluster false Plotter.spec.templates[key].modules[index].chart \u21a9 Parent Chart contains the information needed to use helm to install the capability Name Type Description Required name string Name of helm chart true chartPullSecret string Name of secret containing helm registry credentials false values map[string]string Values to pass to helm chart installation false Plotter.spec.appSelector \u21a9 Parent Selector enables to connect the resource to the application Application labels should match the labels in the selector. For some flows the selector may not be used. Name Type Description Required workloadSelector object WorkloadSelector enables to connect the resource to the application Application labels should match the labels in the selector. true clusterName string Cluster name false Plotter.spec.appSelector.workloadSelector \u21a9 Parent WorkloadSelector enables to connect the resource to the application Application labels should match the labels in the selector. Name Type Description Required matchExpressions []object matchExpressions is a list of label selector requirements. The requirements are ANDed. false matchLabels map[string]string matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels map is equivalent to an element of matchExpressions, whose key field is \"key\", the operator is \"In\", and the values array contains only \"value\". The requirements are ANDed. false Plotter.spec.appSelector.workloadSelector.matchExpressions[index] \u21a9 Parent A label selector requirement is a selector that contains values, a key, and an operator that relates the key and values. Name Type Description Required key string key is the label key that the selector applies to. true operator string operator represents a key's relationship to a set of values. Valid operators are In, NotIn, Exists and DoesNotExist. true values []string values is an array of string values. If the operator is In or NotIn, the values array must be non-empty. If the operator is Exists or DoesNotExist, the values array must be empty. This array is replaced during a strategic merge patch. false Plotter.status \u21a9 Parent PlotterStatus defines the observed state of Plotter This includes readiness, error message, and indicators received from blueprint resources owned by the Plotter for cleanup and status monitoring Name Type Description Required assets map[string]object Assets is a map containing the status per asset. The key of this map is assetId false blueprints map[string]object false conditions []object Conditions represent the possible error and failure conditions false flows map[string]object Flows is a map containing the status for each flow the key is the flow name false observedGeneration integer ObservedGeneration is taken from the Plotter metadata. This is used to determine during reconcile whether reconcile was called because the desired state changed, or whether status of the allocated blueprints should be checked. Format : int64 false observedState object ObservedState includes information to be reported back to the FybrikApplication resource It includes readiness and error indications, as well as user instructions false readyTimestamp string Format : date-time false Plotter.status.assets[key] \u21a9 Parent ObservedState represents a part of the generated Blueprint/Plotter resource status that allows update of FybrikApplication status Name Type Description Required error string Error indicates that there has been an error to orchestrate the modules and provides the error message false ready boolean Ready represents that the modules have been orchestrated successfully and the data is ready for usage false Plotter.status.blueprints[key] \u21a9 Parent MetaBlueprint defines blueprint metadata (name, namespace) and status Name Type Description Required name string true namespace string true status object BlueprintStatus defines the observed state of Blueprint This includes readiness, error message, and indicators for the Kubernetes resources owned by the Blueprint for cleanup and status monitoring true Plotter.status.blueprints[key].status \u21a9 Parent BlueprintStatus defines the observed state of Blueprint This includes readiness, error message, and indicators for the Kubernetes resources owned by the Blueprint for cleanup and status monitoring Name Type Description Required modules map[string]object ModulesState is a map which holds the status of each module its key is the moduleInstanceName which is the unique name for the deployed instance related to this workload false observedGeneration integer ObservedGeneration is taken from the Blueprint metadata. This is used to determine during reconcile whether reconcile was called because the desired state changed, or whether status of the allocated resources should be checked. Format : int64 false observedState object ObservedState includes information to be reported back to the FybrikApplication resource It includes readiness and error indications, as well as user instructions false releases map[string]integer Releases map each release to the observed generation of the blueprint containing this release. At the end of reconcile, each release should be mapped to the latest blueprint version or be uninstalled. false Plotter.status.blueprints[key].status.modules[key] \u21a9 Parent ObservedState represents a part of the generated Blueprint/Plotter resource status that allows update of FybrikApplication status Name Type Description Required error string Error indicates that there has been an error to orchestrate the modules and provides the error message false ready boolean Ready represents that the modules have been orchestrated successfully and the data is ready for usage false Plotter.status.blueprints[key].status.observedState \u21a9 Parent ObservedState includes information to be reported back to the FybrikApplication resource It includes readiness and error indications, as well as user instructions Name Type Description Required error string Error indicates that there has been an error to orchestrate the modules and provides the error message false ready boolean Ready represents that the modules have been orchestrated successfully and the data is ready for usage false Plotter.status.conditions[index] \u21a9 Parent Condition describes the state of a FybrikApplication at a certain point. Name Type Description Required type string Type of the condition true message string Message contains the details of the current condition false observedGeneration integer ObservedGeneration is the version of the resource for which the condition has been evaluated Format : int64 false status enum Status of the condition, one of (`True`, `False`, `Unknown`). Enum : True, False, Unknown Default : Unknown false Plotter.status.flows[key] \u21a9 Parent FlowStatus includes information to be reported back to the FybrikApplication resource It holds the status per data flow Name Type Description Required subFlows map[string]object true status object ObservedState includes information about the current flow It includes readiness and error indications, as well as user instructions false Plotter.status.flows[key].subFlows[key] \u21a9 Parent ObservedState represents a part of the generated Blueprint/Plotter resource status that allows update of FybrikApplication status Name Type Description Required error string Error indicates that there has been an error to orchestrate the modules and provides the error message false ready boolean Ready represents that the modules have been orchestrated successfully and the data is ready for usage false Plotter.status.flows[key].status \u21a9 Parent ObservedState includes information about the current flow It includes readiness and error indications, as well as user instructions Name Type Description Required error string Error indicates that there has been an error to orchestrate the modules and provides the error message false ready boolean Ready represents that the modules have been orchestrated successfully and the data is ready for usage false Plotter.status.observedState \u21a9 Parent ObservedState includes information to be reported back to the FybrikApplication resource It includes readiness and error indications, as well as user instructions Name Type Description Required error string Error indicates that there has been an error to orchestrate the modules and provides the error message false ready boolean Ready represents that the modules have been orchestrated successfully and the data is ready for usage false katalog.fybrik.io/v1alpha1 Resource Types: Asset Asset \u21a9 Parent Asset defines an asset in the catalog Name Type Description Required apiVersion string katalog.fybrik.io/v1alpha1 true kind string Asset true metadata object Refer to the Kubernetes API documentation for the fields of the `metadata` field. true spec object true Asset.spec \u21a9 Parent Name Type Description Required details object Asset details true metadata object Asset metadata true secretRef object Reference to a Secret resource holding credentials for this asset true Asset.spec.details \u21a9 Parent Asset details Name Type Description Required connection object Connection information true dataFormat string Data format false Asset.spec.details.connection \u21a9 Parent Connection information Name Type Description Required name string true Asset.spec.metadata \u21a9 Parent Asset metadata Name Type Description Required columns []object Columns associated with the asset false geography string Geography of the resource false name string Name of the resource false owner string Owner of the resource false tags object Tags associated with the asset false Asset.spec.metadata.columns[index] \u21a9 Parent ResourceColumn represents a column in a tabular resource Name Type Description Required name string Name of the column true tags object Tags associated with the column false Asset.spec.secretRef \u21a9 Parent Reference to a Secret resource holding credentials for this asset Name Type Description Required name string Name of the Secret resource true namespace string Namespace of the Secret resource. If it is empty then the asset namespace is used. false","title":"API Reference"},{"location":"reference/crds/#api-reference","text":"Packages: app.fybrik.io/v1alpha1 katalog.fybrik.io/v1alpha1","title":"API Reference"},{"location":"reference/crds/#appfybrikiov1alpha1","text":"Resource Types: Blueprint FybrikApplication FybrikModule FybrikStorageAccount Plotter","title":"app.fybrik.io/v1alpha1"},{"location":"reference/crds/#blueprint","text":"\u21a9 Parent Blueprint is the Schema for the blueprints API Name Type Description Required apiVersion string app.fybrik.io/v1alpha1 true kind string Blueprint true metadata object Refer to the Kubernetes API documentation for the fields of the `metadata` field. true spec object BlueprintSpec defines the desired state of Blueprint, which defines the components of the workload's data path that run in a particular cluster. In a single cluster environment there is one blueprint per workload (FybrikApplication). In a multi-cluster environment there is one Blueprint per cluster per workload (FybrikApplication). true status object BlueprintStatus defines the observed state of Blueprint This includes readiness, error message, and indicators for the Kubernetes resources owned by the Blueprint for cleanup and status monitoring false","title":"Blueprint"},{"location":"reference/crds/#blueprintspec","text":"\u21a9 Parent BlueprintSpec defines the desired state of Blueprint, which defines the components of the workload's data path that run in a particular cluster. In a single cluster environment there is one blueprint per workload (FybrikApplication). In a multi-cluster environment there is one Blueprint per cluster per workload (FybrikApplication). Name Type Description Required cluster string Cluster indicates the cluster on which the Blueprint runs true modules map[string]object Modules is a map which contains modules that indicate the data path components that run in this cluster The map key is moduleInstanceName which is the unique name for the deployed instance related to this workload true modulesNamespace string ModulesNamespace is the namespace where modules should be allocated true application object ApplicationContext is a context of the origin FybrikApplication (labels, properties, etc.) false","title":"Blueprint.spec"},{"location":"reference/crds/#blueprintspecmoduleskey","text":"\u21a9 Parent BlueprintModule is a copy of a FybrikModule Custom Resource. It contains the information necessary to instantiate a datapath component, including the parameters relevant for the particular workload. Name Type Description Required chart object Chart contains the location of the helm chart with info detailing how to deploy true name string Name of the FybrikModule on which this is based true arguments object Arguments are the input parameters for a specific instance of a module. false assetIds []string assetIDs indicate the assets processed by this module. Included so we can track asset status as well as module status in the future. false","title":"Blueprint.spec.modules[key]"},{"location":"reference/crds/#blueprintspecmoduleskeychart","text":"\u21a9 Parent Chart contains the location of the helm chart with info detailing how to deploy Name Type Description Required name string Name of helm chart true chartPullSecret string Name of secret containing helm registry credentials false values map[string]string Values to pass to helm chart installation false","title":"Blueprint.spec.modules[key].chart"},{"location":"reference/crds/#blueprintspecmoduleskeyarguments","text":"\u21a9 Parent Arguments are the input parameters for a specific instance of a module. Name Type Description Required assets []object Assets define asset related arguments, such as data source, transformations, etc. false","title":"Blueprint.spec.modules[key].arguments"},{"location":"reference/crds/#blueprintspecmoduleskeyargumentsassetsindex","text":"\u21a9 Parent AssetContext defines the input parameters for modules that access an asset Name Type Description Required assetID string AssetID identifies the asset to be used for accessing the data when it is ready It is copied from the FybrikApplication resource true capability string Capability of the module true args []object List of datastores associated with the asset false transformations []object Transformations are different types of processing that may be done to the data as it is copied. false","title":"Blueprint.spec.modules[key].arguments.assets[index]"},{"location":"reference/crds/#blueprintspecmoduleskeyargumentsassetsindexargsindex","text":"\u21a9 Parent DataStore contains the details for accesing the data that are sent by catalog connectors Credentials for accesing the data are stored in Vault, in the location represented by Vault property. Name Type Description Required connection object Connection has the relevant details for accesing the data (url, table, ssl, etc.) true format string Format represents data format (e.g. parquet) as received from catalog connectors false vault map[string]object Holds details for retrieving credentials by the modules from Vault store. It is a map so that different credentials can be stored for the different DataFlow operations. false","title":"Blueprint.spec.modules[key].arguments.assets[index].args[index]"},{"location":"reference/crds/#blueprintspecmoduleskeyargumentsassetsindexargsindexconnection","text":"\u21a9 Parent Connection has the relevant details for accesing the data (url, table, ssl, etc.) Name Type Description Required name string true","title":"Blueprint.spec.modules[key].arguments.assets[index].args[index].connection"},{"location":"reference/crds/#blueprintspecmoduleskeyargumentsassetsindexargsindexvaultkey","text":"\u21a9 Parent Holds details for retrieving credentials from Vault store. Name Type Description Required address string Address is Vault address true authPath string AuthPath is the path to auth method i.e. kubernetes true role string Role is the Vault role used for retrieving the credentials true secretPath string SecretPath is the path of the secret holding the Credentials in Vault true","title":"Blueprint.spec.modules[key].arguments.assets[index].args[index].vault[key]"},{"location":"reference/crds/#blueprintspecmoduleskeyargumentsassetsindextransformationsindex","text":"\u21a9 Parent Name Type Description Required name string true","title":"Blueprint.spec.modules[key].arguments.assets[index].transformations[index]"},{"location":"reference/crds/#blueprintspecapplication","text":"\u21a9 Parent ApplicationContext is a context of the origin FybrikApplication (labels, properties, etc.) Name Type Description Required context object Application context such as intent, role, etc. false selector object Application selector is used to identify the user workload. It is obtained from FybrikApplication spec. false","title":"Blueprint.spec.application"},{"location":"reference/crds/#blueprintspecapplicationselector","text":"\u21a9 Parent Application selector is used to identify the user workload. It is obtained from FybrikApplication spec. Name Type Description Required matchExpressions []object matchExpressions is a list of label selector requirements. The requirements are ANDed. false matchLabels map[string]string matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels map is equivalent to an element of matchExpressions, whose key field is \"key\", the operator is \"In\", and the values array contains only \"value\". The requirements are ANDed. false","title":"Blueprint.spec.application.selector"},{"location":"reference/crds/#blueprintspecapplicationselectormatchexpressionsindex","text":"\u21a9 Parent A label selector requirement is a selector that contains values, a key, and an operator that relates the key and values. Name Type Description Required key string key is the label key that the selector applies to. true operator string operator represents a key's relationship to a set of values. Valid operators are In, NotIn, Exists and DoesNotExist. true values []string values is an array of string values. If the operator is In or NotIn, the values array must be non-empty. If the operator is Exists or DoesNotExist, the values array must be empty. This array is replaced during a strategic merge patch. false","title":"Blueprint.spec.application.selector.matchExpressions[index]"},{"location":"reference/crds/#blueprintstatus","text":"\u21a9 Parent BlueprintStatus defines the observed state of Blueprint This includes readiness, error message, and indicators for the Kubernetes resources owned by the Blueprint for cleanup and status monitoring Name Type Description Required modules map[string]object ModulesState is a map which holds the status of each module its key is the moduleInstanceName which is the unique name for the deployed instance related to this workload false observedGeneration integer ObservedGeneration is taken from the Blueprint metadata. This is used to determine during reconcile whether reconcile was called because the desired state changed, or whether status of the allocated resources should be checked. Format : int64 false observedState object ObservedState includes information to be reported back to the FybrikApplication resource It includes readiness and error indications, as well as user instructions false releases map[string]integer Releases map each release to the observed generation of the blueprint containing this release. At the end of reconcile, each release should be mapped to the latest blueprint version or be uninstalled. false","title":"Blueprint.status"},{"location":"reference/crds/#blueprintstatusmoduleskey","text":"\u21a9 Parent ObservedState represents a part of the generated Blueprint/Plotter resource status that allows update of FybrikApplication status Name Type Description Required error string Error indicates that there has been an error to orchestrate the modules and provides the error message false ready boolean Ready represents that the modules have been orchestrated successfully and the data is ready for usage false","title":"Blueprint.status.modules[key]"},{"location":"reference/crds/#blueprintstatusobservedstate","text":"\u21a9 Parent ObservedState includes information to be reported back to the FybrikApplication resource It includes readiness and error indications, as well as user instructions Name Type Description Required error string Error indicates that there has been an error to orchestrate the modules and provides the error message false ready boolean Ready represents that the modules have been orchestrated successfully and the data is ready for usage false","title":"Blueprint.status.observedState"},{"location":"reference/crds/#fybrikapplication","text":"\u21a9 Parent FybrikApplication provides information about the application whose data is being operated on, the nature of the processing, and the data sets chosen for processing by the application. The FybrikApplication controller obtains instructions regarding any governance related changes that must be performed on the data, identifies the modules capable of performing such changes, and finally generates the Plotter which defines the secure runtime environment and all the components in it. This runtime environment provides the application with access to the data requested in a secure manner and without having to provide any credentials for the data sets. The credentials are obtained automatically by the manager from the credential management system. Name Type Description Required apiVersion string app.fybrik.io/v1alpha1 true kind string FybrikApplication true metadata object Refer to the Kubernetes API documentation for the fields of the `metadata` field. true spec object FybrikApplicationSpec defines data flows needed by the application, the purpose and other contextual information about the application. true status object FybrikApplicationStatus defines the observed state of FybrikApplication. false","title":"FybrikApplication"},{"location":"reference/crds/#fybrikapplicationspec","text":"\u21a9 Parent FybrikApplicationSpec defines data flows needed by the application, the purpose and other contextual information about the application. Name Type Description Required appInfo object AppInfo contains information describing the reasons for the processing that will be done by the application. true data []object Data contains the identifiers of the data to be used by the Data Scientist's application, and the protocol used to access it and the format expected. true secretRef string SecretRef points to the secret that holds credentials for each system the user has been authenticated with. The secret is deployed in FybrikApplication namespace. false selector object Selector enables to connect the resource to the application Application labels should match the labels in the selector. false","title":"FybrikApplication.spec"},{"location":"reference/crds/#fybrikapplicationspecdataindex","text":"\u21a9 Parent DataContext indicates data set being processed by the workload and includes information about the data format and technologies used to access the data. Name Type Description Required dataSetID string DataSetID is a unique identifier of the dataset chosen from the data catalog. For data catalogs that support multiple sub-catalogs, it includes the catalog id and the dataset id. When writing a new dataset it is the name provided by the user or workload generating it. true requirements object Requirements from the system true flow enum Flows indicates what is being done with the particular dataset - ex: read, write, copy (ingest), delete This is optional for the purpose of backward compatibility. If nothing is provided, read is assumed. Enum : read, write, delete, copy false","title":"FybrikApplication.spec.data[index]"},{"location":"reference/crds/#fybrikapplicationspecdataindexrequirements","text":"\u21a9 Parent Requirements from the system Name Type Description Required flowParams object FlowParams include the requirements for particular data flows false interface object Interface indicates the protocol and format expected by the data user false","title":"FybrikApplication.spec.data[index].requirements"},{"location":"reference/crds/#fybrikapplicationspecdataindexrequirementsflowparams","text":"\u21a9 Parent FlowParams include the requirements for particular data flows Name Type Description Required catalog string Catalog indicates that the data asset must be cataloged, and in which catalog to register it false isNewDataSet boolean IsNewDataSet if true indicates that the DataContext.DataSetID is user provided and not a full catalog / dataset ID. Relevant when writing. A unique ID from the catalog will be provided in the FybrikApplication Status after a new catalog entry is created. false metadata object Source asset metadata like asset name, owner, geography, etc Relevant when writing new asset. false storageEstimate integer Storage estimate indicates the estimated amount of storage in MB, GB, TB required when writing new data. Format : int64 false","title":"FybrikApplication.spec.data[index].requirements.flowParams"},{"location":"reference/crds/#fybrikapplicationspecdataindexrequirementsflowparamsmetadata","text":"\u21a9 Parent Source asset metadata like asset name, owner, geography, etc Relevant when writing new asset. Name Type Description Required columns []object Columns associated with the asset false geography string Geography of the resource false name string Name of the resource false owner string Owner of the resource false tags object Tags associated with the asset false","title":"FybrikApplication.spec.data[index].requirements.flowParams.metadata"},{"location":"reference/crds/#fybrikapplicationspecdataindexrequirementsflowparamsmetadatacolumnsindex","text":"\u21a9 Parent ResourceColumn represents a column in a tabular resource Name Type Description Required name string Name of the column true tags object Tags associated with the column false","title":"FybrikApplication.spec.data[index].requirements.flowParams.metadata.columns[index]"},{"location":"reference/crds/#fybrikapplicationspecdataindexrequirementsinterface","text":"\u21a9 Parent Interface indicates the protocol and format expected by the data user Name Type Description Required protocol string Protocol defines the interface protocol used for data transactions true dataformat string DataFormat defines the data format type false","title":"FybrikApplication.spec.data[index].requirements.interface"},{"location":"reference/crds/#fybrikapplicationspecselector","text":"\u21a9 Parent Selector enables to connect the resource to the application Application labels should match the labels in the selector. Name Type Description Required workloadSelector object WorkloadSelector enables to connect the resource to the application Application labels should match the labels in the selector. true clusterName string Cluster name false","title":"FybrikApplication.spec.selector"},{"location":"reference/crds/#fybrikapplicationspecselectorworkloadselector","text":"\u21a9 Parent WorkloadSelector enables to connect the resource to the application Application labels should match the labels in the selector. Name Type Description Required matchExpressions []object matchExpressions is a list of label selector requirements. The requirements are ANDed. false matchLabels map[string]string matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels map is equivalent to an element of matchExpressions, whose key field is \"key\", the operator is \"In\", and the values array contains only \"value\". The requirements are ANDed. false","title":"FybrikApplication.spec.selector.workloadSelector"},{"location":"reference/crds/#fybrikapplicationspecselectorworkloadselectormatchexpressionsindex","text":"\u21a9 Parent A label selector requirement is a selector that contains values, a key, and an operator that relates the key and values. Name Type Description Required key string key is the label key that the selector applies to. true operator string operator represents a key's relationship to a set of values. Valid operators are In, NotIn, Exists and DoesNotExist. true values []string values is an array of string values. If the operator is In or NotIn, the values array must be non-empty. If the operator is Exists or DoesNotExist, the values array must be empty. This array is replaced during a strategic merge patch. false","title":"FybrikApplication.spec.selector.workloadSelector.matchExpressions[index]"},{"location":"reference/crds/#fybrikapplicationstatus","text":"\u21a9 Parent FybrikApplicationStatus defines the observed state of FybrikApplication. Name Type Description Required assetStates map[string]object AssetStates provides a status per asset false errorMessage string ErrorMessage indicates that an error has happened during the reconcile, unrelated to a specific asset false generated object Generated resource identifier false observedGeneration integer ObservedGeneration is taken from the FybrikApplication metadata. This is used to determine during reconcile whether reconcile was called because the desired state changed, or whether the Blueprint status changed. Format : int64 false provisionedStorage map[string]object ProvisionedStorage maps a dataset (identified by AssetID) to the new provisioned bucket. It allows FybrikApplication controller to manage buckets in case the spec has been modified, an error has occurred, or a delete event has been received. ProvisionedStorage has the information required to register the dataset once the owned plotter resource is ready false ready boolean Ready is true if all specified assets are either ready to be used or are denied access. false validApplication string ValidApplication indicates whether the FybrikApplication is valid given the defined taxonomy false validatedGeneration integer ValidatedGeneration is the version of the FyrbikApplication that has been validated with the taxonomy defined. Format : int64 false","title":"FybrikApplication.status"},{"location":"reference/crds/#fybrikapplicationstatusassetstateskey","text":"\u21a9 Parent AssetState defines the observed state of an asset Name Type Description Required catalogedAsset string CatalogedAsset provides a new asset identifier after being registered in the enterprise catalog false conditions []object Conditions indicate the asset state (Ready, Deny, Error) false endpoint object Endpoint provides the endpoint spec from which the asset will be served to the application false","title":"FybrikApplication.status.assetStates[key]"},{"location":"reference/crds/#fybrikapplicationstatusassetstateskeyconditionsindex","text":"\u21a9 Parent Condition describes the state of a FybrikApplication at a certain point. Name Type Description Required type string Type of the condition true message string Message contains the details of the current condition false observedGeneration integer ObservedGeneration is the version of the resource for which the condition has been evaluated Format : int64 false status enum Status of the condition, one of (`True`, `False`, `Unknown`). Enum : True, False, Unknown Default : Unknown false","title":"FybrikApplication.status.assetStates[key].conditions[index]"},{"location":"reference/crds/#fybrikapplicationstatusassetstateskeyendpoint","text":"\u21a9 Parent Endpoint provides the endpoint spec from which the asset will be served to the application Name Type Description Required name string true","title":"FybrikApplication.status.assetStates[key].endpoint"},{"location":"reference/crds/#fybrikapplicationstatusgenerated","text":"\u21a9 Parent Generated resource identifier Name Type Description Required appVersion integer Version of FybrikApplication that has generated this resource Format : int64 true kind string Kind of the resource (Blueprint, Plotter) true name string Name of the resource true namespace string Namespace of the resource true","title":"FybrikApplication.status.generated"},{"location":"reference/crds/#fybrikapplicationstatusprovisionedstoragekey","text":"\u21a9 Parent DatasetDetails holds details of the provisioned storage Name Type Description Required datasetRef string Reference to a Dataset resource containing the request to provision storage false details object Dataset information false resourceMetadata object Resource Metadata false secretRef object Reference to a secret where the credentials are stored false","title":"FybrikApplication.status.provisionedStorage[key]"},{"location":"reference/crds/#fybrikapplicationstatusprovisionedstoragekeydetails","text":"\u21a9 Parent Dataset information Name Type Description Required connection object Connection has the relevant details for accesing the data (url, table, ssl, etc.) true format string Format represents data format (e.g. parquet) as received from catalog connectors false vault map[string]object Holds details for retrieving credentials by the modules from Vault store. It is a map so that different credentials can be stored for the different DataFlow operations. false","title":"FybrikApplication.status.provisionedStorage[key].details"},{"location":"reference/crds/#fybrikapplicationstatusprovisionedstoragekeydetailsconnection","text":"\u21a9 Parent Connection has the relevant details for accesing the data (url, table, ssl, etc.) Name Type Description Required name string true","title":"FybrikApplication.status.provisionedStorage[key].details.connection"},{"location":"reference/crds/#fybrikapplicationstatusprovisionedstoragekeydetailsvaultkey","text":"\u21a9 Parent Holds details for retrieving credentials from Vault store. Name Type Description Required address string Address is Vault address true authPath string AuthPath is the path to auth method i.e. kubernetes true role string Role is the Vault role used for retrieving the credentials true secretPath string SecretPath is the path of the secret holding the Credentials in Vault true","title":"FybrikApplication.status.provisionedStorage[key].details.vault[key]"},{"location":"reference/crds/#fybrikapplicationstatusprovisionedstoragekeyresourcemetadata","text":"\u21a9 Parent Resource Metadata Name Type Description Required columns []object Columns associated with the asset false geography string Geography of the resource false name string Name of the resource false owner string Owner of the resource false tags object Tags associated with the asset false","title":"FybrikApplication.status.provisionedStorage[key].resourceMetadata"},{"location":"reference/crds/#fybrikapplicationstatusprovisionedstoragekeyresourcemetadatacolumnsindex","text":"\u21a9 Parent ResourceColumn represents a column in a tabular resource Name Type Description Required name string Name of the column true tags object Tags associated with the column false","title":"FybrikApplication.status.provisionedStorage[key].resourceMetadata.columns[index]"},{"location":"reference/crds/#fybrikapplicationstatusprovisionedstoragekeysecretref","text":"\u21a9 Parent Reference to a secret where the credentials are stored Name Type Description Required name string Secret name true namespace string Secret Namespace true","title":"FybrikApplication.status.provisionedStorage[key].secretRef"},{"location":"reference/crds/#fybrikmodule","text":"\u21a9 Parent FybrikModule is a description of an injectable component. the parameters it requires, as well as the specification of how to instantiate such a component. It is used as metadata only. There is no status nor reconciliation. Name Type Description Required apiVersion string app.fybrik.io/v1alpha1 true kind string FybrikModule true metadata object Refer to the Kubernetes API documentation for the fields of the `metadata` field. true spec object FybrikModuleSpec contains the info common to all modules, which are one of the components that process, load, write, audit, monitor the data used by the data scientist's application. true status object FybrikModuleStatus defines the observed state of FybrikModule. false","title":"FybrikModule"},{"location":"reference/crds/#fybrikmodulespec","text":"\u21a9 Parent FybrikModuleSpec contains the info common to all modules, which are one of the components that process, load, write, audit, monitor the data used by the data scientist's application. Name Type Description Required capabilities []object Capabilities declares what this module knows how to do and the types of data it knows how to handle The key to the map is a CapabilityType string true chart object Reference to a Helm chart that allows deployment of the resources required for this module true type string May be one of service, config or plugin Service: Means that the control plane deploys the component that performs the capability Config: Another pre-installed service performs the capability and the module deployed configures it for the particular workload or dataset Plugin: Indicates that this module performs a capability as part of another service or module rather than as a stand-alone module true dependencies []object Other components that must be installed in order for this module to work false description string An explanation of what this module does false pluginType string Plugin type indicates the plugin technology used to invoke the capabilities Ex: vault, fybrik-wasm... Should be provided if type is plugin false statusIndicators []object StatusIndicators allow to check status of a non-standard resource that can not be computed by helm/kstatus false","title":"FybrikModule.spec"},{"location":"reference/crds/#fybrikmodulespeccapabilitiesindex","text":"\u21a9 Parent Capability declares what this module knows how to do and the types of data it knows how to handle Name Type Description Required capability string Capability declares what this module knows how to do - ex: read, write, transform... true actions []object Actions are the data transformations that the module supports false api object API indicates to the application how to access the capabilities provided by the module false plugins []object Plugins enable the module to add libraries to perform actions rather than implementing them by itself false scope enum Scope indicates at what level the capability is used: workload, asset, cluster If not indicated it is assumed to be asset Enum : asset, workload, cluster false supportedInterfaces []object Copy should have one or more instances in the list, and its content should have source and sink Read should have one or more instances in the list, each with source populated Write should have one or more instances in the list, each with sink populated This field may not be required if not handling data false","title":"FybrikModule.spec.capabilities[index]"},{"location":"reference/crds/#fybrikmodulespeccapabilitiesindexactionsindex","text":"\u21a9 Parent Name Type Description Required name string Unique name of an action supported by the module true","title":"FybrikModule.spec.capabilities[index].actions[index]"},{"location":"reference/crds/#fybrikmodulespeccapabilitiesindexapi","text":"\u21a9 Parent API indicates to the application how to access the capabilities provided by the module Name Type Description Required connection object Connection information true dataFormat string Data format false","title":"FybrikModule.spec.capabilities[index].api"},{"location":"reference/crds/#fybrikmodulespeccapabilitiesindexapiconnection","text":"\u21a9 Parent Connection information Name Type Description Required name string true","title":"FybrikModule.spec.capabilities[index].api.connection"},{"location":"reference/crds/#fybrikmodulespeccapabilitiesindexpluginsindex","text":"\u21a9 Parent Name Type Description Required dataFormat string DataFormat indicates the format of data the plugin knows how to process true pluginType string PluginType indicates the technology used for the module and the plugin to interact The values supported should come from the module taxonomy Examples of such mechanisms are vault plugins, wasm, etc true","title":"FybrikModule.spec.capabilities[index].plugins[index]"},{"location":"reference/crds/#fybrikmodulespeccapabilitiesindexsupportedinterfacesindex","text":"\u21a9 Parent ModuleInOut specifies the protocol and format of the data input and output by the module - if any Name Type Description Required sink object Sink specifies the output data protocol and format false source object Source specifies the input data protocol and format false","title":"FybrikModule.spec.capabilities[index].supportedInterfaces[index]"},{"location":"reference/crds/#fybrikmodulespeccapabilitiesindexsupportedinterfacesindexsink","text":"\u21a9 Parent Sink specifies the output data protocol and format Name Type Description Required protocol string Protocol defines the interface protocol used for data transactions true dataformat string DataFormat defines the data format type false","title":"FybrikModule.spec.capabilities[index].supportedInterfaces[index].sink"},{"location":"reference/crds/#fybrikmodulespeccapabilitiesindexsupportedinterfacesindexsource","text":"\u21a9 Parent Source specifies the input data protocol and format Name Type Description Required protocol string Protocol defines the interface protocol used for data transactions true dataformat string DataFormat defines the data format type false","title":"FybrikModule.spec.capabilities[index].supportedInterfaces[index].source"},{"location":"reference/crds/#fybrikmodulespecchart","text":"\u21a9 Parent Reference to a Helm chart that allows deployment of the resources required for this module Name Type Description Required name string Name of helm chart true chartPullSecret string Name of secret containing helm registry credentials false values map[string]string Values to pass to helm chart installation false","title":"FybrikModule.spec.chart"},{"location":"reference/crds/#fybrikmodulespecdependenciesindex","text":"\u21a9 Parent Dependency details another component on which this module relies - i.e. a pre-requisit Name Type Description Required name string Name is the name of the dependent component true type enum Type provides information used in determining how to instantiate the component Enum : module, connector, feature true","title":"FybrikModule.spec.dependencies[index]"},{"location":"reference/crds/#fybrikmodulespecstatusindicatorsindex","text":"\u21a9 Parent ResourceStatusIndicator is used to determine the status of an orchestrated resource Name Type Description Required kind string Kind provides information about the resource kind true successCondition string SuccessCondition specifies a condition that indicates that the resource is ready It uses kubernetes label selection syntax (https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/) true errorMessage string ErrorMessage specifies the resource field to check for an error, e.g. status.errorMsg false failureCondition string FailureCondition specifies a condition that indicates the resource failure It uses kubernetes label selection syntax (https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/) false","title":"FybrikModule.spec.statusIndicators[index]"},{"location":"reference/crds/#fybrikmodulestatus","text":"\u21a9 Parent FybrikModuleStatus defines the observed state of FybrikModule. Name Type Description Required conditions []object Conditions indicate the module states with respect to validation false","title":"FybrikModule.status"},{"location":"reference/crds/#fybrikmodulestatusconditionsindex","text":"\u21a9 Parent Condition describes the state of a FybrikApplication at a certain point. Name Type Description Required type string Type of the condition true message string Message contains the details of the current condition false observedGeneration integer ObservedGeneration is the version of the resource for which the condition has been evaluated Format : int64 false status enum Status of the condition, one of (`True`, `False`, `Unknown`). Enum : True, False, Unknown Default : Unknown false","title":"FybrikModule.status.conditions[index]"},{"location":"reference/crds/#fybrikstorageaccount","text":"\u21a9 Parent FybrikStorageAccount defines a storage account used for copying data. Only S3 based storage is supported. It contains endpoint, region and a reference to the credentials a Owner of the asset is responsible to store the credentials Name Type Description Required apiVersion string app.fybrik.io/v1alpha1 true kind string FybrikStorageAccount true metadata object Refer to the Kubernetes API documentation for the fields of the `metadata` field. true spec object FybrikStorageAccountSpec defines the desired state of FybrikStorageAccount true status object FybrikStorageAccountStatus defines the observed state of FybrikStorageAccount false","title":"FybrikStorageAccount"},{"location":"reference/crds/#fybrikstorageaccountspec","text":"\u21a9 Parent FybrikStorageAccountSpec defines the desired state of FybrikStorageAccount Name Type Description Required endpoint string Endpoint for accessing the data true id string Identification of a storage account true region string Storage region true secretRef string A name of k8s secret deployed in the control plane. This secret includes secretKey and accessKey credentials for S3 bucket true","title":"FybrikStorageAccount.spec"},{"location":"reference/crds/#plotter","text":"\u21a9 Parent Plotter is the Schema for the plotters API Name Type Description Required apiVersion string app.fybrik.io/v1alpha1 true kind string Plotter true metadata object Refer to the Kubernetes API documentation for the fields of the `metadata` field. true spec object PlotterSpec defines the desired state of Plotter, which is applied in a multi-clustered environment. Plotter declares what needs to be installed and where (as blueprints running on remote clusters) which provides the Data Scientist's application with secure and governed access to the data requested in the FybrikApplication. true status object PlotterStatus defines the observed state of Plotter This includes readiness, error message, and indicators received from blueprint resources owned by the Plotter for cleanup and status monitoring false","title":"Plotter"},{"location":"reference/crds/#plotterspec","text":"\u21a9 Parent PlotterSpec defines the desired state of Plotter, which is applied in a multi-clustered environment. Plotter declares what needs to be installed and where (as blueprints running on remote clusters) which provides the Data Scientist's application with secure and governed access to the data requested in the FybrikApplication. Name Type Description Required assets map[string]object Assets is a map holding information about the assets The key is the assetID true flows []object true modulesNamespace string ModulesNamespace is the namespace where modules should be allocated true templates map[string]object Templates is a map holding the templates used in this plotter steps The key is the template name true appInfo object Application context to be transferred to the modules false appSelector object Selector enables to connect the resource to the application Application labels should match the labels in the selector. For some flows the selector may not be used. false","title":"Plotter.spec"},{"location":"reference/crds/#plotterspecassetskey","text":"\u21a9 Parent AssetDetails is a list of assets used in the fybrikapplication. In addition to assets declared in fybrikapplication, AssetDetails list also contains assets that are allocated by the control-plane in order to serve fybrikapplication Name Type Description Required assetDetails object DataStore contains the details for accesing the data that are sent by catalog connectors Credentials for accesing the data are stored in Vault, in the location represented by Vault property. true advertisedAssetId string AdvertisedAssetID links this asset to asset from fybrikapplication and is used by user facing services false","title":"Plotter.spec.assets[key]"},{"location":"reference/crds/#plotterspecassetskeyassetdetails","text":"\u21a9 Parent DataStore contains the details for accesing the data that are sent by catalog connectors Credentials for accesing the data are stored in Vault, in the location represented by Vault property. Name Type Description Required connection object Connection has the relevant details for accesing the data (url, table, ssl, etc.) true format string Format represents data format (e.g. parquet) as received from catalog connectors false vault map[string]object Holds details for retrieving credentials by the modules from Vault store. It is a map so that different credentials can be stored for the different DataFlow operations. false","title":"Plotter.spec.assets[key].assetDetails"},{"location":"reference/crds/#plotterspecassetskeyassetdetailsconnection","text":"\u21a9 Parent Connection has the relevant details for accesing the data (url, table, ssl, etc.) Name Type Description Required name string true","title":"Plotter.spec.assets[key].assetDetails.connection"},{"location":"reference/crds/#plotterspecassetskeyassetdetailsvaultkey","text":"\u21a9 Parent Holds details for retrieving credentials from Vault store. Name Type Description Required address string Address is Vault address true authPath string AuthPath is the path to auth method i.e. kubernetes true role string Role is the Vault role used for retrieving the credentials true secretPath string SecretPath is the path of the secret holding the Credentials in Vault true","title":"Plotter.spec.assets[key].assetDetails.vault[key]"},{"location":"reference/crds/#plotterspecflowsindex","text":"\u21a9 Parent Flows is the list of data flows driven from fybrikapplication: Each element in the list holds the flow of the data requested in fybrikapplication. Name Type Description Required assetId string AssetID indicates the data set being used in this data flow true flowType enum Type of the flow (e.g. read) Enum : read, write, delete, copy true name string Name of the flow true subFlows []object true","title":"Plotter.spec.flows[index]"},{"location":"reference/crds/#plotterspecflowsindexsubflowsindex","text":"\u21a9 Parent Subflows is a list of data flows which are originated from the same data asset but are triggered differently (e.g., one upon init trigger and one upon workload trigger) Name Type Description Required flowType enum Type of the flow (e.g. read) Enum : read, write, delete, copy true name string Name of the SubFlow true steps [][]object Steps defines a series of sequential/parallel data flow steps The first dimension represents parallel data flows. The second sequential components within the same parallel data flow. true triggers []enum Triggers true","title":"Plotter.spec.flows[index].subFlows[index]"},{"location":"reference/crds/#plotterspecflowsindexsubflowsindexstepsindexindex","text":"\u21a9 Parent DataFlowStep contains details on a single data flow step Name Type Description Required cluster string Name of the cluster this step is executed on true name string Name of the step true template string Template is the name of the template to execute the step The full details of the template can be extracted from Plotter.spec.templates list field. true parameters object Step parameters TODO why not flatten the parameters into this data flow step false","title":"Plotter.spec.flows[index].subFlows[index].steps[index][index]"},{"location":"reference/crds/#plotterspecflowsindexsubflowsindexstepsindexindexparameters","text":"\u21a9 Parent Step parameters TODO why not flatten the parameters into this data flow step Name Type Description Required action []object Actions are the data transformations that the module supports false api object ResourceDetails includes asset connection details false args []object false","title":"Plotter.spec.flows[index].subFlows[index].steps[index][index].parameters"},{"location":"reference/crds/#plotterspecflowsindexsubflowsindexstepsindexindexparametersactionindex","text":"\u21a9 Parent Name Type Description Required name string true","title":"Plotter.spec.flows[index].subFlows[index].steps[index][index].parameters.action[index]"},{"location":"reference/crds/#plotterspecflowsindexsubflowsindexstepsindexindexparametersapi","text":"\u21a9 Parent ResourceDetails includes asset connection details Name Type Description Required connection object Connection information true dataFormat string Data format false","title":"Plotter.spec.flows[index].subFlows[index].steps[index][index].parameters.api"},{"location":"reference/crds/#plotterspecflowsindexsubflowsindexstepsindexindexparametersapiconnection","text":"\u21a9 Parent Connection information Name Type Description Required name string true","title":"Plotter.spec.flows[index].subFlows[index].steps[index][index].parameters.api.connection"},{"location":"reference/crds/#plotterspecflowsindexsubflowsindexstepsindexindexparametersargsindex","text":"\u21a9 Parent StepArgument describes a step: it could be assetID or an endpoint of another step Name Type Description Required api object API holds information for accessing a module instance false assetId string AssetID identifies the source asset of this step false","title":"Plotter.spec.flows[index].subFlows[index].steps[index][index].parameters.args[index]"},{"location":"reference/crds/#plotterspecflowsindexsubflowsindexstepsindexindexparametersargsindexapi","text":"\u21a9 Parent API holds information for accessing a module instance Name Type Description Required connection object Connection information true dataFormat string Data format false","title":"Plotter.spec.flows[index].subFlows[index].steps[index][index].parameters.args[index].api"},{"location":"reference/crds/#plotterspecflowsindexsubflowsindexstepsindexindexparametersargsindexapiconnection","text":"\u21a9 Parent Connection information Name Type Description Required name string true","title":"Plotter.spec.flows[index].subFlows[index].steps[index][index].parameters.args[index].api.connection"},{"location":"reference/crds/#plotterspectemplateskey","text":"\u21a9 Parent Template contains basic information about the required modules to serve the fybrikapplication e.g., the module helm chart name. Name Type Description Required modules []object Modules is a list of dependent modules. e.g., if a plugin module is used then the service module is used in should appear first in the modules list of the same template. If the modules list contains more than one module, the first module in the list is referred to as the \"primary module\" of which all the parameters to this template are sent to. true name string Name of the template false","title":"Plotter.spec.templates[key]"},{"location":"reference/crds/#plotterspectemplateskeymodulesindex","text":"\u21a9 Parent ModuleInfo is a copy of FybrikModule Custom Resource. It contains information to instantiate resource of type FybrikModule. Name Type Description Required capability string Module capability true chart object Chart contains the information needed to use helm to install the capability true name string Name of the module true type string May be one of service, config or plugin Service: Means that the control plane deploys the component that performs the capability Config: Another pre-installed service performs the capability and the module deployed configures it for the particular workload or dataset Plugin: Indicates that this module performs a capability as part of another service or module rather than as a stand-alone module true scope enum Scope indicates at what level the capability is used: workload, asset, cluster If not indicated it is assumed to be asset Enum : asset, workload, cluster false","title":"Plotter.spec.templates[key].modules[index]"},{"location":"reference/crds/#plotterspectemplateskeymodulesindexchart","text":"\u21a9 Parent Chart contains the information needed to use helm to install the capability Name Type Description Required name string Name of helm chart true chartPullSecret string Name of secret containing helm registry credentials false values map[string]string Values to pass to helm chart installation false","title":"Plotter.spec.templates[key].modules[index].chart"},{"location":"reference/crds/#plotterspecappselector","text":"\u21a9 Parent Selector enables to connect the resource to the application Application labels should match the labels in the selector. For some flows the selector may not be used. Name Type Description Required workloadSelector object WorkloadSelector enables to connect the resource to the application Application labels should match the labels in the selector. true clusterName string Cluster name false","title":"Plotter.spec.appSelector"},{"location":"reference/crds/#plotterspecappselectorworkloadselector","text":"\u21a9 Parent WorkloadSelector enables to connect the resource to the application Application labels should match the labels in the selector. Name Type Description Required matchExpressions []object matchExpressions is a list of label selector requirements. The requirements are ANDed. false matchLabels map[string]string matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels map is equivalent to an element of matchExpressions, whose key field is \"key\", the operator is \"In\", and the values array contains only \"value\". The requirements are ANDed. false","title":"Plotter.spec.appSelector.workloadSelector"},{"location":"reference/crds/#plotterspecappselectorworkloadselectormatchexpressionsindex","text":"\u21a9 Parent A label selector requirement is a selector that contains values, a key, and an operator that relates the key and values. Name Type Description Required key string key is the label key that the selector applies to. true operator string operator represents a key's relationship to a set of values. Valid operators are In, NotIn, Exists and DoesNotExist. true values []string values is an array of string values. If the operator is In or NotIn, the values array must be non-empty. If the operator is Exists or DoesNotExist, the values array must be empty. This array is replaced during a strategic merge patch. false","title":"Plotter.spec.appSelector.workloadSelector.matchExpressions[index]"},{"location":"reference/crds/#plotterstatus","text":"\u21a9 Parent PlotterStatus defines the observed state of Plotter This includes readiness, error message, and indicators received from blueprint resources owned by the Plotter for cleanup and status monitoring Name Type Description Required assets map[string]object Assets is a map containing the status per asset. The key of this map is assetId false blueprints map[string]object false conditions []object Conditions represent the possible error and failure conditions false flows map[string]object Flows is a map containing the status for each flow the key is the flow name false observedGeneration integer ObservedGeneration is taken from the Plotter metadata. This is used to determine during reconcile whether reconcile was called because the desired state changed, or whether status of the allocated blueprints should be checked. Format : int64 false observedState object ObservedState includes information to be reported back to the FybrikApplication resource It includes readiness and error indications, as well as user instructions false readyTimestamp string Format : date-time false","title":"Plotter.status"},{"location":"reference/crds/#plotterstatusassetskey","text":"\u21a9 Parent ObservedState represents a part of the generated Blueprint/Plotter resource status that allows update of FybrikApplication status Name Type Description Required error string Error indicates that there has been an error to orchestrate the modules and provides the error message false ready boolean Ready represents that the modules have been orchestrated successfully and the data is ready for usage false","title":"Plotter.status.assets[key]"},{"location":"reference/crds/#plotterstatusblueprintskey","text":"\u21a9 Parent MetaBlueprint defines blueprint metadata (name, namespace) and status Name Type Description Required name string true namespace string true status object BlueprintStatus defines the observed state of Blueprint This includes readiness, error message, and indicators for the Kubernetes resources owned by the Blueprint for cleanup and status monitoring true","title":"Plotter.status.blueprints[key]"},{"location":"reference/crds/#plotterstatusblueprintskeystatus","text":"\u21a9 Parent BlueprintStatus defines the observed state of Blueprint This includes readiness, error message, and indicators for the Kubernetes resources owned by the Blueprint for cleanup and status monitoring Name Type Description Required modules map[string]object ModulesState is a map which holds the status of each module its key is the moduleInstanceName which is the unique name for the deployed instance related to this workload false observedGeneration integer ObservedGeneration is taken from the Blueprint metadata. This is used to determine during reconcile whether reconcile was called because the desired state changed, or whether status of the allocated resources should be checked. Format : int64 false observedState object ObservedState includes information to be reported back to the FybrikApplication resource It includes readiness and error indications, as well as user instructions false releases map[string]integer Releases map each release to the observed generation of the blueprint containing this release. At the end of reconcile, each release should be mapped to the latest blueprint version or be uninstalled. false","title":"Plotter.status.blueprints[key].status"},{"location":"reference/crds/#plotterstatusblueprintskeystatusmoduleskey","text":"\u21a9 Parent ObservedState represents a part of the generated Blueprint/Plotter resource status that allows update of FybrikApplication status Name Type Description Required error string Error indicates that there has been an error to orchestrate the modules and provides the error message false ready boolean Ready represents that the modules have been orchestrated successfully and the data is ready for usage false","title":"Plotter.status.blueprints[key].status.modules[key]"},{"location":"reference/crds/#plotterstatusblueprintskeystatusobservedstate","text":"\u21a9 Parent ObservedState includes information to be reported back to the FybrikApplication resource It includes readiness and error indications, as well as user instructions Name Type Description Required error string Error indicates that there has been an error to orchestrate the modules and provides the error message false ready boolean Ready represents that the modules have been orchestrated successfully and the data is ready for usage false","title":"Plotter.status.blueprints[key].status.observedState"},{"location":"reference/crds/#plotterstatusconditionsindex","text":"\u21a9 Parent Condition describes the state of a FybrikApplication at a certain point. Name Type Description Required type string Type of the condition true message string Message contains the details of the current condition false observedGeneration integer ObservedGeneration is the version of the resource for which the condition has been evaluated Format : int64 false status enum Status of the condition, one of (`True`, `False`, `Unknown`). Enum : True, False, Unknown Default : Unknown false","title":"Plotter.status.conditions[index]"},{"location":"reference/crds/#plotterstatusflowskey","text":"\u21a9 Parent FlowStatus includes information to be reported back to the FybrikApplication resource It holds the status per data flow Name Type Description Required subFlows map[string]object true status object ObservedState includes information about the current flow It includes readiness and error indications, as well as user instructions false","title":"Plotter.status.flows[key]"},{"location":"reference/crds/#plotterstatusflowskeysubflowskey","text":"\u21a9 Parent ObservedState represents a part of the generated Blueprint/Plotter resource status that allows update of FybrikApplication status Name Type Description Required error string Error indicates that there has been an error to orchestrate the modules and provides the error message false ready boolean Ready represents that the modules have been orchestrated successfully and the data is ready for usage false","title":"Plotter.status.flows[key].subFlows[key]"},{"location":"reference/crds/#plotterstatusflowskeystatus","text":"\u21a9 Parent ObservedState includes information about the current flow It includes readiness and error indications, as well as user instructions Name Type Description Required error string Error indicates that there has been an error to orchestrate the modules and provides the error message false ready boolean Ready represents that the modules have been orchestrated successfully and the data is ready for usage false","title":"Plotter.status.flows[key].status"},{"location":"reference/crds/#plotterstatusobservedstate","text":"\u21a9 Parent ObservedState includes information to be reported back to the FybrikApplication resource It includes readiness and error indications, as well as user instructions Name Type Description Required error string Error indicates that there has been an error to orchestrate the modules and provides the error message false ready boolean Ready represents that the modules have been orchestrated successfully and the data is ready for usage false","title":"Plotter.status.observedState"},{"location":"reference/crds/#katalogfybrikiov1alpha1","text":"Resource Types: Asset","title":"katalog.fybrik.io/v1alpha1"},{"location":"reference/crds/#asset","text":"\u21a9 Parent Asset defines an asset in the catalog Name Type Description Required apiVersion string katalog.fybrik.io/v1alpha1 true kind string Asset true metadata object Refer to the Kubernetes API documentation for the fields of the `metadata` field. true spec object true","title":"Asset"},{"location":"reference/crds/#assetspec","text":"\u21a9 Parent Name Type Description Required details object Asset details true metadata object Asset metadata true secretRef object Reference to a Secret resource holding credentials for this asset true","title":"Asset.spec"},{"location":"reference/crds/#assetspecdetails","text":"\u21a9 Parent Asset details Name Type Description Required connection object Connection information true dataFormat string Data format false","title":"Asset.spec.details"},{"location":"reference/crds/#assetspecdetailsconnection","text":"\u21a9 Parent Connection information Name Type Description Required name string true","title":"Asset.spec.details.connection"},{"location":"reference/crds/#assetspecmetadata","text":"\u21a9 Parent Asset metadata Name Type Description Required columns []object Columns associated with the asset false geography string Geography of the resource false name string Name of the resource false owner string Owner of the resource false tags object Tags associated with the asset false","title":"Asset.spec.metadata"},{"location":"reference/crds/#assetspecmetadatacolumnsindex","text":"\u21a9 Parent ResourceColumn represents a column in a tabular resource Name Type Description Required name string Name of the column true tags object Tags associated with the column false","title":"Asset.spec.metadata.columns[index]"},{"location":"reference/crds/#assetspecsecretref","text":"\u21a9 Parent Reference to a Secret resource holding credentials for this asset Name Type Description Required name string Name of the Secret resource true namespace string Namespace of the Secret resource. If it is empty then the asset namespace is used. false","title":"Asset.spec.secretRef"},{"location":"reference/ddc/","text":"Data Distribution Controller Overview of Requirements and Functionality The Data Distribution Controller (DDC) handles the movement of data between data stores. Fybrik uses the DDC to perform an action called \"implicit copy\", i.e. the movement of a data set from one data store to another with possibly some unitary transform applied to that data set. It corresponds to Step 8 in the Architecture . Data can be copied from data store to data store in a large variety of different ways, depending on the types of the data store (e.g. COS, Relational DB) and nature of the data capture (Streamed, Snapshot). This document defines the functionality as well as the boundary conditions of the data distribution controller. Goals This document introduces fundamental concepts of the data distribution component and describes a high-level API for invoking data distributions. The initial focus is on structured (tabular) data. One goal of the data distribution component is to maximize congruence across different data stores and formats by preserving not only the data content but also the structure of the data as faithfully as possible. Fully unstructured data such (e.g. \"binary content\") will also be supported but that is not the focus of the initial version. Semi-structured data will be supported on a case-by-case basis. Non-Goals The focus is on how to invoke data distribution and not the if and when. This document doesn't describe the control component that is required to decide whether, when and how often data should be copied across storage systems. Neither does the data distribution perform any policy enforcement. This is done by the component that controls the data distribution system. High-Level Design Before providing an outline of the API functionality, some fundamental concepts are defined. Data Sets and Data Assets The following definition is aligned with the terminology used in the Watson Knowledge Catalog. Data is organized into data sets and data assets. A data set is a collection of data assets that is administered by a single body using a set of policies. Both data sets and data assets are uniquely identifiable. A data set is a collection of data assets with the same structure. Some examples: A data set is data that resides in a relational database where the database tables or views form the data asset. A data set consisting of objects that reside in a COS bucket where object prefix paths that have a common format are data assets. E.g. a set of partitioned parquet files with the same schema. A data set may be formed by a set of Kafka topics where each topic contains messages in compatible format. A data asset is represented by the content of the topic. The unit of data distribution is the data asset. Data Stores A data store allows access to data sets and data assets. Each store allows to individually access data through a data store specific API, e.g. S3 API or JDBC. Additional properties that are relevant for data distribution: Granularity of data access for reading: Some systems provide access to entire data assets only. (e.g. single unpartitioned files on COS). Other storage systems support queries to retrieve a sub-set (a selection and/or projection) of an individual data assets. (e.g. queries on Db2 or partitioned/bucketed prefixes on COS) Granularity of data access for writing: Fine-granular write access is required to apply delta-updates of individual data assets, i.e. update and insert ( upsert ) operations as well as deletes on record level are needed to process streams of changes. Systems that support fine-granular updates are relational database systems, elastic search indexes, and in-memory caches. Other systems such as traditional parquet files stored on COS or HDFS only allow data assets to be updated in their entirety. More sophisticated storage formats such as Delta Lake , Apache Iceberg or Hudi extend the capabilities of parquet. Fidelity of the type system: Data stores use various different typing systems and have different data models that require type conversions as data is distributed between these systems. For example, when moving the content of an elastic search index into a relational database we are moving between two entirely different data models. In order to minimize loss of information, type specific metadata (technical metadata) may need to be preserved as separate entities. In addition, schema inference might be needed to support certain data distributions. The invoker of the DDC is assumed to have knowledge of the technical metadata present at the source data asset and of the desired technical metadata of that data asset at the target. If the invoker does not specify this the DDC will attempt to infer it where possible. In both cases the source and target technical metadata are returned as part of the result of the data distribution. If the passed source or target technical metadata is inconsistent with the data asset at the source, then the data distribution fails. The version 1.0 of the DDC supports the following data stores: Db2 LUW v10.5 or newer Apache Kafka v2.2 or newer (Raw + Confluent KTable format serialized in JSON or Avro) IBM COS with Parquet, JSON and ORC (using a Stocator based approach) Transformations The data distribution supports simple transformations and filtering such as: - Filtering of individual rows or columns based on condition clauses. - Masking of specific columns. - Encrypting/hashing of specific columns. - Sampling of a subset of rows. This is specifically for creating a derived version of a specific data asset and is NOT to enrich or combine data assets, i.e. this is a not a general purpose computation environment. Data Life-cycle The DDC moves a data asset from a source to a target data store. The copy of the data asset will be retained at the target until explicitly removed by the invoker via the DDC API. API High-level Description The API follows the custom resource definition approach (CRD) for Kubernetes. The following basic CRD types exist: - BatchTransfer : One-time or periodic transfer of a single data asset from a source data store to a destination data store. This is also called snapshotting. This is similar to a job in K8s and will inherit many features from it, e.g. the state is kept in K8s after the batch transfer has completed and must be deleted manually. - SyncTransfer : Continuous synchronization of a single data asset from a source data store to a destination data store. The main use-case is to continuously update a destination data asset as it is typically used in a streaming or change-data-capture scenario. This CRD is similar to a stateful set in K8s. Both transfer types will have the same API concerning the core transfer definitions such as: - The source data store including connection details and data asset. - The path (in Vault) to the credentials required to access the source data store. - The destination data store including connection details and data asset. - The path (in Vault) to the credentials required to access the destination data store. - Transfer properties that define parameters such as schedule, retries, transformations etc. The difference is that SyncTransfer is running continuously, BatchTransfer requires a schedule or is a one-time transfer. Initially we will limit SyncTransfer to the movement of data from Kafka to COS or from Kafka to Db2. The status of the CRD is continuously updated with the state of the data distribution. It is used to detect both success or error situations as well as freshness. It also provides transfer statistics. Using the status of the CRD a user may examine: - where data assets have been moved - when this was last successfully completed (for _BatchTransfer_s) - statistics, i.e. how long this took, how many bytes, rows etc. were transferred - what technical metadata about the data was used at the source/destination Other K8s controllers can watch the objects and subscribe to statistics or technical metadata updates and forward these changes e.g. in dashboards or WKT. Secret management The data distribution API should not define any secrets in the CRD spec in a production environment. For development and testing direct definitions can be used but in a production environment credentials shall be retrieved from the secret provider. The secret provider can be accessed via a REST API using a role and a secret name. This secret name refers to a path in vault. At the movement operator shall not create any secrets in Kubernetes that contain any credentials and credentials shall only be maintained in memory. The fetching of secrets will be executed by the datamover component. The datamover component retrieves configuration from a JSON file that is passed on as a Kubernetes secret. The goal is that vault paths can be specified in this JSON configuration file and will be substituted by values retrieved from the secret provider. The following example illustrates this mechanism: Given the example configuration file: { \"db2URL\": \"jdbc:db2//host1:1234/MYDB\", \"user\": \"myuser\" \"vaultPath\": \"/myvault/db2Password\" } and the following string in vault: {\"password\": \"mypassword\"} The substitution in the datamover will find a JSON field called vaultPath and look up the value using the secret provider. The substitution happens at the same level as the vaultPath field was found. This works whenever the data that is stored in vault is a JSON object itself. The advantage is that the in-memory configuration will be the same as in a dev/test environment after the substitution. The result of the given example after substitution will be: { \"db2URL\": \"jdbc:db2//host1:1234/MYDB\", \"user\": \"myuser\" \"password\": \"mypassword\" } This credential substitution can also be used in the options field of transformations. Error handling The data distribution API is using validation hooks to do simple checks when a CRD is created or updated. This is a first kind of error that will result in an error when creating/updating the CRD. It will specify an error message about which fields are not valid. (e.g. an invalid cron pattern for the schedule property) As validation errors are checked before objects are created they return an error via the Kubernetes API. If an error occurred during a BatchTransfer the status of the CRD will be set to FAILED and a possible error reason will show in the error field. The error messages will differ depending on the type of exception that is thrown in the internal datamover process. The internal datamover process will communicate errors to Kubernetes via a termination message . The content of the termination message will be written into the error field of the BatchTransfer . The error message shall describe the error as good as possible without any stack traces to keep it readable and displayable in a short form. Actions for possible error states: * Pending - Nothing to do. Normal process * Running - Nothing to do. Normal process * Succeeded - Possibly execute on succeeded actions (e.g. updating a catalog, ...) * Failed - Operator will try to recover. * Fatal - Operator could not recover. Possibly recreate CRD to resolve and investigate error further. Events In addition to errors the datamover application that is called by the data distribution api will publish Kubernetes events for the CRD in order to give feedback for errors and successes. Errors will contain the error message. Successful messages will contain additional metrics such as number of transferred rows or technical metadata information. API Specification The formalism to use to describe this is to be decided, possibilities are Go using kubebuilder OR CRD directly. As the definition of transfer specific parameters is the same for BatchTransfer kind and SyncTransfer kind the definition below focusses on the BatchTransfer kind. (Think of it like a pod template definition that is the same for a job or a deployment) A possible but not complete list of Go structs using kubebuilder is: // BatchTransferSpec defines the desired state of BatchTransfer type BatchTransferSpec struct { Source DataStore `json:\"source\"` Destination DataStore `json:\"destination\"` Transformation []Transformation `json:\"transformation,omitempty\"` Schedule string `json:\"schedule,omitempty\"` Image string `json:\"image\"` // Has default value from webhook ImagePullPolicy corev1.PullPolicy `json:\"imagePullPolicy\"` // Has default value from webhook SecretProviderURL string `json:\"secretProviderURL\"` // Has default value from webhook SecretProviderRole string `json:\"secretProviderRole\"` // Has default value from webhook Suspend bool `json:\"suspend,omitempty\"` // Has default value from webhook MaxFailedRetries int `json:\"maxFailedRetries,omitempty\"` // Has default value from webhook SuccessfulJobHistoryLimit int `json:\"successfulJobHistoryLimit,omitempty\"` // Has default value from webhook FailedJobHistoryLimit int `json:\"failedJobHistoryLimit,omitempty\"` // Has default value from webhook } type DataStore struct { DataAsset string `json:\"dataAsset\"` Database *Database `json:\"database,omitempty\"` S3 *S3 `json:\"s3,omitempty\"` Kafka *Kafka `json:\"kafka,omitempty\"` } type Database struct { Db2URL string `json:\"db2URL\"` User string `json:\"user\"` Password *string `json:\"password,omitempty\"` // Please use for dev/test only! VaultPath *string `json:\"vaultPath,omitempty\"` } type S3 struct { Endpoint string `json:\"endpoint\"` Region string `json:\"region,omitempty\"` Bucket string `json:\"bucket\"` AccessKey *string `json:\"accessKey,omitempty\"` // Please use for dev/test only! SecretKey *string `json:\"secretKey,omitempty\"` // Please use for dev/test only! VaultPath *string `json:\"vaultPath,omitempty\"` ObjectKey string `json:\"objectKey\"` DataFormat string `json:\"dataFormat,omitempty\"` } type Kafka struct { KafkaBrokers string `json:\"kafkaBrokers\"` SchemaRegistryURL string `json:\"schemaRegistryURL\"` User string `json:\"user\"` Password *string `json:\"password,omitempty\"` // Please use for dev/test only! VaultPath *string `json:\"vaultPath,omitempty\"` SslTruststoreLocation string `json:\"sslTruststoreLocation,omitempty\"` SslTruststorePassword string `json:\"sslTruststorePassword,omitempty\"` KafkaTopic string `json:\"kafkaTopic\"` CreateSnapshot bool `json:\"createSnapshot,omitempty\"` } type Transformation struct { Name string `json:\"name,omitempty\"` Action Action `json:\"action,omitempty\"` Columns []string `json:\"columns,omitempty\"` Options map[string]string `json:\"options,omitempty\"` } type Action string const ( RemoveColumn Action = \"RemoveColumn\" Filter Action = \"Filter\" Encrypt Action = \"Encrypt\" Sample Action = \"Sample\" Digest Action = \"Digest\" // md5, sha1, crc32, sha256, sha512, xxhash32, xxhash64, murmur32 Redact Action = \"Redact\" // random, fixed, formatted, etc ) // BatchTransferStatus defines the observed state of BatchTransfer type BatchTransferStatus struct { Active *corev1.ObjectReference `json:\"active,omitempty\"` Status Status `json:\"status,omitempty\"` Error string `json:\"status,omitempty\"` LastCompleted *corev1.ObjectReference `json:\"lastCompleted,omitempty\"` LastFailed *corev1.ObjectReference `json:\"lastFailed,omitempty\"` LastSuccessTime *metav1.Time `json:\"lastSuccessTime,omitempty\"` LastRecordTime *metav1.Time `json:\"lastRecordTime,omitempty\"` NumRecords int64 `json:\"numRecords,omitempty\"` LastScheduleTime *metav1.Time `json:\"lastScheduleTime,omitempty\"` } // +kubebuilder:validation:Enum=Pending;Running;Succeeded;Failed;Fatal;ConfigurationError type Status string const ( Pending Status = \"Pending\" // Starting up transfers Running Status = \"Running\" // Transfers are running Succeeded Status = \"Succeeded\" // Transfers succeeded Failed Status = \"Failed\" // Transfers failed (Maybe recoverable (e.g. temporary connection issues)) Fatal Status = \"Fatal\" // Fatal. Cannot recover. Manual intervention needed ) // +kubebuilder:object:root=true // BatchTransfer is the Schema for the batchtransfers API type BatchTransfer struct { metav1.TypeMeta `json:\",inline\"` metav1.ObjectMeta `json:\"metadata,omitempty\"` Spec BatchTransferSpec `json:\"spec,omitempty\"` Status BatchTransferStatus `json:\"status,omitempty\"` } // +kubebuilder:object:root=true // BatchTransferList contains a list of BatchTransfer type BatchTransferList struct { metav1.TypeMeta `json:\",inline\"` metav1.ListMeta `json:\"metadata,omitempty\"` Assets []BatchTransfer `json:\"assets\"` } Examples --- apiVersion: \"fybrik.io/v1\" kind: BatchTransfer metadata: name: copy1 namespace: myNamespace spec: source: db: db2URL: \"jdbc:db2://db1.psvc-dev.zc2.ibm.com:50602/DHUBS:sslConnection=true;\" user: myuser password: \"mypassword\" destination: cos: endpoint: s3... bucket: myBucket accessKey: 0123 secretKey: 0123 transformation: - name: \"Remove column A\" action: \"RemoveColumn\" columns: [\"A\"] - name: \"Digest column B\" action: \"Digest\" columns: [\"B\"] options: algo: \"md5\" schedule: null # Cron schedule definition if needed maxFailedRetries: 3 # Maximum retries if failed suspend: false successfulJobsHistoryLimit: 2 failedJobsHistoryLimit: 5 status: lastCompleted: corev1.ObjectReference # Reference to child K8s objects lastScheduledTime: 2018-01-01T00:00:00Z lastSuccessTime: 2018-01-01T00:00:00Z lastRecordTime: 2018-01-01T00:00:00Z # inspect data? numRecords: 23113 External Dependencies Data distribution will be implemented in different ways, depending on the distribution kind, on the source and destination data store technologies as well as depending on the requested transformations. The control layer of the data distribution is implemented following the operator pattern of Kubernetes. In addition, the following technologies are relevant for specific distribution scenarios: - Redhat Debezium for Change Data Capture - IBM Event Streams (Apache Kafka) for SyncTransfer - Apache Spark - Db2 client - COS client - Reference to IBM Specific JDBC driver for streaming into a relation database. Relevant Code Repositories The data distribution core libraries that are Scala/Spark based The data distribution operator that is the operator of data distribution and will be integrated into Fybrik code as part of the manager in future. Roadmap Integration with Parquet Encryption + KeyProtect (As Target) Integration with Iceberg (As Target) Integration with Relational Databases (As Target) Integration with KTables (As Source)","title":"Data Distribution Controller"},{"location":"reference/ddc/#data-distribution-controller","text":"","title":"Data Distribution Controller"},{"location":"reference/ddc/#overview-of-requirements-and-functionality","text":"The Data Distribution Controller (DDC) handles the movement of data between data stores. Fybrik uses the DDC to perform an action called \"implicit copy\", i.e. the movement of a data set from one data store to another with possibly some unitary transform applied to that data set. It corresponds to Step 8 in the Architecture . Data can be copied from data store to data store in a large variety of different ways, depending on the types of the data store (e.g. COS, Relational DB) and nature of the data capture (Streamed, Snapshot). This document defines the functionality as well as the boundary conditions of the data distribution controller.","title":"Overview of Requirements and Functionality"},{"location":"reference/ddc/#goals","text":"This document introduces fundamental concepts of the data distribution component and describes a high-level API for invoking data distributions. The initial focus is on structured (tabular) data. One goal of the data distribution component is to maximize congruence across different data stores and formats by preserving not only the data content but also the structure of the data as faithfully as possible. Fully unstructured data such (e.g. \"binary content\") will also be supported but that is not the focus of the initial version. Semi-structured data will be supported on a case-by-case basis.","title":"Goals"},{"location":"reference/ddc/#non-goals","text":"The focus is on how to invoke data distribution and not the if and when. This document doesn't describe the control component that is required to decide whether, when and how often data should be copied across storage systems. Neither does the data distribution perform any policy enforcement. This is done by the component that controls the data distribution system.","title":"Non-Goals"},{"location":"reference/ddc/#high-level-design","text":"Before providing an outline of the API functionality, some fundamental concepts are defined.","title":"High-Level Design"},{"location":"reference/ddc/#data-sets-and-data-assets","text":"The following definition is aligned with the terminology used in the Watson Knowledge Catalog. Data is organized into data sets and data assets. A data set is a collection of data assets that is administered by a single body using a set of policies. Both data sets and data assets are uniquely identifiable. A data set is a collection of data assets with the same structure. Some examples: A data set is data that resides in a relational database where the database tables or views form the data asset. A data set consisting of objects that reside in a COS bucket where object prefix paths that have a common format are data assets. E.g. a set of partitioned parquet files with the same schema. A data set may be formed by a set of Kafka topics where each topic contains messages in compatible format. A data asset is represented by the content of the topic. The unit of data distribution is the data asset.","title":"Data Sets and Data Assets"},{"location":"reference/ddc/#data-stores","text":"A data store allows access to data sets and data assets. Each store allows to individually access data through a data store specific API, e.g. S3 API or JDBC. Additional properties that are relevant for data distribution: Granularity of data access for reading: Some systems provide access to entire data assets only. (e.g. single unpartitioned files on COS). Other storage systems support queries to retrieve a sub-set (a selection and/or projection) of an individual data assets. (e.g. queries on Db2 or partitioned/bucketed prefixes on COS) Granularity of data access for writing: Fine-granular write access is required to apply delta-updates of individual data assets, i.e. update and insert ( upsert ) operations as well as deletes on record level are needed to process streams of changes. Systems that support fine-granular updates are relational database systems, elastic search indexes, and in-memory caches. Other systems such as traditional parquet files stored on COS or HDFS only allow data assets to be updated in their entirety. More sophisticated storage formats such as Delta Lake , Apache Iceberg or Hudi extend the capabilities of parquet. Fidelity of the type system: Data stores use various different typing systems and have different data models that require type conversions as data is distributed between these systems. For example, when moving the content of an elastic search index into a relational database we are moving between two entirely different data models. In order to minimize loss of information, type specific metadata (technical metadata) may need to be preserved as separate entities. In addition, schema inference might be needed to support certain data distributions. The invoker of the DDC is assumed to have knowledge of the technical metadata present at the source data asset and of the desired technical metadata of that data asset at the target. If the invoker does not specify this the DDC will attempt to infer it where possible. In both cases the source and target technical metadata are returned as part of the result of the data distribution. If the passed source or target technical metadata is inconsistent with the data asset at the source, then the data distribution fails. The version 1.0 of the DDC supports the following data stores: Db2 LUW v10.5 or newer Apache Kafka v2.2 or newer (Raw + Confluent KTable format serialized in JSON or Avro) IBM COS with Parquet, JSON and ORC (using a Stocator based approach)","title":"Data Stores"},{"location":"reference/ddc/#transformations","text":"The data distribution supports simple transformations and filtering such as: - Filtering of individual rows or columns based on condition clauses. - Masking of specific columns. - Encrypting/hashing of specific columns. - Sampling of a subset of rows. This is specifically for creating a derived version of a specific data asset and is NOT to enrich or combine data assets, i.e. this is a not a general purpose computation environment.","title":"Transformations"},{"location":"reference/ddc/#data-life-cycle","text":"The DDC moves a data asset from a source to a target data store. The copy of the data asset will be retained at the target until explicitly removed by the invoker via the DDC API.","title":"Data Life-cycle"},{"location":"reference/ddc/#api-high-level-description","text":"The API follows the custom resource definition approach (CRD) for Kubernetes. The following basic CRD types exist: - BatchTransfer : One-time or periodic transfer of a single data asset from a source data store to a destination data store. This is also called snapshotting. This is similar to a job in K8s and will inherit many features from it, e.g. the state is kept in K8s after the batch transfer has completed and must be deleted manually. - SyncTransfer : Continuous synchronization of a single data asset from a source data store to a destination data store. The main use-case is to continuously update a destination data asset as it is typically used in a streaming or change-data-capture scenario. This CRD is similar to a stateful set in K8s. Both transfer types will have the same API concerning the core transfer definitions such as: - The source data store including connection details and data asset. - The path (in Vault) to the credentials required to access the source data store. - The destination data store including connection details and data asset. - The path (in Vault) to the credentials required to access the destination data store. - Transfer properties that define parameters such as schedule, retries, transformations etc. The difference is that SyncTransfer is running continuously, BatchTransfer requires a schedule or is a one-time transfer. Initially we will limit SyncTransfer to the movement of data from Kafka to COS or from Kafka to Db2. The status of the CRD is continuously updated with the state of the data distribution. It is used to detect both success or error situations as well as freshness. It also provides transfer statistics. Using the status of the CRD a user may examine: - where data assets have been moved - when this was last successfully completed (for _BatchTransfer_s) - statistics, i.e. how long this took, how many bytes, rows etc. were transferred - what technical metadata about the data was used at the source/destination Other K8s controllers can watch the objects and subscribe to statistics or technical metadata updates and forward these changes e.g. in dashboards or WKT.","title":"API High-level Description"},{"location":"reference/ddc/#secret-management","text":"The data distribution API should not define any secrets in the CRD spec in a production environment. For development and testing direct definitions can be used but in a production environment credentials shall be retrieved from the secret provider. The secret provider can be accessed via a REST API using a role and a secret name. This secret name refers to a path in vault. At the movement operator shall not create any secrets in Kubernetes that contain any credentials and credentials shall only be maintained in memory. The fetching of secrets will be executed by the datamover component. The datamover component retrieves configuration from a JSON file that is passed on as a Kubernetes secret. The goal is that vault paths can be specified in this JSON configuration file and will be substituted by values retrieved from the secret provider. The following example illustrates this mechanism: Given the example configuration file: { \"db2URL\": \"jdbc:db2//host1:1234/MYDB\", \"user\": \"myuser\" \"vaultPath\": \"/myvault/db2Password\" } and the following string in vault: {\"password\": \"mypassword\"} The substitution in the datamover will find a JSON field called vaultPath and look up the value using the secret provider. The substitution happens at the same level as the vaultPath field was found. This works whenever the data that is stored in vault is a JSON object itself. The advantage is that the in-memory configuration will be the same as in a dev/test environment after the substitution. The result of the given example after substitution will be: { \"db2URL\": \"jdbc:db2//host1:1234/MYDB\", \"user\": \"myuser\" \"password\": \"mypassword\" } This credential substitution can also be used in the options field of transformations.","title":"Secret management"},{"location":"reference/ddc/#error-handling","text":"The data distribution API is using validation hooks to do simple checks when a CRD is created or updated. This is a first kind of error that will result in an error when creating/updating the CRD. It will specify an error message about which fields are not valid. (e.g. an invalid cron pattern for the schedule property) As validation errors are checked before objects are created they return an error via the Kubernetes API. If an error occurred during a BatchTransfer the status of the CRD will be set to FAILED and a possible error reason will show in the error field. The error messages will differ depending on the type of exception that is thrown in the internal datamover process. The internal datamover process will communicate errors to Kubernetes via a termination message . The content of the termination message will be written into the error field of the BatchTransfer . The error message shall describe the error as good as possible without any stack traces to keep it readable and displayable in a short form. Actions for possible error states: * Pending - Nothing to do. Normal process * Running - Nothing to do. Normal process * Succeeded - Possibly execute on succeeded actions (e.g. updating a catalog, ...) * Failed - Operator will try to recover. * Fatal - Operator could not recover. Possibly recreate CRD to resolve and investigate error further.","title":"Error handling"},{"location":"reference/ddc/#events","text":"In addition to errors the datamover application that is called by the data distribution api will publish Kubernetes events for the CRD in order to give feedback for errors and successes. Errors will contain the error message. Successful messages will contain additional metrics such as number of transferred rows or technical metadata information.","title":"Events"},{"location":"reference/ddc/#api-specification","text":"The formalism to use to describe this is to be decided, possibilities are Go using kubebuilder OR CRD directly. As the definition of transfer specific parameters is the same for BatchTransfer kind and SyncTransfer kind the definition below focusses on the BatchTransfer kind. (Think of it like a pod template definition that is the same for a job or a deployment) A possible but not complete list of Go structs using kubebuilder is: // BatchTransferSpec defines the desired state of BatchTransfer type BatchTransferSpec struct { Source DataStore `json:\"source\"` Destination DataStore `json:\"destination\"` Transformation []Transformation `json:\"transformation,omitempty\"` Schedule string `json:\"schedule,omitempty\"` Image string `json:\"image\"` // Has default value from webhook ImagePullPolicy corev1.PullPolicy `json:\"imagePullPolicy\"` // Has default value from webhook SecretProviderURL string `json:\"secretProviderURL\"` // Has default value from webhook SecretProviderRole string `json:\"secretProviderRole\"` // Has default value from webhook Suspend bool `json:\"suspend,omitempty\"` // Has default value from webhook MaxFailedRetries int `json:\"maxFailedRetries,omitempty\"` // Has default value from webhook SuccessfulJobHistoryLimit int `json:\"successfulJobHistoryLimit,omitempty\"` // Has default value from webhook FailedJobHistoryLimit int `json:\"failedJobHistoryLimit,omitempty\"` // Has default value from webhook } type DataStore struct { DataAsset string `json:\"dataAsset\"` Database *Database `json:\"database,omitempty\"` S3 *S3 `json:\"s3,omitempty\"` Kafka *Kafka `json:\"kafka,omitempty\"` } type Database struct { Db2URL string `json:\"db2URL\"` User string `json:\"user\"` Password *string `json:\"password,omitempty\"` // Please use for dev/test only! VaultPath *string `json:\"vaultPath,omitempty\"` } type S3 struct { Endpoint string `json:\"endpoint\"` Region string `json:\"region,omitempty\"` Bucket string `json:\"bucket\"` AccessKey *string `json:\"accessKey,omitempty\"` // Please use for dev/test only! SecretKey *string `json:\"secretKey,omitempty\"` // Please use for dev/test only! VaultPath *string `json:\"vaultPath,omitempty\"` ObjectKey string `json:\"objectKey\"` DataFormat string `json:\"dataFormat,omitempty\"` } type Kafka struct { KafkaBrokers string `json:\"kafkaBrokers\"` SchemaRegistryURL string `json:\"schemaRegistryURL\"` User string `json:\"user\"` Password *string `json:\"password,omitempty\"` // Please use for dev/test only! VaultPath *string `json:\"vaultPath,omitempty\"` SslTruststoreLocation string `json:\"sslTruststoreLocation,omitempty\"` SslTruststorePassword string `json:\"sslTruststorePassword,omitempty\"` KafkaTopic string `json:\"kafkaTopic\"` CreateSnapshot bool `json:\"createSnapshot,omitempty\"` } type Transformation struct { Name string `json:\"name,omitempty\"` Action Action `json:\"action,omitempty\"` Columns []string `json:\"columns,omitempty\"` Options map[string]string `json:\"options,omitempty\"` } type Action string const ( RemoveColumn Action = \"RemoveColumn\" Filter Action = \"Filter\" Encrypt Action = \"Encrypt\" Sample Action = \"Sample\" Digest Action = \"Digest\" // md5, sha1, crc32, sha256, sha512, xxhash32, xxhash64, murmur32 Redact Action = \"Redact\" // random, fixed, formatted, etc ) // BatchTransferStatus defines the observed state of BatchTransfer type BatchTransferStatus struct { Active *corev1.ObjectReference `json:\"active,omitempty\"` Status Status `json:\"status,omitempty\"` Error string `json:\"status,omitempty\"` LastCompleted *corev1.ObjectReference `json:\"lastCompleted,omitempty\"` LastFailed *corev1.ObjectReference `json:\"lastFailed,omitempty\"` LastSuccessTime *metav1.Time `json:\"lastSuccessTime,omitempty\"` LastRecordTime *metav1.Time `json:\"lastRecordTime,omitempty\"` NumRecords int64 `json:\"numRecords,omitempty\"` LastScheduleTime *metav1.Time `json:\"lastScheduleTime,omitempty\"` } // +kubebuilder:validation:Enum=Pending;Running;Succeeded;Failed;Fatal;ConfigurationError type Status string const ( Pending Status = \"Pending\" // Starting up transfers Running Status = \"Running\" // Transfers are running Succeeded Status = \"Succeeded\" // Transfers succeeded Failed Status = \"Failed\" // Transfers failed (Maybe recoverable (e.g. temporary connection issues)) Fatal Status = \"Fatal\" // Fatal. Cannot recover. Manual intervention needed ) // +kubebuilder:object:root=true // BatchTransfer is the Schema for the batchtransfers API type BatchTransfer struct { metav1.TypeMeta `json:\",inline\"` metav1.ObjectMeta `json:\"metadata,omitempty\"` Spec BatchTransferSpec `json:\"spec,omitempty\"` Status BatchTransferStatus `json:\"status,omitempty\"` } // +kubebuilder:object:root=true // BatchTransferList contains a list of BatchTransfer type BatchTransferList struct { metav1.TypeMeta `json:\",inline\"` metav1.ListMeta `json:\"metadata,omitempty\"` Assets []BatchTransfer `json:\"assets\"` }","title":"API Specification"},{"location":"reference/ddc/#examples","text":"--- apiVersion: \"fybrik.io/v1\" kind: BatchTransfer metadata: name: copy1 namespace: myNamespace spec: source: db: db2URL: \"jdbc:db2://db1.psvc-dev.zc2.ibm.com:50602/DHUBS:sslConnection=true;\" user: myuser password: \"mypassword\" destination: cos: endpoint: s3... bucket: myBucket accessKey: 0123 secretKey: 0123 transformation: - name: \"Remove column A\" action: \"RemoveColumn\" columns: [\"A\"] - name: \"Digest column B\" action: \"Digest\" columns: [\"B\"] options: algo: \"md5\" schedule: null # Cron schedule definition if needed maxFailedRetries: 3 # Maximum retries if failed suspend: false successfulJobsHistoryLimit: 2 failedJobsHistoryLimit: 5 status: lastCompleted: corev1.ObjectReference # Reference to child K8s objects lastScheduledTime: 2018-01-01T00:00:00Z lastSuccessTime: 2018-01-01T00:00:00Z lastRecordTime: 2018-01-01T00:00:00Z # inspect data? numRecords: 23113","title":"Examples"},{"location":"reference/ddc/#external-dependencies","text":"Data distribution will be implemented in different ways, depending on the distribution kind, on the source and destination data store technologies as well as depending on the requested transformations. The control layer of the data distribution is implemented following the operator pattern of Kubernetes. In addition, the following technologies are relevant for specific distribution scenarios: - Redhat Debezium for Change Data Capture - IBM Event Streams (Apache Kafka) for SyncTransfer - Apache Spark - Db2 client - COS client - Reference to IBM Specific JDBC driver for streaming into a relation database.","title":"External Dependencies"},{"location":"reference/ddc/#relevant-code-repositories","text":"The data distribution core libraries that are Scala/Spark based The data distribution operator that is the operator of data distribution and will be integrated into Fybrik code as part of the manager in future.","title":"Relevant Code Repositories"},{"location":"reference/ddc/#roadmap","text":"Integration with Parquet Encryption + KeyProtect (As Target) Integration with Iceberg (As Target) Integration with Relational Databases (As Target) Integration with KTables (As Source)","title":"Roadmap"},{"location":"reference/katalog/","text":"Katalog Katalog is a data catalog that is included in Fybrik for evaluation purposes. It is powered by Kubernetes resources: Asset CRD for managing data assets Secret resources for managing data access credentials Usage An Asset CRD includes a reference to a credentials Secret , connection information, and other metadata such as columns and associated security tags. Apply it like any other Kubernetes resource. Access credenditals are stored in Kubernetes Secret resources. You can use Basic authentication secrets or Opaque secrets with the following keys: Name Type Description Required access_key string Access key also known as AccessKeyId false secret_key string Secret key also known as SecretAccessKey false api_key string API key used in various IAM enabled services false password string Password for basic authentication false username string Username for basic authentication false Manage users Kubernetes RBAC is used for user management: To view Asset resources a Kubernetes user must be granted the katalog-viewer cluster role. To manage Asset resources a Kubernetes user must be granted the katalog-editor cluster role. As always, create a RoleBinding to grant these permissions to assets in a specific namespace and a ClusterRoleBinding to grant these premissions cluster wide.","title":"Katalog"},{"location":"reference/katalog/#katalog","text":"Katalog is a data catalog that is included in Fybrik for evaluation purposes. It is powered by Kubernetes resources: Asset CRD for managing data assets Secret resources for managing data access credentials","title":"Katalog"},{"location":"reference/katalog/#usage","text":"An Asset CRD includes a reference to a credentials Secret , connection information, and other metadata such as columns and associated security tags. Apply it like any other Kubernetes resource. Access credenditals are stored in Kubernetes Secret resources. You can use Basic authentication secrets or Opaque secrets with the following keys: Name Type Description Required access_key string Access key also known as AccessKeyId false secret_key string Secret key also known as SecretAccessKey false api_key string API key used in various IAM enabled services false password string Password for basic authentication false username string Username for basic authentication false","title":"Usage"},{"location":"reference/katalog/#manage-users","text":"Kubernetes RBAC is used for user management: To view Asset resources a Kubernetes user must be granted the katalog-viewer cluster role. To manage Asset resources a Kubernetes user must be granted the katalog-editor cluster role. As always, create a RoleBinding to grant these permissions to assets in a specific namespace and a ClusterRoleBinding to grant these premissions cluster wide.","title":"Manage users"},{"location":"reference/connectors-datacatalog/","text":"Data Catalog Service - Asset Details Documentation for API Endpoints All URIs are relative to https://localhost:8080 Class Method HTTP request Description DefaultApi createAsset POST /createAsset This REST API writes data asset information to the data catalog configured in fybrik DefaultApi deleteAsset DELETE /deleteAsset This REST API deletes data asset DefaultApi getAssetInfo POST /getAssetInfo This REST API gets data asset information from the data catalog configured in fybrik for the data sets indicated in FybrikApplication yaml DefaultApi updateAsset PATCH /updateAsset This REST API updates data asset information in the data catalog configured in fybrik Documentation for Models Connection CreateAssetRequest CreateAssetResponse DeleteAssetRequest DeleteAssetResponse GetAssetRequest GetAssetResponse OperationType ResourceColumn ResourceDetails ResourceMetadata UpdateAssetRequest UpdateAssetResponse Documentation for Authorization All endpoints do not require authorization.","title":"Data catalog"},{"location":"reference/connectors-datacatalog/#data-catalog-service-asset-details","text":"","title":"Data Catalog Service - Asset Details"},{"location":"reference/connectors-datacatalog/#documentation-for-api-endpoints","text":"All URIs are relative to https://localhost:8080 Class Method HTTP request Description DefaultApi createAsset POST /createAsset This REST API writes data asset information to the data catalog configured in fybrik DefaultApi deleteAsset DELETE /deleteAsset This REST API deletes data asset DefaultApi getAssetInfo POST /getAssetInfo This REST API gets data asset information from the data catalog configured in fybrik for the data sets indicated in FybrikApplication yaml DefaultApi updateAsset PATCH /updateAsset This REST API updates data asset information in the data catalog configured in fybrik","title":"Documentation for API Endpoints"},{"location":"reference/connectors-datacatalog/#documentation-for-models","text":"Connection CreateAssetRequest CreateAssetResponse DeleteAssetRequest DeleteAssetResponse GetAssetRequest GetAssetResponse OperationType ResourceColumn ResourceDetails ResourceMetadata UpdateAssetRequest UpdateAssetResponse","title":"Documentation for Models"},{"location":"reference/connectors-datacatalog/#documentation-for-authorization","text":"All endpoints do not require authorization.","title":"Documentation for Authorization"},{"location":"reference/connectors-datacatalog/Apis/DefaultApi/","text":"DefaultApi All URIs are relative to https://localhost:8080 Method HTTP request Description createAsset POST /createAsset This REST API writes data asset information to the data catalog configured in fybrik deleteAsset DELETE /deleteAsset This REST API deletes data asset getAssetInfo POST /getAssetInfo This REST API gets data asset information from the data catalog configured in fybrik for the data sets indicated in FybrikApplication yaml updateAsset PATCH /updateAsset This REST API updates data asset information in the data catalog configured in fybrik createAsset CreateAssetResponse createAsset(X-Request-Datacatalog-Write-CredCreateAssetRequest) This REST API writes data asset information to the data catalog configured in fybrik Parameters Name Type Description Notes X-Request-Datacatalog-Write-Cred String This header carries credential information related to accessing the relevant destination catalog. [default to null] CreateAssetRequest CreateAssetRequest Write Asset Request Return type CreateAssetResponse Authorization No authorization required HTTP request headers Content-Type : application/json Accept : application/json [Back to API-Specification] deleteAsset DeleteAssetResponse deleteAsset(X-Request-Datacatalog-CredDeleteAssetRequest) This REST API deletes data asset Parameters Name Type Description Notes X-Request-Datacatalog-Cred String This header carries credential information related to relevant catalog from which the asset information needs to be retrieved. [default to null] DeleteAssetRequest DeleteAssetRequest Delete Asset Request Return type DeleteAssetResponse Authorization No authorization required HTTP request headers Content-Type : application/json Accept : application/json [Back to API-Specification] getAssetInfo GetAssetResponse getAssetInfo(X-Request-Datacatalog-CredGetAssetRequest) This REST API gets data asset information from the data catalog configured in fybrik for the data sets indicated in FybrikApplication yaml Parameters Name Type Description Notes X-Request-Datacatalog-Cred String This header carries credential information related to relevant catalog from which the asset information needs to be retrieved. [default to null] GetAssetRequest GetAssetRequest Data Catalog Request Object. Return type GetAssetResponse Authorization No authorization required HTTP request headers Content-Type : application/json Accept : application/json [Back to API-Specification] updateAsset UpdateAssetResponse updateAsset(X-Request-Datacatalog-Update-CredUpdateAssetRequest) This REST API updates data asset information in the data catalog configured in fybrik Parameters Name Type Description Notes X-Request-Datacatalog-Update-Cred String This header carries credential information related to accessing the relevant destination catalog. [default to null] UpdateAssetRequest UpdateAssetRequest Update Asset Request Return type UpdateAssetResponse Authorization No authorization required HTTP request headers Content-Type : application/json Accept : application/json [Back to API-Specification]","title":"DefaultApi"},{"location":"reference/connectors-datacatalog/Apis/DefaultApi/#defaultapi","text":"All URIs are relative to https://localhost:8080 Method HTTP request Description createAsset POST /createAsset This REST API writes data asset information to the data catalog configured in fybrik deleteAsset DELETE /deleteAsset This REST API deletes data asset getAssetInfo POST /getAssetInfo This REST API gets data asset information from the data catalog configured in fybrik for the data sets indicated in FybrikApplication yaml updateAsset PATCH /updateAsset This REST API updates data asset information in the data catalog configured in fybrik","title":"DefaultApi"},{"location":"reference/connectors-datacatalog/Apis/DefaultApi/#createasset","text":"CreateAssetResponse createAsset(X-Request-Datacatalog-Write-CredCreateAssetRequest) This REST API writes data asset information to the data catalog configured in fybrik","title":"createAsset"},{"location":"reference/connectors-datacatalog/Apis/DefaultApi/#parameters","text":"Name Type Description Notes X-Request-Datacatalog-Write-Cred String This header carries credential information related to accessing the relevant destination catalog. [default to null] CreateAssetRequest CreateAssetRequest Write Asset Request","title":"Parameters"},{"location":"reference/connectors-datacatalog/Apis/DefaultApi/#return-type","text":"CreateAssetResponse","title":"Return type"},{"location":"reference/connectors-datacatalog/Apis/DefaultApi/#authorization","text":"No authorization required","title":"Authorization"},{"location":"reference/connectors-datacatalog/Apis/DefaultApi/#http-request-headers","text":"Content-Type : application/json Accept : application/json [Back to API-Specification]","title":"HTTP request headers"},{"location":"reference/connectors-datacatalog/Apis/DefaultApi/#deleteasset","text":"DeleteAssetResponse deleteAsset(X-Request-Datacatalog-CredDeleteAssetRequest) This REST API deletes data asset","title":"deleteAsset"},{"location":"reference/connectors-datacatalog/Apis/DefaultApi/#parameters_1","text":"Name Type Description Notes X-Request-Datacatalog-Cred String This header carries credential information related to relevant catalog from which the asset information needs to be retrieved. [default to null] DeleteAssetRequest DeleteAssetRequest Delete Asset Request","title":"Parameters"},{"location":"reference/connectors-datacatalog/Apis/DefaultApi/#return-type_1","text":"DeleteAssetResponse","title":"Return type"},{"location":"reference/connectors-datacatalog/Apis/DefaultApi/#authorization_1","text":"No authorization required","title":"Authorization"},{"location":"reference/connectors-datacatalog/Apis/DefaultApi/#http-request-headers_1","text":"Content-Type : application/json Accept : application/json [Back to API-Specification]","title":"HTTP request headers"},{"location":"reference/connectors-datacatalog/Apis/DefaultApi/#getassetinfo","text":"GetAssetResponse getAssetInfo(X-Request-Datacatalog-CredGetAssetRequest) This REST API gets data asset information from the data catalog configured in fybrik for the data sets indicated in FybrikApplication yaml","title":"getAssetInfo"},{"location":"reference/connectors-datacatalog/Apis/DefaultApi/#parameters_2","text":"Name Type Description Notes X-Request-Datacatalog-Cred String This header carries credential information related to relevant catalog from which the asset information needs to be retrieved. [default to null] GetAssetRequest GetAssetRequest Data Catalog Request Object.","title":"Parameters"},{"location":"reference/connectors-datacatalog/Apis/DefaultApi/#return-type_2","text":"GetAssetResponse","title":"Return type"},{"location":"reference/connectors-datacatalog/Apis/DefaultApi/#authorization_2","text":"No authorization required","title":"Authorization"},{"location":"reference/connectors-datacatalog/Apis/DefaultApi/#http-request-headers_2","text":"Content-Type : application/json Accept : application/json [Back to API-Specification]","title":"HTTP request headers"},{"location":"reference/connectors-datacatalog/Apis/DefaultApi/#updateasset","text":"UpdateAssetResponse updateAsset(X-Request-Datacatalog-Update-CredUpdateAssetRequest) This REST API updates data asset information in the data catalog configured in fybrik","title":"updateAsset"},{"location":"reference/connectors-datacatalog/Apis/DefaultApi/#parameters_3","text":"Name Type Description Notes X-Request-Datacatalog-Update-Cred String This header carries credential information related to accessing the relevant destination catalog. [default to null] UpdateAssetRequest UpdateAssetRequest Update Asset Request","title":"Parameters"},{"location":"reference/connectors-datacatalog/Apis/DefaultApi/#return-type_3","text":"UpdateAssetResponse","title":"Return type"},{"location":"reference/connectors-datacatalog/Apis/DefaultApi/#authorization_3","text":"No authorization required","title":"Authorization"},{"location":"reference/connectors-datacatalog/Apis/DefaultApi/#http-request-headers_3","text":"Content-Type : application/json Accept : application/json [Back to API-Specification]","title":"HTTP request headers"},{"location":"reference/connectors-datacatalog/Models/Connection/","text":"Connection Properties Name Type Description Notes name String [default: null] [Back to Model list] [Back to API list] [Back to API-Specification]","title":"Connection"},{"location":"reference/connectors-datacatalog/Models/Connection/#connection","text":"","title":"Connection"},{"location":"reference/connectors-datacatalog/Models/Connection/#properties","text":"Name Type Description Notes name String [default: null] [Back to Model list] [Back to API list] [Back to API-Specification]","title":"Properties"},{"location":"reference/connectors-datacatalog/Models/CreateAssetRequest/","text":"CreateAssetRequest Properties Name Type Description Notes credentials String The vault plugin path where the destination data credentials will be stored as kubernetes secrets [optional] [default: null] destinationAssetID String Asset ID to be used for the created asset [optional] [default: null] destinationCatalogID String The destination catalog id in which the new asset will be created based on the information provided in ResourceMetadata and ResourceDetails field [default: null] details Details [default: null] resourceMetadata ResourceMetadata [default: null] [Back to Model list] [Back to API list] [Back to API-Specification]","title":"CreateAssetRequest"},{"location":"reference/connectors-datacatalog/Models/CreateAssetRequest/#createassetrequest","text":"","title":"CreateAssetRequest"},{"location":"reference/connectors-datacatalog/Models/CreateAssetRequest/#properties","text":"Name Type Description Notes credentials String The vault plugin path where the destination data credentials will be stored as kubernetes secrets [optional] [default: null] destinationAssetID String Asset ID to be used for the created asset [optional] [default: null] destinationCatalogID String The destination catalog id in which the new asset will be created based on the information provided in ResourceMetadata and ResourceDetails field [default: null] details Details [default: null] resourceMetadata ResourceMetadata [default: null] [Back to Model list] [Back to API list] [Back to API-Specification]","title":"Properties"},{"location":"reference/connectors-datacatalog/Models/CreateAssetResponse/","text":"CreateAssetResponse Properties Name Type Description Notes assetID String The ID of the created asset based on the source asset information given in CreateAssetRequest object [default: null] [Back to Model list] [Back to API list] [Back to API-Specification]","title":"CreateAssetResponse"},{"location":"reference/connectors-datacatalog/Models/CreateAssetResponse/#createassetresponse","text":"","title":"CreateAssetResponse"},{"location":"reference/connectors-datacatalog/Models/CreateAssetResponse/#properties","text":"Name Type Description Notes assetID String The ID of the created asset based on the source asset information given in CreateAssetRequest object [default: null] [Back to Model list] [Back to API list] [Back to API-Specification]","title":"Properties"},{"location":"reference/connectors-datacatalog/Models/DeleteAssetRequest/","text":"DeleteAssetRequest Properties Name Type Description Notes assetID String [default: null] [Back to Model list] [Back to API list] [Back to API-Specification]","title":"DeleteAssetRequest"},{"location":"reference/connectors-datacatalog/Models/DeleteAssetRequest/#deleteassetrequest","text":"","title":"DeleteAssetRequest"},{"location":"reference/connectors-datacatalog/Models/DeleteAssetRequest/#properties","text":"Name Type Description Notes assetID String [default: null] [Back to Model list] [Back to API list] [Back to API-Specification]","title":"Properties"},{"location":"reference/connectors-datacatalog/Models/DeleteAssetResponse/","text":"DeleteAssetResponse Properties Name Type Description Notes status String The deletion status [optional] [default: null] [Back to Model list] [Back to API list] [Back to API-Specification]","title":"DeleteAssetResponse"},{"location":"reference/connectors-datacatalog/Models/DeleteAssetResponse/#deleteassetresponse","text":"","title":"DeleteAssetResponse"},{"location":"reference/connectors-datacatalog/Models/DeleteAssetResponse/#properties","text":"Name Type Description Notes status String The deletion status [optional] [default: null] [Back to Model list] [Back to API list] [Back to API-Specification]","title":"Properties"},{"location":"reference/connectors-datacatalog/Models/GetAssetRequest/","text":"GetAssetRequest Properties Name Type Description Notes assetID String [default: null] operationType OperationType [default: null] [Back to Model list] [Back to API list] [Back to API-Specification]","title":"GetAssetRequest"},{"location":"reference/connectors-datacatalog/Models/GetAssetRequest/#getassetrequest","text":"","title":"GetAssetRequest"},{"location":"reference/connectors-datacatalog/Models/GetAssetRequest/#properties","text":"Name Type Description Notes assetID String [default: null] operationType OperationType [default: null] [Back to Model list] [Back to API list] [Back to API-Specification]","title":"Properties"},{"location":"reference/connectors-datacatalog/Models/GetAssetResponse/","text":"GetAssetResponse Properties Name Type Description Notes credentials String Vault plugin path where the data credentials will be stored as kubernetes secrets This value is assumed to be known to the catalog connector. [default: null] details Details [default: null] resourceMetadata ResourceMetadata [default: null] [Back to Model list] [Back to API list] [Back to API-Specification]","title":"GetAssetResponse"},{"location":"reference/connectors-datacatalog/Models/GetAssetResponse/#getassetresponse","text":"","title":"GetAssetResponse"},{"location":"reference/connectors-datacatalog/Models/GetAssetResponse/#properties","text":"Name Type Description Notes credentials String Vault plugin path where the data credentials will be stored as kubernetes secrets This value is assumed to be known to the catalog connector. [default: null] details Details [default: null] resourceMetadata ResourceMetadata [default: null] [Back to Model list] [Back to API list] [Back to API-Specification]","title":"Properties"},{"location":"reference/connectors-datacatalog/Models/OperationType/","text":"OperationType OperationType Type of operation requested for the asset Properties Name Type Description Notes [Back to Model list] [Back to API list] [Back to API-Specification]","title":"OperationType"},{"location":"reference/connectors-datacatalog/Models/OperationType/#operationtype","text":"OperationType Type of operation requested for the asset","title":"OperationType"},{"location":"reference/connectors-datacatalog/Models/OperationType/#properties","text":"Name Type Description Notes [Back to Model list] [Back to API list] [Back to API-Specification]","title":"Properties"},{"location":"reference/connectors-datacatalog/Models/ResourceColumn/","text":"ResourceColumn ResourceColumn represents a column in a tabular resource Properties Name Type Description Notes name String Name of the column [default: null] tags Map [optional] [default: null] [Back to Model list] [Back to API list] [Back to API-Specification]","title":"ResourceColumn"},{"location":"reference/connectors-datacatalog/Models/ResourceColumn/#resourcecolumn","text":"ResourceColumn represents a column in a tabular resource","title":"ResourceColumn"},{"location":"reference/connectors-datacatalog/Models/ResourceColumn/#properties","text":"Name Type Description Notes name String Name of the column [default: null] tags Map [optional] [default: null] [Back to Model list] [Back to API list] [Back to API-Specification]","title":"Properties"},{"location":"reference/connectors-datacatalog/Models/ResourceDetails/","text":"ResourceDetails ResourceDetails includes asset connection details Properties Name Type Description Notes connection Connection [default: null] dataFormat String [optional] [default: null] [Back to Model list] [Back to API list] [Back to API-Specification]","title":"ResourceDetails"},{"location":"reference/connectors-datacatalog/Models/ResourceDetails/#resourcedetails","text":"ResourceDetails includes asset connection details","title":"ResourceDetails"},{"location":"reference/connectors-datacatalog/Models/ResourceDetails/#properties","text":"Name Type Description Notes connection Connection [default: null] dataFormat String [optional] [default: null] [Back to Model list] [Back to API list] [Back to API-Specification]","title":"Properties"},{"location":"reference/connectors-datacatalog/Models/ResourceMetadata/","text":"ResourceMetadata ResourceMetadata defines model for resource metadata Properties Name Type Description Notes columns List Columns associated with the asset [optional] [default: null] geography String Geography of the resource [optional] [default: null] name String Name of the resource [optional] [default: null] owner String Owner of the resource [optional] [default: null] tags Map [optional] [default: null] [Back to Model list] [Back to API list] [Back to API-Specification]","title":"ResourceMetadata"},{"location":"reference/connectors-datacatalog/Models/ResourceMetadata/#resourcemetadata","text":"ResourceMetadata defines model for resource metadata","title":"ResourceMetadata"},{"location":"reference/connectors-datacatalog/Models/ResourceMetadata/#properties","text":"Name Type Description Notes columns List Columns associated with the asset [optional] [default: null] geography String Geography of the resource [optional] [default: null] name String Name of the resource [optional] [default: null] owner String Owner of the resource [optional] [default: null] tags Map [optional] [default: null] [Back to Model list] [Back to API list] [Back to API-Specification]","title":"Properties"},{"location":"reference/connectors-datacatalog/Models/UpdateAssetRequest/","text":"UpdateAssetRequest Properties Name Type Description Notes assetID String [default: null] columns List New columns associated with the asset [optional] [default: null] name String New name of the resource [optional] [default: null] owner String New owner of the resource [optional] [default: null] tags Map [optional] [default: null] [Back to Model list] [Back to API list] [Back to API-Specification]","title":"UpdateAssetRequest"},{"location":"reference/connectors-datacatalog/Models/UpdateAssetRequest/#updateassetrequest","text":"","title":"UpdateAssetRequest"},{"location":"reference/connectors-datacatalog/Models/UpdateAssetRequest/#properties","text":"Name Type Description Notes assetID String [default: null] columns List New columns associated with the asset [optional] [default: null] name String New name of the resource [optional] [default: null] owner String New owner of the resource [optional] [default: null] tags Map [optional] [default: null] [Back to Model list] [Back to API list] [Back to API-Specification]","title":"Properties"},{"location":"reference/connectors-datacatalog/Models/UpdateAssetResponse/","text":"UpdateAssetResponse Properties Name Type Description Notes status String The updation status [optional] [default: null] [Back to Model list] [Back to API list] [Back to API-Specification]","title":"UpdateAssetResponse"},{"location":"reference/connectors-datacatalog/Models/UpdateAssetResponse/#updateassetresponse","text":"","title":"UpdateAssetResponse"},{"location":"reference/connectors-datacatalog/Models/UpdateAssetResponse/#properties","text":"Name Type Description Notes status String The updation status [optional] [default: null] [Back to Model list] [Back to API list] [Back to API-Specification]","title":"Properties"},{"location":"reference/connectors-policymanager/","text":"Policy Manager Service Documentation for API Endpoints All URIs are relative to https://localhost:8080 Class Method HTTP request Description DefaultApi getPoliciesDecisions POST /getPoliciesDecisions This REST API gets data governance decisions for the data sets indicated in FybrikApplication yaml based on the context indicated Documentation for Models Action DataFlow GetPolicyDecisionsRequest GetPolicyDecisionsResponse RequestAction Resource ResourceColumn ResourceMetadata ResultItem Documentation for Authorization All endpoints do not require authorization.","title":"Policy manager"},{"location":"reference/connectors-policymanager/#policy-manager-service","text":"","title":"Policy Manager Service"},{"location":"reference/connectors-policymanager/#documentation-for-api-endpoints","text":"All URIs are relative to https://localhost:8080 Class Method HTTP request Description DefaultApi getPoliciesDecisions POST /getPoliciesDecisions This REST API gets data governance decisions for the data sets indicated in FybrikApplication yaml based on the context indicated","title":"Documentation for API Endpoints"},{"location":"reference/connectors-policymanager/#documentation-for-models","text":"Action DataFlow GetPolicyDecisionsRequest GetPolicyDecisionsResponse RequestAction Resource ResourceColumn ResourceMetadata ResultItem","title":"Documentation for Models"},{"location":"reference/connectors-policymanager/#documentation-for-authorization","text":"All endpoints do not require authorization.","title":"Documentation for Authorization"},{"location":"reference/connectors-policymanager/Apis/DefaultApi/","text":"DefaultApi All URIs are relative to https://localhost:8080 Method HTTP request Description getPoliciesDecisions POST /getPoliciesDecisions This REST API gets data governance decisions for the data sets indicated in FybrikApplication yaml based on the context indicated getPoliciesDecisions GetPolicyDecisionsResponse getPoliciesDecisions(X-Request-CredGetPolicyDecisionsRequest) This REST API gets data governance decisions for the data sets indicated in FybrikApplication yaml based on the context indicated Parameters Name Type Description Notes X-Request-Cred String [default to null] GetPolicyDecisionsRequest GetPolicyDecisionsRequest Policy Manager Request Object. Return type GetPolicyDecisionsResponse Authorization No authorization required HTTP request headers Content-Type : application/json Accept : application/json [Back to API-Specification]","title":"DefaultApi"},{"location":"reference/connectors-policymanager/Apis/DefaultApi/#defaultapi","text":"All URIs are relative to https://localhost:8080 Method HTTP request Description getPoliciesDecisions POST /getPoliciesDecisions This REST API gets data governance decisions for the data sets indicated in FybrikApplication yaml based on the context indicated","title":"DefaultApi"},{"location":"reference/connectors-policymanager/Apis/DefaultApi/#getpoliciesdecisions","text":"GetPolicyDecisionsResponse getPoliciesDecisions(X-Request-CredGetPolicyDecisionsRequest) This REST API gets data governance decisions for the data sets indicated in FybrikApplication yaml based on the context indicated","title":"getPoliciesDecisions"},{"location":"reference/connectors-policymanager/Apis/DefaultApi/#parameters","text":"Name Type Description Notes X-Request-Cred String [default to null] GetPolicyDecisionsRequest GetPolicyDecisionsRequest Policy Manager Request Object.","title":"Parameters"},{"location":"reference/connectors-policymanager/Apis/DefaultApi/#return-type","text":"GetPolicyDecisionsResponse","title":"Return type"},{"location":"reference/connectors-policymanager/Apis/DefaultApi/#authorization","text":"No authorization required","title":"Authorization"},{"location":"reference/connectors-policymanager/Apis/DefaultApi/#http-request-headers","text":"Content-Type : application/json Accept : application/json [Back to API-Specification]","title":"HTTP request headers"},{"location":"reference/connectors-policymanager/Models/Action/","text":"Action Properties Name Type Description Notes name String [default: null] [Back to Model list] [Back to API list] [Back to API-Specification]","title":"Action"},{"location":"reference/connectors-policymanager/Models/Action/#action","text":"","title":"Action"},{"location":"reference/connectors-policymanager/Models/Action/#properties","text":"Name Type Description Notes name String [default: null] [Back to Model list] [Back to API list] [Back to API-Specification]","title":"Properties"},{"location":"reference/connectors-policymanager/Models/DataFlow/","text":"DataFlow Properties Name Type Description Notes [Back to Model list] [Back to API list] [Back to API-Specification]","title":"DataFlow"},{"location":"reference/connectors-policymanager/Models/DataFlow/#dataflow","text":"","title":"DataFlow"},{"location":"reference/connectors-policymanager/Models/DataFlow/#properties","text":"Name Type Description Notes [Back to Model list] [Back to API list] [Back to API-Specification]","title":"Properties"},{"location":"reference/connectors-policymanager/Models/GetPolicyDecisionsRequest/","text":"GetPolicyDecisionsRequest Properties Name Type Description Notes action Action [default: null] context Map [optional] [default: null] resource Resource [default: null] [Back to Model list] [Back to API list] [Back to API-Specification]","title":"GetPolicyDecisionsRequest"},{"location":"reference/connectors-policymanager/Models/GetPolicyDecisionsRequest/#getpolicydecisionsrequest","text":"","title":"GetPolicyDecisionsRequest"},{"location":"reference/connectors-policymanager/Models/GetPolicyDecisionsRequest/#properties","text":"Name Type Description Notes action Action [default: null] context Map [optional] [default: null] resource Resource [default: null] [Back to Model list] [Back to API list] [Back to API-Specification]","title":"Properties"},{"location":"reference/connectors-policymanager/Models/GetPolicyDecisionsResponse/","text":"GetPolicyDecisionsResponse Properties Name Type Description Notes decision_id String [optional] [default: null] result List [default: null] [Back to Model list] [Back to API list] [Back to API-Specification]","title":"GetPolicyDecisionsResponse"},{"location":"reference/connectors-policymanager/Models/GetPolicyDecisionsResponse/#getpolicydecisionsresponse","text":"","title":"GetPolicyDecisionsResponse"},{"location":"reference/connectors-policymanager/Models/GetPolicyDecisionsResponse/#properties","text":"Name Type Description Notes decision_id String [optional] [default: null] result List [default: null] [Back to Model list] [Back to API list] [Back to API-Specification]","title":"Properties"},{"location":"reference/connectors-policymanager/Models/RequestAction/","text":"RequestAction Properties Name Type Description Notes actionType DataFlow [default: null] destination String [optional] [default: null] processingLocation String [optional] [default: null] [Back to Model list] [Back to API list] [Back to API-Specification]","title":"RequestAction"},{"location":"reference/connectors-policymanager/Models/RequestAction/#requestaction","text":"","title":"RequestAction"},{"location":"reference/connectors-policymanager/Models/RequestAction/#properties","text":"Name Type Description Notes actionType DataFlow [default: null] destination String [optional] [default: null] processingLocation String [optional] [default: null] [Back to Model list] [Back to API list] [Back to API-Specification]","title":"Properties"},{"location":"reference/connectors-policymanager/Models/Resource/","text":"Resource Properties Name Type Description Notes id String [default: null] metadata Metadata [optional] [default: null] [Back to Model list] [Back to API list] [Back to API-Specification]","title":"Resource"},{"location":"reference/connectors-policymanager/Models/Resource/#resource","text":"","title":"Resource"},{"location":"reference/connectors-policymanager/Models/Resource/#properties","text":"Name Type Description Notes id String [default: null] metadata Metadata [optional] [default: null] [Back to Model list] [Back to API list] [Back to API-Specification]","title":"Properties"},{"location":"reference/connectors-policymanager/Models/ResourceColumn/","text":"ResourceColumn ResourceColumn represents a column in a tabular resource Properties Name Type Description Notes name String Name of the column [default: null] tags Map [optional] [default: null] [Back to Model list] [Back to API list] [Back to API-Specification]","title":"ResourceColumn"},{"location":"reference/connectors-policymanager/Models/ResourceColumn/#resourcecolumn","text":"ResourceColumn represents a column in a tabular resource","title":"ResourceColumn"},{"location":"reference/connectors-policymanager/Models/ResourceColumn/#properties","text":"Name Type Description Notes name String Name of the column [default: null] tags Map [optional] [default: null] [Back to Model list] [Back to API list] [Back to API-Specification]","title":"Properties"},{"location":"reference/connectors-policymanager/Models/ResourceMetadata/","text":"ResourceMetadata ResourceMetadata defines model for resource metadata Properties Name Type Description Notes columns List Columns associated with the asset [optional] [default: null] geography String Geography of the resource [optional] [default: null] name String Name of the resource [optional] [default: null] owner String Owner of the resource [optional] [default: null] tags Map [optional] [default: null] [Back to Model list] [Back to API list] [Back to API-Specification]","title":"ResourceMetadata"},{"location":"reference/connectors-policymanager/Models/ResourceMetadata/#resourcemetadata","text":"ResourceMetadata defines model for resource metadata","title":"ResourceMetadata"},{"location":"reference/connectors-policymanager/Models/ResourceMetadata/#properties","text":"Name Type Description Notes columns List Columns associated with the asset [optional] [default: null] geography String Geography of the resource [optional] [default: null] name String Name of the resource [optional] [default: null] owner String Owner of the resource [optional] [default: null] tags Map [optional] [default: null] [Back to Model list] [Back to API list] [Back to API-Specification]","title":"Properties"},{"location":"reference/connectors-policymanager/Models/ResultItem/","text":"ResultItem Properties Name Type Description Notes action Action [default: null] policy String The policy on which the decision was based [default: null] [Back to Model list] [Back to API list] [Back to API-Specification]","title":"ResultItem"},{"location":"reference/connectors-policymanager/Models/ResultItem/#resultitem","text":"","title":"ResultItem"},{"location":"reference/connectors-policymanager/Models/ResultItem/#properties","text":"Name Type Description Notes action Action [default: null] policy String The policy on which the decision was based [default: null] [Back to Model list] [Back to API list] [Back to API-Specification]","title":"Properties"},{"location":"samples/chaining-sample/","text":"FybrikModule chaining sample This sample shows how to implement a use case where, based on the data source and governance policies, the Fybrik manager determines that it must deploy two FybrikModules to allow a workload access to a dataset. One FybrikModule handles reading the data and the second does the data transformation. Data is passed between the FybrikModules without writing to intermediate storage. The data read in this example is the userdata dataset, a Parquet file found in https://github.com/Teradata/kylo/blob/master/samples/sample-data/parquet/userdata2.parquet. Two FybrikModules are available for use by the Fybrik control plane: the arrow-flight-module and the airbyte-module . Only the airbyte-module can give read access to the dataset. However, it does not have any data transformation capabilities. Therefore, to satisfy constraints, the Fybrik manager must deploy both modules: the airbyte module for reading the dataset, and the arrow-flight-module for transforming the dataset based on the governance policies. To recreate this scenario, you will need a copy of the Fybrik repository ( git clone https://github.com/fybrik/fybrik.git ), and a copy of the airbyte-module repository ( git clone https://github.com/fybrik/airbyte-module.git ). Set the following environment variables: FYBRIK_DIR for the path of the fybrik directory, and AIRBYTE_MODULE_DIR for the path of the airbyte-module directory. Install Fybrik Prerequisites. Follow the instruction in the Fybrik Quick Start Guide . Stop before the \"Install control plane\" section. Before installing the control plane, we need to customize the Fybrik taxonomy to define the new connection and interface types. Run: cd $FYBRIK_DIR go run main.go taxonomy compile --out custom-taxonomy.json --base charts/fybrik/files/taxonomy/taxonomy.json $AIRBYTE_MODULE_DIR /fybrik/fybrik-taxonomy-customize.yaml helm install fybrik-crd charts/fybrik-crd -n fybrik-system --wait helm install fybrik charts/fybrik --set global.tag = master --set global.imagePullPolicy = Always -n fybrik-system --wait --set-file taxonomyOverride = custom-taxonomy.json Install the Airbyte module: kubectl apply -f $AIRBYTE_MODULE_DIR /module.yaml -n fybrik-system Install the arrow-flight module for transformations: kubectl apply -f https://raw.githubusercontent.com/fybrik/arrow-flight-module/master/module.yaml -n fybrik-system Create a new namespace for the application, and set it as default: kubectl create namespace fybrik-airbyte-sample kubectl config set-context --current --namespace = fybrik-airbyte-sample Create an asset (the userdata asset) in fybrik's mini data catalog, the policy to access it (we use a policy that requires redactions to PII columns), and a FybrikApplication indicating the workload, context, and data requested: kubectl apply -f $AIRBYTE_MODULE_DIR /fybrik/asset.yaml kubectl -n fybrik-system create configmap sample-policy --from-file = $AIRBYTE_MODULE_DIR /fybrik/sample-policy-restrictive.rego kubectl -n fybrik-system label configmap sample-policy openpolicyagent.org/policy = rego while [[ $( kubectl get cm sample-policy -n fybrik-system -o 'jsonpath={.metadata.annotations.openpolicyagent\\.org/policy-status}' ) ! = '{\"status\":\"ok\"}' ]] ; do echo \"waiting for policy to be applied\" && sleep 5 ; done kubectl apply -f $AIRBYTE_MODULE_DIR /fybrik/application.yaml After the FybrikApplication is applied, the Fybrik control plane attempts to create the data path for the application. Fybrik realizes that the Airbyte module can give the application access to the userdata dataset, and that the arrow-flight module could provide the redaction transformation. Fybrik deploys both modules in the fybrik-blueprints namespace. To verify that the Airbyte module and the arrow-flight module were indeed deployed, run: kubectl get pods -n fybrik-blueprints You should see pods with names similar to: NAME READY STATUS RESTARTS AGE my-app-fybrik-airbyte-sample-airbyte-module-airbyte-module4kvrq 2 /2 Running 0 43s my-app-fybrik-airbyte-sample-arrow-flight-module-arrow-flibxsq2 1 /1 Running 0 43s To verify that the Airbyte module gives access to the userdata dataset, run: cd $AIRBYTE_MODULE_DIR /helm/client ./deploy_airbyte_module_client_pod.sh kubectl exec -it my-shell -n default -- python3 /root/client.py --host my-app-fybrik-airbyte-sample-arrow-flight-module.fybrik-blueprints --port 80 --asset fybrik-airbyte-sample/userdata You should see the following output: registration_dttm id first_name last_name email ... country birthdate salary title comments 0 2016 -02-03T13:36:39 1 .0 XXXXX XXXXX XXXXX ... Indonesia XXXXX 140249 .37 Senior Financial Analyst 1 2016 -02-03T00:22:28 2 .0 XXXXX XXXXX XXXXX ... China XXXXX NaN 2 2016 -02-03T18:29:04 3 .0 XXXXX XXXXX XXXXX ... France XXXXX 236219 .26 Teacher 3 2016 -02-03T13:42:19 4 .0 XXXXX XXXXX XXXXX ... Russia XXXXX NaN Nuclear Power Engineer 4 2016 -02-03T00:15:29 5 .0 XXXXX XXXXX XXXXX ... France XXXXX 50210 .02 Senior Editor .. ... ... ... ... ... ... ... ... ... ... ... 995 2016 -02-03T13:36:49 996 .0 XXXXX XXXXX XXXXX ... China XXXXX 185421 .82 \" 996 2016-02-03T04:39:01 997.0 XXXXX XXXXX XXXXX ... Malaysia XXXXX 279671.68 997 2016-02-03T00:33:54 998.0 XXXXX XXXXX XXXXX ... Poland XXXXX 112275.78 998 2016-02-03T00:15:08 999.0 XXXXX XXXXX XXXXX ... Kazakhstan XXXXX 53564.76 Speech Pathologist 999 2016-02-03T00:53:53 1000.0 XXXXX XXXXX XXXXX ... Nigeria XXXXX 239858.70 [1000 rows x 13 columns] Alternatively, one can access the userdata dataset from a Jupyter notebook, as described in the notebook sample . To determine the virtual endpoint from which to access the data set, run: ENDPOINT_SCHEME = $( kubectl get fybrikapplication my-app -o jsonpath ={ .status.assetStates.fybrik-notebook-sample/userdata.endpoint.fybrik-arrow-flight.scheme } ) ENDPOINT_HOSTNAME = $( kubectl get fybrikapplication my-app -o jsonpath ={ .status.assetStates.fybrik-notebook-sample/userdata.endpoint.fybrik-arrow-flight.hostname } ) ENDPOINT_PORT = $( kubectl get fybrikapplication my-app -o jsonpath ={ .status.assetStates.fybrik-notebook-sample/userdata.endpoint.fybrik-arrow-flight.port } ) printf \" ${ ENDPOINT_SCHEME } :// ${ ENDPOINT_HOSTNAME } : ${ ENDPOINT_PORT } \" Note that the virtual endpoint determined from the FybrikApplication status points to the arrow-flight transform module, although this transparent to the user. Insert a new notebook cell to install pandas and pyarrow packages: %pip install pandas pyarrow Finally, given the endpoint value determined above, insert the following to a new notebook cell: import json import pyarrow.flight as fl # Create a Flight client client = fl.connect ( '<ENDPOINT>' ) # Prepare the request request = { \"asset\" : \"fybrik-airbyte-sample/userdata\" , } # Send request and fetch result as a pandas DataFrame info = client.get_flight_info ( fl.FlightDescriptor.for_command ( json.dumps ( request ))) reader: fl.FlightStreamReader = client.do_get ( info.endpoints [ 0 ] .ticket ) print ( reader.read_pandas ())","title":"FybrikModule chaining sample"},{"location":"samples/chaining-sample/#fybrikmodule-chaining-sample","text":"This sample shows how to implement a use case where, based on the data source and governance policies, the Fybrik manager determines that it must deploy two FybrikModules to allow a workload access to a dataset. One FybrikModule handles reading the data and the second does the data transformation. Data is passed between the FybrikModules without writing to intermediate storage. The data read in this example is the userdata dataset, a Parquet file found in https://github.com/Teradata/kylo/blob/master/samples/sample-data/parquet/userdata2.parquet. Two FybrikModules are available for use by the Fybrik control plane: the arrow-flight-module and the airbyte-module . Only the airbyte-module can give read access to the dataset. However, it does not have any data transformation capabilities. Therefore, to satisfy constraints, the Fybrik manager must deploy both modules: the airbyte module for reading the dataset, and the arrow-flight-module for transforming the dataset based on the governance policies. To recreate this scenario, you will need a copy of the Fybrik repository ( git clone https://github.com/fybrik/fybrik.git ), and a copy of the airbyte-module repository ( git clone https://github.com/fybrik/airbyte-module.git ). Set the following environment variables: FYBRIK_DIR for the path of the fybrik directory, and AIRBYTE_MODULE_DIR for the path of the airbyte-module directory. Install Fybrik Prerequisites. Follow the instruction in the Fybrik Quick Start Guide . Stop before the \"Install control plane\" section. Before installing the control plane, we need to customize the Fybrik taxonomy to define the new connection and interface types. Run: cd $FYBRIK_DIR go run main.go taxonomy compile --out custom-taxonomy.json --base charts/fybrik/files/taxonomy/taxonomy.json $AIRBYTE_MODULE_DIR /fybrik/fybrik-taxonomy-customize.yaml helm install fybrik-crd charts/fybrik-crd -n fybrik-system --wait helm install fybrik charts/fybrik --set global.tag = master --set global.imagePullPolicy = Always -n fybrik-system --wait --set-file taxonomyOverride = custom-taxonomy.json Install the Airbyte module: kubectl apply -f $AIRBYTE_MODULE_DIR /module.yaml -n fybrik-system Install the arrow-flight module for transformations: kubectl apply -f https://raw.githubusercontent.com/fybrik/arrow-flight-module/master/module.yaml -n fybrik-system Create a new namespace for the application, and set it as default: kubectl create namespace fybrik-airbyte-sample kubectl config set-context --current --namespace = fybrik-airbyte-sample Create an asset (the userdata asset) in fybrik's mini data catalog, the policy to access it (we use a policy that requires redactions to PII columns), and a FybrikApplication indicating the workload, context, and data requested: kubectl apply -f $AIRBYTE_MODULE_DIR /fybrik/asset.yaml kubectl -n fybrik-system create configmap sample-policy --from-file = $AIRBYTE_MODULE_DIR /fybrik/sample-policy-restrictive.rego kubectl -n fybrik-system label configmap sample-policy openpolicyagent.org/policy = rego while [[ $( kubectl get cm sample-policy -n fybrik-system -o 'jsonpath={.metadata.annotations.openpolicyagent\\.org/policy-status}' ) ! = '{\"status\":\"ok\"}' ]] ; do echo \"waiting for policy to be applied\" && sleep 5 ; done kubectl apply -f $AIRBYTE_MODULE_DIR /fybrik/application.yaml After the FybrikApplication is applied, the Fybrik control plane attempts to create the data path for the application. Fybrik realizes that the Airbyte module can give the application access to the userdata dataset, and that the arrow-flight module could provide the redaction transformation. Fybrik deploys both modules in the fybrik-blueprints namespace. To verify that the Airbyte module and the arrow-flight module were indeed deployed, run: kubectl get pods -n fybrik-blueprints You should see pods with names similar to: NAME READY STATUS RESTARTS AGE my-app-fybrik-airbyte-sample-airbyte-module-airbyte-module4kvrq 2 /2 Running 0 43s my-app-fybrik-airbyte-sample-arrow-flight-module-arrow-flibxsq2 1 /1 Running 0 43s To verify that the Airbyte module gives access to the userdata dataset, run: cd $AIRBYTE_MODULE_DIR /helm/client ./deploy_airbyte_module_client_pod.sh kubectl exec -it my-shell -n default -- python3 /root/client.py --host my-app-fybrik-airbyte-sample-arrow-flight-module.fybrik-blueprints --port 80 --asset fybrik-airbyte-sample/userdata You should see the following output: registration_dttm id first_name last_name email ... country birthdate salary title comments 0 2016 -02-03T13:36:39 1 .0 XXXXX XXXXX XXXXX ... Indonesia XXXXX 140249 .37 Senior Financial Analyst 1 2016 -02-03T00:22:28 2 .0 XXXXX XXXXX XXXXX ... China XXXXX NaN 2 2016 -02-03T18:29:04 3 .0 XXXXX XXXXX XXXXX ... France XXXXX 236219 .26 Teacher 3 2016 -02-03T13:42:19 4 .0 XXXXX XXXXX XXXXX ... Russia XXXXX NaN Nuclear Power Engineer 4 2016 -02-03T00:15:29 5 .0 XXXXX XXXXX XXXXX ... France XXXXX 50210 .02 Senior Editor .. ... ... ... ... ... ... ... ... ... ... ... 995 2016 -02-03T13:36:49 996 .0 XXXXX XXXXX XXXXX ... China XXXXX 185421 .82 \" 996 2016-02-03T04:39:01 997.0 XXXXX XXXXX XXXXX ... Malaysia XXXXX 279671.68 997 2016-02-03T00:33:54 998.0 XXXXX XXXXX XXXXX ... Poland XXXXX 112275.78 998 2016-02-03T00:15:08 999.0 XXXXX XXXXX XXXXX ... Kazakhstan XXXXX 53564.76 Speech Pathologist 999 2016-02-03T00:53:53 1000.0 XXXXX XXXXX XXXXX ... Nigeria XXXXX 239858.70 [1000 rows x 13 columns] Alternatively, one can access the userdata dataset from a Jupyter notebook, as described in the notebook sample . To determine the virtual endpoint from which to access the data set, run: ENDPOINT_SCHEME = $( kubectl get fybrikapplication my-app -o jsonpath ={ .status.assetStates.fybrik-notebook-sample/userdata.endpoint.fybrik-arrow-flight.scheme } ) ENDPOINT_HOSTNAME = $( kubectl get fybrikapplication my-app -o jsonpath ={ .status.assetStates.fybrik-notebook-sample/userdata.endpoint.fybrik-arrow-flight.hostname } ) ENDPOINT_PORT = $( kubectl get fybrikapplication my-app -o jsonpath ={ .status.assetStates.fybrik-notebook-sample/userdata.endpoint.fybrik-arrow-flight.port } ) printf \" ${ ENDPOINT_SCHEME } :// ${ ENDPOINT_HOSTNAME } : ${ ENDPOINT_PORT } \" Note that the virtual endpoint determined from the FybrikApplication status points to the arrow-flight transform module, although this transparent to the user. Insert a new notebook cell to install pandas and pyarrow packages: %pip install pandas pyarrow Finally, given the endpoint value determined above, insert the following to a new notebook cell: import json import pyarrow.flight as fl # Create a Flight client client = fl.connect ( '<ENDPOINT>' ) # Prepare the request request = { \"asset\" : \"fybrik-airbyte-sample/userdata\" , } # Send request and fetch result as a pandas DataFrame info = client.get_flight_info ( fl.FlightDescriptor.for_command ( json.dumps ( request ))) reader: fl.FlightStreamReader = client.do_get ( info.endpoints [ 0 ] .ticket ) print ( reader.read_pandas ())","title":"FybrikModule chaining sample"},{"location":"samples/cleanup/","text":"Cleanup When you're finished experimenting with a sample, you may clean up as follows: Stop kubectl port-forward processes (e.g., using pkill kubectl ) Delete the namespace created for this sample: kubectl delete namespace fybrik-notebook-sample Delete the policy created in the fybrik-system namespace: NS = \"fybrik-system\" ; kubectl -n $NS get configmap | awk '/sample/{print $1}' | xargs kubectl delete -n $NS configmap","title":"Cleanup"},{"location":"samples/cleanup/#cleanup","text":"When you're finished experimenting with a sample, you may clean up as follows: Stop kubectl port-forward processes (e.g., using pkill kubectl ) Delete the namespace created for this sample: kubectl delete namespace fybrik-notebook-sample Delete the policy created in the fybrik-system namespace: NS = \"fybrik-system\" ; kubectl -n $NS get configmap | awk '/sample/{print $1}' | xargs kubectl delete -n $NS configmap","title":"Cleanup"},{"location":"samples/dashboard-sample/","text":"Dashboard sample This sample shows how Fybrik enables a dashboard application to display data from a backend REST server and displaying only compliant information with the person operating the dashboard. In the sample we show how a Fybrik module that support REST protocol can control what information is displayed in the dashboard through policies. Specifically in this sample we demonstrate: - Just in time policy decisions - A Fybrik module that supports REST API Note: At this time some features in this demo use mocks as some underlying support in Fybrik is still pending. About the dashboard application The dashboard application here is based on the smart manufacturing use-case of the European Horizon 2020 research project Fogprotect . Details for the use case are available here . The dashboard included in this sample is a mock dashboard of a remote manufacturing factory which can be controlled/supervised by expert operators from remote, without the need for the expert operator to be physically present in the factory. The scenario here is simplified for the purpose of this sample and demonstrates the following scenario: 1. A manufacturing robot that operates the manufacturing facility which can be activated or stopped remotely. 2. A safety dashboard observing the manufacturing floor which is composed of two areas. A production and a non-production area, that are observed by a video camera with image processing which monitors and counts the number of employees with and without helmets in each area. This is called the safety data of the factory. We demonstrate Fybrik abilities to control what data is seen by each role. In this scenario we show the following roles: 1. A Foreman who is allowed to access all of the assets. Furthermore, only the Foreman is allowed to control the robot in the manufacturing area. 2. A Worker who is not allowed to control the robot, and doesn't have privileges to see the number of employees wearing/not wearing helmets in each of the available areas. However, a Worker can see the total number of employees in each area. 3. HR personnel who are also not allowed to control the robot, but have access to view the number of employees wearing/not wearing helmets in each area. Dashboard sample architecture The project contains 3 main components: - A backend data service, which provides the mock data for the dashboard. - A fybrik module , responsible for intercepting HTTP requests sent from a user trying to read or write data. - A dashboard application, which performs HTTP requests and displays the responses for the user. For a more detailed description of the implementation visit fogProtect-dashboard-sample . Before you begin Install Fybrik using the Quick Start guide. A web browser. Create a namespace for the sample Create a new Kubernetes namespace and set it as the active namespace: kubectl create namespace fogprotect kubectl config set-context --current --namespace = fogprotect This enables easy cleanup once you're done experimenting with the sample. Backend data server The backend data server is responsible for reading or writing data to a database or any other resource. In this example the backend data server is a simple server that returns the mock data for the dashboard. Run the backend data server: helm chart pull ghcr.io/fybrik/backend-server-chart:v0.0.1 helm chart export --destination = ./tmp ghcr.io/fybrik/backend-server-chart:v0.0.1 helm install rel1-backend-server ./tmp/backend_server Register the assets In this example we use 3 of the endpoints that the backend data server exposes. For each endpoint, we define an asset describing the data that will be returned as response from the backend data server. The description is used later to apply policies on the data. Register the assets: kubectl apply -f https://raw.githubusercontent.com/fybrik/fogProtect-dashboard-sample/main/assets/asset_get_safety_data.yaml kubectl apply -f https://raw.githubusercontent.com/fybrik/fogProtect-dashboard-sample/main/assets/asset_start_robot.yaml kubectl apply -f https://raw.githubusercontent.com/fybrik/fogProtect-dashboard-sample/main/assets/asset_stop_robot.yaml The identifier of the assets is <namespace>/<name> , and it should be used in the FybrikApplication that we will describe later on. For example the identifier of the asset defined in the file asset_start_robot.yaml is fogprotect/api.control.start-robot . Create JWT authentication key As the HTTP requests should contain the role of the user in the header, the dashboard application uses JWT to pass the JWT of the relevant role in the header. The JWT is authenticated using a secret key that we store as a secret in the cluster, in both fogprotect and fybrik-blueprints namespaces. Create the JWT secret: kubectl apply -f https://raw.githubusercontent.com/fybrik/fogProtect-dashboard-sample/main/secrets/jwt_key_secret.yaml kubectl apply -n fybrik-blueprints -f https://raw.githubusercontent.com/fybrik/fogProtect-dashboard-sample/main/secrets/jwt_key_secret.yaml Fybrik manager RBAC In order for the fybrik manager to be able to access the assets and to deploy the fybrik module that we created, give the manager the relevant RBAC authorization : kubectl apply -n fybrik-system -f https://raw.githubusercontent.com/fybrik/fogProtect-dashboard-sample/main/fybrik-system-manager-rbac.yaml Deploy the module In our case, the module as we described earlier is responsible for intercepting the HTTP requests received from the user. Once a request is received a decision must be made regarding the request, it should be either allowed, columns redacted or blocked depending on the role of the user. The decision is made using OpenPolicyAgent , and applying the policy described in About the dashboard application and specified here . Deploy the fybrik module and application: kubectl apply -n fybrik-system -f https://raw.githubusercontent.com/fybrik/fogProtect-dashboard-sample/main/rest-read-module.yaml kubectl apply -f https://raw.githubusercontent.com/fybrik/fogProtect-dashboard-sample/main/rest-read-application.yaml kubectl wait --for = condition = ready --all pod --timeout = 120s Deploy the dashboard We now deploy the dashboard that will display a table that contains the safety data of the factory, along with two buttons to start and stop the manufacturing robot. One can change the role of the user using a pull down menu. First create a port-forwarding to the service that will be intercepting the HTTP requests: kubectl -n fybrik-blueprints port-forward svc/rest-read 5559 :5559 & Afterwards, deploy the dashboard application: helm chart pull ghcr.io/fybrik/factory-gui-chart:v0.0.1 helm chart export --destination = ./tmp ghcr.io/fybrik/factory-gui-chart:v0.0.1 helm install rel1-factory-gui ./tmp/factory_gui kubectl wait --for = condition = ready --all pod --timeout = 120s Lastly, create a port-forwarding to the dashboard service in order to be able to open the dashboard in your browser: kubectl port-forward svc/factory-gui 3001 :3000 & Open your browser and go to http://127.0.0.1:3001. Cleanup Stop the port-forwarding: pgrep -f \"kubectl port-forward svc/factory-gui 3001:3000\" | xargs kill pgrep -f \"kubectl -n fybrik-blueprints port-forward svc/rest-read 5559:5559\" | xargs kill Remove the tmp directory that was created temporarily: rm -r tmp Delete the fybrik application and module: kubectl delete fybrikapplication rest-read kubectl -n fybrik-system delete fybrikmodule rest-read-module Delete the fogprotect namespace: kubectl delete namespace fogprotect Delete the RBAC authorization of the manager: kubectl -n fybrik-system delete -f https://raw.githubusercontent.com/fybrik/fogProtect-dashboard-sample/main/fybrik-system-manager-rbac.yaml Delete the JWT secret: kubectl delete -n fybrik-blueprints -f https://raw.githubusercontent.com/fybrik/fogProtect-dashboard-sample/main/secrets/jwt_key_secret.yaml","title":"Dashboard sample"},{"location":"samples/dashboard-sample/#dashboard-sample","text":"This sample shows how Fybrik enables a dashboard application to display data from a backend REST server and displaying only compliant information with the person operating the dashboard. In the sample we show how a Fybrik module that support REST protocol can control what information is displayed in the dashboard through policies. Specifically in this sample we demonstrate: - Just in time policy decisions - A Fybrik module that supports REST API Note: At this time some features in this demo use mocks as some underlying support in Fybrik is still pending.","title":"Dashboard sample"},{"location":"samples/dashboard-sample/#about-the-dashboard-application","text":"The dashboard application here is based on the smart manufacturing use-case of the European Horizon 2020 research project Fogprotect . Details for the use case are available here . The dashboard included in this sample is a mock dashboard of a remote manufacturing factory which can be controlled/supervised by expert operators from remote, without the need for the expert operator to be physically present in the factory. The scenario here is simplified for the purpose of this sample and demonstrates the following scenario: 1. A manufacturing robot that operates the manufacturing facility which can be activated or stopped remotely. 2. A safety dashboard observing the manufacturing floor which is composed of two areas. A production and a non-production area, that are observed by a video camera with image processing which monitors and counts the number of employees with and without helmets in each area. This is called the safety data of the factory. We demonstrate Fybrik abilities to control what data is seen by each role. In this scenario we show the following roles: 1. A Foreman who is allowed to access all of the assets. Furthermore, only the Foreman is allowed to control the robot in the manufacturing area. 2. A Worker who is not allowed to control the robot, and doesn't have privileges to see the number of employees wearing/not wearing helmets in each of the available areas. However, a Worker can see the total number of employees in each area. 3. HR personnel who are also not allowed to control the robot, but have access to view the number of employees wearing/not wearing helmets in each area.","title":"About the dashboard application"},{"location":"samples/dashboard-sample/#dashboard-sample-architecture","text":"The project contains 3 main components: - A backend data service, which provides the mock data for the dashboard. - A fybrik module , responsible for intercepting HTTP requests sent from a user trying to read or write data. - A dashboard application, which performs HTTP requests and displays the responses for the user. For a more detailed description of the implementation visit fogProtect-dashboard-sample .","title":"Dashboard sample architecture"},{"location":"samples/dashboard-sample/#before-you-begin","text":"Install Fybrik using the Quick Start guide. A web browser.","title":"Before you begin"},{"location":"samples/dashboard-sample/#create-a-namespace-for-the-sample","text":"Create a new Kubernetes namespace and set it as the active namespace: kubectl create namespace fogprotect kubectl config set-context --current --namespace = fogprotect This enables easy cleanup once you're done experimenting with the sample.","title":"Create a namespace for the sample"},{"location":"samples/dashboard-sample/#backend-data-server","text":"The backend data server is responsible for reading or writing data to a database or any other resource. In this example the backend data server is a simple server that returns the mock data for the dashboard. Run the backend data server: helm chart pull ghcr.io/fybrik/backend-server-chart:v0.0.1 helm chart export --destination = ./tmp ghcr.io/fybrik/backend-server-chart:v0.0.1 helm install rel1-backend-server ./tmp/backend_server","title":"Backend data server"},{"location":"samples/dashboard-sample/#register-the-assets","text":"In this example we use 3 of the endpoints that the backend data server exposes. For each endpoint, we define an asset describing the data that will be returned as response from the backend data server. The description is used later to apply policies on the data. Register the assets: kubectl apply -f https://raw.githubusercontent.com/fybrik/fogProtect-dashboard-sample/main/assets/asset_get_safety_data.yaml kubectl apply -f https://raw.githubusercontent.com/fybrik/fogProtect-dashboard-sample/main/assets/asset_start_robot.yaml kubectl apply -f https://raw.githubusercontent.com/fybrik/fogProtect-dashboard-sample/main/assets/asset_stop_robot.yaml The identifier of the assets is <namespace>/<name> , and it should be used in the FybrikApplication that we will describe later on. For example the identifier of the asset defined in the file asset_start_robot.yaml is fogprotect/api.control.start-robot .","title":"Register the assets"},{"location":"samples/dashboard-sample/#create-jwt-authentication-key","text":"As the HTTP requests should contain the role of the user in the header, the dashboard application uses JWT to pass the JWT of the relevant role in the header. The JWT is authenticated using a secret key that we store as a secret in the cluster, in both fogprotect and fybrik-blueprints namespaces. Create the JWT secret: kubectl apply -f https://raw.githubusercontent.com/fybrik/fogProtect-dashboard-sample/main/secrets/jwt_key_secret.yaml kubectl apply -n fybrik-blueprints -f https://raw.githubusercontent.com/fybrik/fogProtect-dashboard-sample/main/secrets/jwt_key_secret.yaml","title":"Create JWT authentication key"},{"location":"samples/dashboard-sample/#fybrik-manager-rbac","text":"In order for the fybrik manager to be able to access the assets and to deploy the fybrik module that we created, give the manager the relevant RBAC authorization : kubectl apply -n fybrik-system -f https://raw.githubusercontent.com/fybrik/fogProtect-dashboard-sample/main/fybrik-system-manager-rbac.yaml","title":"Fybrik manager RBAC"},{"location":"samples/dashboard-sample/#deploy-the-module","text":"In our case, the module as we described earlier is responsible for intercepting the HTTP requests received from the user. Once a request is received a decision must be made regarding the request, it should be either allowed, columns redacted or blocked depending on the role of the user. The decision is made using OpenPolicyAgent , and applying the policy described in About the dashboard application and specified here . Deploy the fybrik module and application: kubectl apply -n fybrik-system -f https://raw.githubusercontent.com/fybrik/fogProtect-dashboard-sample/main/rest-read-module.yaml kubectl apply -f https://raw.githubusercontent.com/fybrik/fogProtect-dashboard-sample/main/rest-read-application.yaml kubectl wait --for = condition = ready --all pod --timeout = 120s","title":"Deploy the module"},{"location":"samples/dashboard-sample/#deploy-the-dashboard","text":"We now deploy the dashboard that will display a table that contains the safety data of the factory, along with two buttons to start and stop the manufacturing robot. One can change the role of the user using a pull down menu. First create a port-forwarding to the service that will be intercepting the HTTP requests: kubectl -n fybrik-blueprints port-forward svc/rest-read 5559 :5559 & Afterwards, deploy the dashboard application: helm chart pull ghcr.io/fybrik/factory-gui-chart:v0.0.1 helm chart export --destination = ./tmp ghcr.io/fybrik/factory-gui-chart:v0.0.1 helm install rel1-factory-gui ./tmp/factory_gui kubectl wait --for = condition = ready --all pod --timeout = 120s Lastly, create a port-forwarding to the dashboard service in order to be able to open the dashboard in your browser: kubectl port-forward svc/factory-gui 3001 :3000 & Open your browser and go to http://127.0.0.1:3001.","title":"Deploy the dashboard"},{"location":"samples/dashboard-sample/#cleanup","text":"Stop the port-forwarding: pgrep -f \"kubectl port-forward svc/factory-gui 3001:3000\" | xargs kill pgrep -f \"kubectl -n fybrik-blueprints port-forward svc/rest-read 5559:5559\" | xargs kill Remove the tmp directory that was created temporarily: rm -r tmp Delete the fybrik application and module: kubectl delete fybrikapplication rest-read kubectl -n fybrik-system delete fybrikmodule rest-read-module Delete the fogprotect namespace: kubectl delete namespace fogprotect Delete the RBAC authorization of the manager: kubectl -n fybrik-system delete -f https://raw.githubusercontent.com/fybrik/fogProtect-dashboard-sample/main/fybrik-system-manager-rbac.yaml Delete the JWT secret: kubectl delete -n fybrik-blueprints -f https://raw.githubusercontent.com/fybrik/fogProtect-dashboard-sample/main/secrets/jwt_key_secret.yaml","title":"Cleanup"},{"location":"samples/delete-sample/","text":"Sample for the delete flow This sample demonstrate how to delete an S3 object from a bucket. Install module To apply the latest development version of arrow-flight-module: kubectl apply -f -n fybrik-system https://raw.githubusercontent.com/fybrik/delete-module/main/module.yaml Prepare dataset This sample uses the Synthetic Financial Datasets For Fraud Detection dataset 1 as the data that the notebook needs to read. Download and extract the file to your machine. You should now see a file named PS_20174392719_1491204439457_log.csv . Alternatively, use a sample of 100 lines of the same dataset by downloading PS_20174392719_1491204439457_log.csv from GitHub. Upload the CSV file to an object storage of your choice such as AWS S3, IBM Cloud Object Storage or Ceph. Make a note of the service endpoint, bucket name, and access credentials. You will need them later. Setup and upload to localstack For experimentation you can install localstack to your cluster instead of using a cloud service. Define variables for access key and secret key export ACCESS_KEY = \"myaccesskey\" export SECRET_KEY = \"mysecretkey\" Install localstack to the currently active namespace and wait for it to be ready: helm repo add localstack-charts https://localstack.github.io/helm-charts helm install localstack localstack-charts/localstack --set startServices = \"s3\" --set service.type = ClusterIP kubectl wait --for = condition = ready --all pod -n fybrik-notebook-sample --timeout = 120s Create a port-forward to communicate with localstack server: kubectl port-forward svc/localstack 4566 :4566 & Use AWS CLI to upload the dataset to a new created bucket in the localstack server: export ENDPOINT = \"http://127.0.0.1:4566\" export BUCKET = \"demo\" export OBJECT_KEY = \"PS_20174392719_1491204439457_log.csv\" export FILEPATH = \"/path/to/PS_20174392719_1491204439457_log.csv\" aws configure set aws_access_key_id ${ ACCESS_KEY } && aws configure set aws_secret_access_key ${ SECRET_KEY } && aws --endpoint-url = ${ ENDPOINT } s3api create-bucket --bucket ${ BUCKET } && aws --endpoint-url = ${ ENDPOINT } s3api put-object --bucket ${ BUCKET } --key ${ OBJECT_KEY } --body ${ FILEPATH } Before we delete the object, we make sure it's been created. You can check with the object storage serive that you used or with AWS CLI : aws --endpoint-url=${ENDPOINT} s3api list-objects --bucket=${BUCKET} You should see the new created object: { \"Contents\": [ { \"Key\": \"PS_20174392719_1491204439457_log.csv\", \"LastModified\": \"2022-06-06T07:12:16.000Z\", \"ETag\": \"\\\"9a34903326938d8c33c29f4a1170a7b1\\\"\", \"Size\": 6551, \"StorageClass\": \"STANDARD\", \"Owner\": { \"DisplayName\": \"webfile\", \"ID\": \"75aa57f09aa0c8caeab4f8c24e99d10f8e7faeebf76c078efc7c6caea54ba06a\" } } ] } Register the dataset in a data catalog In this step you are performing the role of the data owner, registering his data in the data catalog and registering the credentials for accessing the data in the credential manager. Register the credentials required for accessing the dataset as a kubernetes secret. Replace the values for access_key and secret_key with the values from the object storage service that you used and run: cat << EOF | kubectl apply -f - apiVersion : v1 kind : Secret metadata : name : paysim-csv type : Opaque stringData : access_key : \"${ACCESS_KEY}\" secret_key : \"${SECRET_KEY}\" EOF Then, register the data asset itself in the data catalog katalog used for samples. Replace the values for endpoint , bucket and object_key with values from the object storage service that you used and run: cat << EOF | kubectl apply -f - apiVersion : katalog.fybrik.io/v1alpha1 kind : Asset metadata : name : paysim-csv spec : secretRef : name : paysim-csv details : dataFormat : csv connection : name : s3 s3 : endpoint : \"http://localstack.fybrik-notebook-sample.svc.cluster.local:4566\" bucket : \"demo\" object_key : \"PS_20174392719_1491204439457_log.csv\" metadata : name : Synthetic Financial Datasets For Fraud Detection geography : theshire tags : finance : true EOF The asset is now registered in the catalog. The identifier of the asset is fybrik-notebook-sample/paysim-csv (i.e. <namespace>/<name> ). You will use that name in the FybrikApplication later. Notice the metadata field above. It specifies the dataset geography and tags. These attributes can later be used in policies. For example, in the yaml above, the geography is set to theshire , you need make sure it is same with the region of your fybrik control plane, you can get the information with the below command: kubectl get configmap cluster-metadata -n fybrik-system -o 'jsonpath={.data.Region}' Quick Start installs a fybrik control plane with the region theshire by default. If you change it or the geography in the yaml above, a copy module will be required by the policies, but we do not install any copy module in the Quick Start . Define data access policy Acting as the data steward, define an OpenPolicyAgent policy. In this sample we only specify the action taken. Below is the policy (written in Rego language): package dataapi.authz rule[{}] { description := \"allow the delete operation\" input.action.actionType == \"delete\" } In this sample only the policy above is applied. Copy the policy to a file named sample-policy.rego and then run: kubectl -n fybrik-system create configmap sample-policy --from-file = sample-policy.rego kubectl -n fybrik-system label configmap sample-policy openpolicyagent.org/policy = rego while [[ $( kubectl get cm sample-policy -n fybrik-system -o 'jsonpath={.metadata.annotations.openpolicyagent\\.org/policy-status}' ) ! = '{\"status\":\"ok\"}' ]] ; do echo \"waiting for policy to be applied\" && sleep 5 ; done You can similarly apply a directory holding multiple rego files. Create a FybrikApplication resource Create a FybrikApplication resource to register the notebook workload to the control plane of Fybrik: cat <<EOF | kubectl apply -f - apiVersion : app.fybrik.io/v1alpha1 kind : FybrikApplication metadata : name : delete-app namespace : fybrik-notebook-sample spec : selector : workloadSelector : matchLabels : {} appInfo : intent : Fraud Detection role : Security data : - dataSetID : 'fybrik-notebook-sample/paysim-csv' flow : delete requirements : {} EOF Notice that the data field includes a dataSetID that matches the asset identifier in the catalog. Run the following command to wait until the FybrikApplication is ready: while [[ $( kubectl get fybrikapplication delete-app -o 'jsonpath={.status.ready}' ) ! = \"true\" ]] ; do echo \"waiting for FybrikApplication\" && sleep 5 ; done while [[ $( kubectl get fybrikapplication delete-app -o 'jsonpath={.status.assetStates.fybrik-notebook-sample/paysim-csv.conditions[?(@.type == \"Ready\")].status}' ) ! = \"True\" ]] ; do echo \"waiting for fybrik-notebook-sample/paysim-csv asset\" && sleep 5 ; done Ensure the object is deleted Now the object should be deleted. We can check again with AWS CLI : aws --endpoint-url=${ENDPOINT} s3api list-objects --bucket=${BUCKET} Now you should see that the object is no longer in the list (or no list at all if the bukcet is empty). Created by NTNU and shared under the CC BY-SA 4.0 license. \u21a9","title":"Sample for the delete flow"},{"location":"samples/delete-sample/#sample-for-the-delete-flow","text":"This sample demonstrate how to delete an S3 object from a bucket.","title":"Sample for the delete flow"},{"location":"samples/delete-sample/#install-module","text":"To apply the latest development version of arrow-flight-module: kubectl apply -f -n fybrik-system https://raw.githubusercontent.com/fybrik/delete-module/main/module.yaml","title":"Install module"},{"location":"samples/delete-sample/#prepare-dataset","text":"This sample uses the Synthetic Financial Datasets For Fraud Detection dataset 1 as the data that the notebook needs to read. Download and extract the file to your machine. You should now see a file named PS_20174392719_1491204439457_log.csv . Alternatively, use a sample of 100 lines of the same dataset by downloading PS_20174392719_1491204439457_log.csv from GitHub. Upload the CSV file to an object storage of your choice such as AWS S3, IBM Cloud Object Storage or Ceph. Make a note of the service endpoint, bucket name, and access credentials. You will need them later. Setup and upload to localstack For experimentation you can install localstack to your cluster instead of using a cloud service. Define variables for access key and secret key export ACCESS_KEY = \"myaccesskey\" export SECRET_KEY = \"mysecretkey\" Install localstack to the currently active namespace and wait for it to be ready: helm repo add localstack-charts https://localstack.github.io/helm-charts helm install localstack localstack-charts/localstack --set startServices = \"s3\" --set service.type = ClusterIP kubectl wait --for = condition = ready --all pod -n fybrik-notebook-sample --timeout = 120s Create a port-forward to communicate with localstack server: kubectl port-forward svc/localstack 4566 :4566 & Use AWS CLI to upload the dataset to a new created bucket in the localstack server: export ENDPOINT = \"http://127.0.0.1:4566\" export BUCKET = \"demo\" export OBJECT_KEY = \"PS_20174392719_1491204439457_log.csv\" export FILEPATH = \"/path/to/PS_20174392719_1491204439457_log.csv\" aws configure set aws_access_key_id ${ ACCESS_KEY } && aws configure set aws_secret_access_key ${ SECRET_KEY } && aws --endpoint-url = ${ ENDPOINT } s3api create-bucket --bucket ${ BUCKET } && aws --endpoint-url = ${ ENDPOINT } s3api put-object --bucket ${ BUCKET } --key ${ OBJECT_KEY } --body ${ FILEPATH } Before we delete the object, we make sure it's been created. You can check with the object storage serive that you used or with AWS CLI : aws --endpoint-url=${ENDPOINT} s3api list-objects --bucket=${BUCKET} You should see the new created object: { \"Contents\": [ { \"Key\": \"PS_20174392719_1491204439457_log.csv\", \"LastModified\": \"2022-06-06T07:12:16.000Z\", \"ETag\": \"\\\"9a34903326938d8c33c29f4a1170a7b1\\\"\", \"Size\": 6551, \"StorageClass\": \"STANDARD\", \"Owner\": { \"DisplayName\": \"webfile\", \"ID\": \"75aa57f09aa0c8caeab4f8c24e99d10f8e7faeebf76c078efc7c6caea54ba06a\" } } ] }","title":"Prepare dataset"},{"location":"samples/delete-sample/#register-the-dataset-in-a-data-catalog","text":"In this step you are performing the role of the data owner, registering his data in the data catalog and registering the credentials for accessing the data in the credential manager. Register the credentials required for accessing the dataset as a kubernetes secret. Replace the values for access_key and secret_key with the values from the object storage service that you used and run: cat << EOF | kubectl apply -f - apiVersion : v1 kind : Secret metadata : name : paysim-csv type : Opaque stringData : access_key : \"${ACCESS_KEY}\" secret_key : \"${SECRET_KEY}\" EOF Then, register the data asset itself in the data catalog katalog used for samples. Replace the values for endpoint , bucket and object_key with values from the object storage service that you used and run: cat << EOF | kubectl apply -f - apiVersion : katalog.fybrik.io/v1alpha1 kind : Asset metadata : name : paysim-csv spec : secretRef : name : paysim-csv details : dataFormat : csv connection : name : s3 s3 : endpoint : \"http://localstack.fybrik-notebook-sample.svc.cluster.local:4566\" bucket : \"demo\" object_key : \"PS_20174392719_1491204439457_log.csv\" metadata : name : Synthetic Financial Datasets For Fraud Detection geography : theshire tags : finance : true EOF The asset is now registered in the catalog. The identifier of the asset is fybrik-notebook-sample/paysim-csv (i.e. <namespace>/<name> ). You will use that name in the FybrikApplication later. Notice the metadata field above. It specifies the dataset geography and tags. These attributes can later be used in policies. For example, in the yaml above, the geography is set to theshire , you need make sure it is same with the region of your fybrik control plane, you can get the information with the below command: kubectl get configmap cluster-metadata -n fybrik-system -o 'jsonpath={.data.Region}' Quick Start installs a fybrik control plane with the region theshire by default. If you change it or the geography in the yaml above, a copy module will be required by the policies, but we do not install any copy module in the Quick Start .","title":"Register the dataset in a data catalog"},{"location":"samples/delete-sample/#define-data-access-policy","text":"Acting as the data steward, define an OpenPolicyAgent policy. In this sample we only specify the action taken. Below is the policy (written in Rego language): package dataapi.authz rule[{}] { description := \"allow the delete operation\" input.action.actionType == \"delete\" } In this sample only the policy above is applied. Copy the policy to a file named sample-policy.rego and then run: kubectl -n fybrik-system create configmap sample-policy --from-file = sample-policy.rego kubectl -n fybrik-system label configmap sample-policy openpolicyagent.org/policy = rego while [[ $( kubectl get cm sample-policy -n fybrik-system -o 'jsonpath={.metadata.annotations.openpolicyagent\\.org/policy-status}' ) ! = '{\"status\":\"ok\"}' ]] ; do echo \"waiting for policy to be applied\" && sleep 5 ; done You can similarly apply a directory holding multiple rego files.","title":"Define data access policy"},{"location":"samples/delete-sample/#create-a-fybrikapplication-resource","text":"Create a FybrikApplication resource to register the notebook workload to the control plane of Fybrik: cat <<EOF | kubectl apply -f - apiVersion : app.fybrik.io/v1alpha1 kind : FybrikApplication metadata : name : delete-app namespace : fybrik-notebook-sample spec : selector : workloadSelector : matchLabels : {} appInfo : intent : Fraud Detection role : Security data : - dataSetID : 'fybrik-notebook-sample/paysim-csv' flow : delete requirements : {} EOF Notice that the data field includes a dataSetID that matches the asset identifier in the catalog. Run the following command to wait until the FybrikApplication is ready: while [[ $( kubectl get fybrikapplication delete-app -o 'jsonpath={.status.ready}' ) ! = \"true\" ]] ; do echo \"waiting for FybrikApplication\" && sleep 5 ; done while [[ $( kubectl get fybrikapplication delete-app -o 'jsonpath={.status.assetStates.fybrik-notebook-sample/paysim-csv.conditions[?(@.type == \"Ready\")].status}' ) ! = \"True\" ]] ; do echo \"waiting for fybrik-notebook-sample/paysim-csv asset\" && sleep 5 ; done","title":"Create a FybrikApplication resource"},{"location":"samples/delete-sample/#ensure-the-object-is-deleted","text":"Now the object should be deleted. We can check again with AWS CLI : aws --endpoint-url=${ENDPOINT} s3api list-objects --bucket=${BUCKET} Now you should see that the object is no longer in the list (or no list at all if the bukcet is empty). Created by NTNU and shared under the CC BY-SA 4.0 license. \u21a9","title":"Ensure the object is deleted"},{"location":"samples/notebook-read/","text":"Notebook sample for the read flow This sample demonstrates the following: how Fybrik enables a Jupyter notebook workload to access a cataloged dataset. how arrow-flight module is used for reading and transforming data. how policies regarding the use of personal information are seamlessly applied when accessing a dataset containing financial data. In this sample you play multiple roles: As a data owner you upload a dataset and register it in a data catalog As a data steward you setup data governance policies As a data user you specify your data usage requirements and use a notebook to consume the data Prepare a dataset to be accessed by the notebook This sample uses the Synthetic Financial Datasets For Fraud Detection dataset 1 as the data that the notebook needs to read. Download and extract the file to your machine. You should now see a file named PS_20174392719_1491204439457_log.csv . Alternatively, use a sample of 100 lines of the same dataset by downloading PS_20174392719_1491204439457_log.csv from GitHub. Upload the CSV file to an object storage of your choice such as AWS S3, IBM Cloud Object Storage or Ceph. Make a note of the service endpoint, bucket name, and access credentials. You will need them later. Setup and upload to localstack For experimentation you can install localstack to your cluster instead of using a cloud service. Define variables for access key and secret key export ACCESS_KEY = \"myaccesskey\" export SECRET_KEY = \"mysecretkey\" Install localstack to the currently active namespace and wait for it to be ready: helm repo add localstack-charts https://localstack.github.io/helm-charts helm install localstack localstack-charts/localstack --set startServices = \"s3\" --set service.type = ClusterIP kubectl wait --for = condition = ready --all pod -n fybrik-notebook-sample --timeout = 120s Create a port-forward to communicate with localstack server: kubectl port-forward svc/localstack 4566 :4566 & Use AWS CLI to upload the dataset to a new created bucket in the localstack server: export ENDPOINT = \"http://127.0.0.1:4566\" export BUCKET = \"demo\" export OBJECT_KEY = \"PS_20174392719_1491204439457_log.csv\" export FILEPATH = \"/path/to/PS_20174392719_1491204439457_log.csv\" aws configure set aws_access_key_id ${ ACCESS_KEY } && aws configure set aws_secret_access_key ${ SECRET_KEY } && aws --endpoint-url = ${ ENDPOINT } s3api create-bucket --bucket ${ BUCKET } && aws --endpoint-url = ${ ENDPOINT } s3api put-object --bucket ${ BUCKET } --key ${ OBJECT_KEY } --body ${ FILEPATH } Register the dataset in a data catalog In this step you are performing the role of the data owner, registering his data in the data catalog and registering the credentials for accessing the data in the credential manager. Register the credentials required for accessing the dataset as a kubernetes secret. Replace the values for access_key and secret_key with the values from the object storage service that you used and run: cat << EOF | kubectl apply -f - apiVersion : v1 kind : Secret metadata : name : paysim-csv type : Opaque stringData : access_key : \"${ACCESS_KEY}\" secret_key : \"${SECRET_KEY}\" EOF Then, register the data asset itself in the data catalog katalog used for samples. Replace the values for endpoint , bucket and object_key with values from the object storage service that you used and run: cat << EOF | kubectl apply -f - apiVersion : katalog.fybrik.io/v1alpha1 kind : Asset metadata : name : paysim-csv spec : secretRef : name : paysim-csv details : dataFormat : csv connection : name : s3 s3 : endpoint : \"http://localstack.fybrik-notebook-sample.svc.cluster.local:4566\" bucket : \"demo\" object_key : \"PS_20174392719_1491204439457_log.csv\" metadata : name : Synthetic Financial Datasets For Fraud Detection geography : theshire tags : finance : true columns : - name : nameOrig tags : PII : true - name : oldbalanceOrg tags : PII : true - name : newbalanceOrig tags : PII : true EOF The asset is now registered in the catalog. The identifier of the asset is fybrik-notebook-sample/paysim-csv (i.e. <namespace>/<name> ). You will use that name in the FybrikApplication later. Notice the metadata field above. It specifies the dataset geography and tags. These attributes can later be used in policies. For example, in the yaml above, the geography is set to theshire , you need make sure it is same with the region of your fybrik control plane, you can get the information with the below command: kubectl get configmap cluster-metadata -n fybrik-system -o 'jsonpath={.data.Region}' Quick Start installs a fybrik control plane with the region theshire by default. If you change it or the geography in the yaml above, a copy module will be required by the policies, but we do not install any copy module in the Quick Start . Define data access policies Acting as the data steward, define an OpenPolicyAgent policy to redact the columns tagged as PII for datasets tagged with finance . Below is the policy (written in Rego language): package dataapi.authz rule[{\"action\": {\"name\":\"RedactAction\", \"columns\": column_names}, \"policy\": description}] { description := \"Redact columns tagged as PII in datasets tagged with finance = true\" input.action.actionType == \"read\" input.resource.metadata.tags.finance column_names := [input.resource.metadata.columns[i].name | input.resource.metadata.columns[i].tags.PII] count(column_names) > 0 } In this sample only the policy above is applied. Copy the policy to a file named sample-policy.rego and then run: kubectl -n fybrik-system create configmap sample-policy --from-file = sample-policy.rego kubectl -n fybrik-system label configmap sample-policy openpolicyagent.org/policy = rego while [[ $( kubectl get cm sample-policy -n fybrik-system -o 'jsonpath={.metadata.annotations.openpolicyagent\\.org/policy-status}' ) ! = '{\"status\":\"ok\"}' ]] ; do echo \"waiting for policy to be applied\" && sleep 5 ; done You can similarly apply a directory holding multiple rego files. Deploy a Jupyter notebook In this sample a Jupyter notebook is used as the user workload and its business logic requires reading the asset that we registered (e.g., for creating a fraud detection model). Deploy a notebook to your cluster: JupyterLab Kubeflow Deploy JupyterLab: kubectl create deployment my-notebook --image = jupyter/base-notebook --port = 8888 -- start.sh jupyter lab --LabApp.token = '' kubectl set env deployment my-notebook JUPYTER_ENABLE_LAB = yes kubectl label deployment my-notebook app.kubernetes.io/name = my-notebook kubectl wait --for = condition = available --timeout = 120s deployment/my-notebook kubectl expose deployment my-notebook --port = 80 --target-port = 8888 Create a port-forward to communicate with JupyterLab: kubectl port-forward svc/my-notebook 8080 :80 & Open your browser and go to http://localhost:8080/ . Create a new notebook in the server Ensure that Kubeflow is installed in your cluster Create a port-forward to communicate with Kubeflow: kubectl port-forward svc/istio-ingressgateway -n istio-system 8080 :80 & Open your browser and go to http://localhost:8080/ . Click Start Setup and then Finish (use the anonymous namespace). Click Notebook Servers (in the left). In the notebooks page select in the top left the anonymous namespace and then click New Server . In the notebook server creation page, set my-notebook in the Name box and then click Launch . Wait for the server to become ready. Click Connect and create a new notebook in the server. Create a FybrikApplication resource for the notebook Create a FybrikApplication resource to register the notebook workload to the control plane of Fybrik: cat <<EOF | kubectl apply -f - apiVersion : app.fybrik.io/v1alpha1 kind : FybrikApplication metadata : name : my-notebook labels : app : my-notebook spec : selector : workloadSelector : matchLabels : app : my-notebook appInfo : intent : Fraud Detection data : - dataSetID : \"fybrik-notebook-sample/paysim-csv\" requirements : interface : protocol : fybrik-arrow-flight EOF Notice that: The selector field matches the labels of our Jupyter notebook workload. The data field includes a dataSetID that matches the asset identifier in the catalog. The protocol indicates that the developer wants to consume the data using Apache Arrow Flight. For some protocols a dataformat can be specified as well (e.g., s3 protocol and parquet format). Run the following command to wait until the FybrikApplication is ready: while [[ $( kubectl get fybrikapplication my-notebook -o 'jsonpath={.status.ready}' ) ! = \"true\" ]] ; do echo \"waiting for FybrikApplication\" && sleep 5 ; done while [[ $( kubectl get fybrikapplication my-notebook -o 'jsonpath={.status.assetStates.fybrik-notebook-sample/paysim-csv.conditions[?(@.type == \"Ready\")].status}' ) ! = \"True\" ]] ; do echo \"waiting for fybrik-notebook-sample/paysim-csv asset\" && sleep 5 ; done Read the dataset from the notebook In your terminal , run the following command to print the endpoint to use for reading the data. It fetches the code from the FybrikApplication resource: ENDPOINT_SCHEME = $( kubectl get fybrikapplication my-notebook -o jsonpath ={ .status.assetStates.fybrik-notebook-sample/paysim-csv.endpoint.fybrik-arrow-flight.scheme } ) ENDPOINT_HOSTNAME = $( kubectl get fybrikapplication my-notebook -o jsonpath ={ .status.assetStates.fybrik-notebook-sample/paysim-csv.endpoint.fybrik-arrow-flight.hostname } ) ENDPOINT_PORT = $( kubectl get fybrikapplication my-notebook -o jsonpath ={ .status.assetStates.fybrik-notebook-sample/paysim-csv.endpoint.fybrik-arrow-flight.port } ) printf \"\\n ${ ENDPOINT_SCHEME } :// ${ ENDPOINT_HOSTNAME } : ${ ENDPOINT_PORT } \\n\\n\" The next steps use the endpoint to read the data in a python notebook Insert a new notebook cell to install pandas and pyarrow packages: % pip install pandas pyarrow == 7.0 .* Insert a new notebook cell to read the data using the endpoint value extracted from the FybrikApplication in the previous step: import json import pyarrow.flight as fl import pandas as pd # Create a Flight client client = fl.connect ( '<ENDPOINT>' ) # Prepare the request request = { \"asset\" : \"fybrik-notebook-sample/paysim-csv\" , # To request specific columns add to the request a \"columns\" key with a list of column names # \"columns\": [...] } # Send request and fetch result as a pandas DataFrame info = client.get_flight_info ( fl.FlightDescriptor.for_command ( json.dumps ( request ))) reader: fl.FlightStreamReader = client.do_get ( info.endpoints [ 0 ] .ticket ) df: pd.DataFrame = reader.read_pandas () Insert a new notebook cell with the following command to visualize the result: df Execute all notebook cells and notice that the nameOrig , oldbalanceOrg and newbalanceOrig columns appear redacted. Created by NTNU and shared under the CC BY-SA 4.0 license. \u21a9","title":"Notebook sample for the read flow"},{"location":"samples/notebook-read/#notebook-sample-for-the-read-flow","text":"This sample demonstrates the following: how Fybrik enables a Jupyter notebook workload to access a cataloged dataset. how arrow-flight module is used for reading and transforming data. how policies regarding the use of personal information are seamlessly applied when accessing a dataset containing financial data. In this sample you play multiple roles: As a data owner you upload a dataset and register it in a data catalog As a data steward you setup data governance policies As a data user you specify your data usage requirements and use a notebook to consume the data","title":"Notebook sample for the read flow"},{"location":"samples/notebook-read/#prepare-a-dataset-to-be-accessed-by-the-notebook","text":"This sample uses the Synthetic Financial Datasets For Fraud Detection dataset 1 as the data that the notebook needs to read. Download and extract the file to your machine. You should now see a file named PS_20174392719_1491204439457_log.csv . Alternatively, use a sample of 100 lines of the same dataset by downloading PS_20174392719_1491204439457_log.csv from GitHub. Upload the CSV file to an object storage of your choice such as AWS S3, IBM Cloud Object Storage or Ceph. Make a note of the service endpoint, bucket name, and access credentials. You will need them later. Setup and upload to localstack For experimentation you can install localstack to your cluster instead of using a cloud service. Define variables for access key and secret key export ACCESS_KEY = \"myaccesskey\" export SECRET_KEY = \"mysecretkey\" Install localstack to the currently active namespace and wait for it to be ready: helm repo add localstack-charts https://localstack.github.io/helm-charts helm install localstack localstack-charts/localstack --set startServices = \"s3\" --set service.type = ClusterIP kubectl wait --for = condition = ready --all pod -n fybrik-notebook-sample --timeout = 120s Create a port-forward to communicate with localstack server: kubectl port-forward svc/localstack 4566 :4566 & Use AWS CLI to upload the dataset to a new created bucket in the localstack server: export ENDPOINT = \"http://127.0.0.1:4566\" export BUCKET = \"demo\" export OBJECT_KEY = \"PS_20174392719_1491204439457_log.csv\" export FILEPATH = \"/path/to/PS_20174392719_1491204439457_log.csv\" aws configure set aws_access_key_id ${ ACCESS_KEY } && aws configure set aws_secret_access_key ${ SECRET_KEY } && aws --endpoint-url = ${ ENDPOINT } s3api create-bucket --bucket ${ BUCKET } && aws --endpoint-url = ${ ENDPOINT } s3api put-object --bucket ${ BUCKET } --key ${ OBJECT_KEY } --body ${ FILEPATH }","title":"Prepare a dataset to be accessed by the notebook"},{"location":"samples/notebook-read/#register-the-dataset-in-a-data-catalog","text":"In this step you are performing the role of the data owner, registering his data in the data catalog and registering the credentials for accessing the data in the credential manager. Register the credentials required for accessing the dataset as a kubernetes secret. Replace the values for access_key and secret_key with the values from the object storage service that you used and run: cat << EOF | kubectl apply -f - apiVersion : v1 kind : Secret metadata : name : paysim-csv type : Opaque stringData : access_key : \"${ACCESS_KEY}\" secret_key : \"${SECRET_KEY}\" EOF Then, register the data asset itself in the data catalog katalog used for samples. Replace the values for endpoint , bucket and object_key with values from the object storage service that you used and run: cat << EOF | kubectl apply -f - apiVersion : katalog.fybrik.io/v1alpha1 kind : Asset metadata : name : paysim-csv spec : secretRef : name : paysim-csv details : dataFormat : csv connection : name : s3 s3 : endpoint : \"http://localstack.fybrik-notebook-sample.svc.cluster.local:4566\" bucket : \"demo\" object_key : \"PS_20174392719_1491204439457_log.csv\" metadata : name : Synthetic Financial Datasets For Fraud Detection geography : theshire tags : finance : true columns : - name : nameOrig tags : PII : true - name : oldbalanceOrg tags : PII : true - name : newbalanceOrig tags : PII : true EOF The asset is now registered in the catalog. The identifier of the asset is fybrik-notebook-sample/paysim-csv (i.e. <namespace>/<name> ). You will use that name in the FybrikApplication later. Notice the metadata field above. It specifies the dataset geography and tags. These attributes can later be used in policies. For example, in the yaml above, the geography is set to theshire , you need make sure it is same with the region of your fybrik control plane, you can get the information with the below command: kubectl get configmap cluster-metadata -n fybrik-system -o 'jsonpath={.data.Region}' Quick Start installs a fybrik control plane with the region theshire by default. If you change it or the geography in the yaml above, a copy module will be required by the policies, but we do not install any copy module in the Quick Start .","title":"Register the dataset in a data catalog"},{"location":"samples/notebook-read/#define-data-access-policies","text":"Acting as the data steward, define an OpenPolicyAgent policy to redact the columns tagged as PII for datasets tagged with finance . Below is the policy (written in Rego language): package dataapi.authz rule[{\"action\": {\"name\":\"RedactAction\", \"columns\": column_names}, \"policy\": description}] { description := \"Redact columns tagged as PII in datasets tagged with finance = true\" input.action.actionType == \"read\" input.resource.metadata.tags.finance column_names := [input.resource.metadata.columns[i].name | input.resource.metadata.columns[i].tags.PII] count(column_names) > 0 } In this sample only the policy above is applied. Copy the policy to a file named sample-policy.rego and then run: kubectl -n fybrik-system create configmap sample-policy --from-file = sample-policy.rego kubectl -n fybrik-system label configmap sample-policy openpolicyagent.org/policy = rego while [[ $( kubectl get cm sample-policy -n fybrik-system -o 'jsonpath={.metadata.annotations.openpolicyagent\\.org/policy-status}' ) ! = '{\"status\":\"ok\"}' ]] ; do echo \"waiting for policy to be applied\" && sleep 5 ; done You can similarly apply a directory holding multiple rego files.","title":"Define data access policies"},{"location":"samples/notebook-read/#deploy-a-jupyter-notebook","text":"In this sample a Jupyter notebook is used as the user workload and its business logic requires reading the asset that we registered (e.g., for creating a fraud detection model). Deploy a notebook to your cluster: JupyterLab Kubeflow Deploy JupyterLab: kubectl create deployment my-notebook --image = jupyter/base-notebook --port = 8888 -- start.sh jupyter lab --LabApp.token = '' kubectl set env deployment my-notebook JUPYTER_ENABLE_LAB = yes kubectl label deployment my-notebook app.kubernetes.io/name = my-notebook kubectl wait --for = condition = available --timeout = 120s deployment/my-notebook kubectl expose deployment my-notebook --port = 80 --target-port = 8888 Create a port-forward to communicate with JupyterLab: kubectl port-forward svc/my-notebook 8080 :80 & Open your browser and go to http://localhost:8080/ . Create a new notebook in the server Ensure that Kubeflow is installed in your cluster Create a port-forward to communicate with Kubeflow: kubectl port-forward svc/istio-ingressgateway -n istio-system 8080 :80 & Open your browser and go to http://localhost:8080/ . Click Start Setup and then Finish (use the anonymous namespace). Click Notebook Servers (in the left). In the notebooks page select in the top left the anonymous namespace and then click New Server . In the notebook server creation page, set my-notebook in the Name box and then click Launch . Wait for the server to become ready. Click Connect and create a new notebook in the server.","title":"Deploy a Jupyter notebook"},{"location":"samples/notebook-read/#create-a-fybrikapplication-resource-for-the-notebook","text":"Create a FybrikApplication resource to register the notebook workload to the control plane of Fybrik: cat <<EOF | kubectl apply -f - apiVersion : app.fybrik.io/v1alpha1 kind : FybrikApplication metadata : name : my-notebook labels : app : my-notebook spec : selector : workloadSelector : matchLabels : app : my-notebook appInfo : intent : Fraud Detection data : - dataSetID : \"fybrik-notebook-sample/paysim-csv\" requirements : interface : protocol : fybrik-arrow-flight EOF Notice that: The selector field matches the labels of our Jupyter notebook workload. The data field includes a dataSetID that matches the asset identifier in the catalog. The protocol indicates that the developer wants to consume the data using Apache Arrow Flight. For some protocols a dataformat can be specified as well (e.g., s3 protocol and parquet format). Run the following command to wait until the FybrikApplication is ready: while [[ $( kubectl get fybrikapplication my-notebook -o 'jsonpath={.status.ready}' ) ! = \"true\" ]] ; do echo \"waiting for FybrikApplication\" && sleep 5 ; done while [[ $( kubectl get fybrikapplication my-notebook -o 'jsonpath={.status.assetStates.fybrik-notebook-sample/paysim-csv.conditions[?(@.type == \"Ready\")].status}' ) ! = \"True\" ]] ; do echo \"waiting for fybrik-notebook-sample/paysim-csv asset\" && sleep 5 ; done","title":"Create a FybrikApplication resource for the notebook"},{"location":"samples/notebook-read/#read-the-dataset-from-the-notebook","text":"In your terminal , run the following command to print the endpoint to use for reading the data. It fetches the code from the FybrikApplication resource: ENDPOINT_SCHEME = $( kubectl get fybrikapplication my-notebook -o jsonpath ={ .status.assetStates.fybrik-notebook-sample/paysim-csv.endpoint.fybrik-arrow-flight.scheme } ) ENDPOINT_HOSTNAME = $( kubectl get fybrikapplication my-notebook -o jsonpath ={ .status.assetStates.fybrik-notebook-sample/paysim-csv.endpoint.fybrik-arrow-flight.hostname } ) ENDPOINT_PORT = $( kubectl get fybrikapplication my-notebook -o jsonpath ={ .status.assetStates.fybrik-notebook-sample/paysim-csv.endpoint.fybrik-arrow-flight.port } ) printf \"\\n ${ ENDPOINT_SCHEME } :// ${ ENDPOINT_HOSTNAME } : ${ ENDPOINT_PORT } \\n\\n\" The next steps use the endpoint to read the data in a python notebook Insert a new notebook cell to install pandas and pyarrow packages: % pip install pandas pyarrow == 7.0 .* Insert a new notebook cell to read the data using the endpoint value extracted from the FybrikApplication in the previous step: import json import pyarrow.flight as fl import pandas as pd # Create a Flight client client = fl.connect ( '<ENDPOINT>' ) # Prepare the request request = { \"asset\" : \"fybrik-notebook-sample/paysim-csv\" , # To request specific columns add to the request a \"columns\" key with a list of column names # \"columns\": [...] } # Send request and fetch result as a pandas DataFrame info = client.get_flight_info ( fl.FlightDescriptor.for_command ( json.dumps ( request ))) reader: fl.FlightStreamReader = client.do_get ( info.endpoints [ 0 ] .ticket ) df: pd.DataFrame = reader.read_pandas () Insert a new notebook cell with the following command to visualize the result: df Execute all notebook cells and notice that the nameOrig , oldbalanceOrg and newbalanceOrig columns appear redacted. Created by NTNU and shared under the CC BY-SA 4.0 license. \u21a9","title":"Read the dataset from the notebook"},{"location":"samples/notebook-write/","text":"Notebook sample for the write flow This sample shows three scenarios: how fybrik prevents writing a new asset due to governance restrictions. how to write data generated by the workload to an object store. how to read data from a dataset stored in an object store. In this sample you play multiple roles: As a data governance officer you setup data governance policies. As a data user you specify your data usage requirements and use a notebook to write and read the data. Create an account in object storage Create an account in object storage of your choice such as AWS S3, IBM Cloud Object Storage or Ceph. Make a note of the service endpoint and access credentials. You will need them later. Setup localstack For experimentation you can install localstack to your cluster instead of using a cloud service. Define variables for access key and secret key export ACCESS_KEY = \"myaccesskey\" export SECRET_KEY = \"mysecretkey\" Install localstack to the currently active namespace and wait for it to be ready: helm repo add localstack-charts https://localstack.github.io/helm-charts helm install localstack localstack-charts/localstack --set startServices = \"s3\" --set service.type = ClusterIP kubectl wait --for = condition = ready --all pod -n fybrik-notebook-sample --timeout = 120s create a port-forward to communicate with localstack server: kubectl port-forward svc/localstack 4566 :4566 & Use AWS CLI to configure localstack server: aws configure set aws_access_key_id ${ ACCESS_KEY } && aws configure set aws_secret_access_key ${ SECRET_KEY } Deploy resources for write scenarios Deploy Datashim : kubectl apply -f https://raw.githubusercontent.com/fybrik/fybrik/master/third_party/datashim/dlf.yaml For more deployment options of Datashim based on your environment please refer to the datashim site . Register the credentials required for accessing the object storage. Replace the values for access_key and secret_key with the values from the object storage service that you used and run: cat << EOF | kubectl apply -f - apiVersion : v1 kind : Secret metadata : name : bucket-creds namespace : fybrik-system type : Opaque stringData : access_key : \"${ACCESS_KEY}\" accessKeyID : \"${ACCESS_KEY}\" secret_key : \"${SECRET_KEY}\" secretAccessKey : \"${SECRET_KEY}\" EOF Then, register two storage accounts: one in theshire and one in neverland . Replace the value for endpoint with value from the object storage service that you used and run: cat << EOF | kubectl apply -f - apiVersion : app.fybrik.io/v1alpha1 kind : FybrikStorageAccount metadata : name : theshire-storage-account namespace : fybrik-system spec : id : theshire-object-store region : theshire endpoint : \"http://localstack.fybrik-notebook-sample.svc.cluster.local:4566\" secretRef : bucket-creds EOF cat << EOF | kubectl apply -f - apiVersion : app.fybrik.io/v1alpha1 kind : FybrikStorageAccount metadata : name : neverland-storage-account namespace : fybrik-system spec : id : neverland-object-store region : neverland endpoint : \"http://localstack.fybrik-notebook-sample.svc.cluster.local:4566\" secretRef : bucket-creds EOF Note that for evaluation purposes the same object store is used for different regions in the storage accounts. Scenario one: write is forbidden due to governance restrictions Define data governance policies for write Define an OpenPolicyAgent policy to forbid the writing of sensitive data to regions neverland and theshire in datasets tagged with finance . This policy prevents the writing as the deployed fybrik storage account resources applied are in neverland and theshire . Below is the policy (written in Rego language): package dataapi.authz rule[{\"policy\": description}] { description := \"Forbid writing sensitive data in `theshire` and `neverland` storage accounts in datasets tagged with `finance`\" input.action.actionType == \"write\" input.resource.metadata.tags.finance input.action.destination != \"theshire\" input.action.destination != \"neverland\" input.resource.metadata.columns[i].tags.sensitive } For more details on OPA policies please refer to Using OPA for Data Governance task. Copy the policy to a file named sample-policy-write.rego and then run: kubectl -n fybrik-system create configmap sample-policy-write --from-file = sample-policy-write.rego kubectl -n fybrik-system label configmap sample-policy-write openpolicyagent.org/policy = rego while [[ $( kubectl get cm sample-policy-write -n fybrik-system -o 'jsonpath={.metadata.annotations.openpolicyagent\\.org/policy-status}' ) ! = '{\"status\":\"ok\"}' ]] ; do echo \"waiting for policy to be applied\" && sleep 5 ; done Create a FybrikApplication resource to write the new asset Create a FybrikApplication resource to register the notebook workload to the control plane of Fybrik: cat <<EOF | kubectl apply -f - apiVersion : app.fybrik.io/v1alpha1 kind : FybrikApplication metadata : name : my-notebook-write namespace : fybrik-notebook-sample labels : app : my-notebook-write spec : selector : clusterName : thegreendragon workloadSelector : matchLabels : app : my-notebook-write appInfo : intent : Fraud Detection data : - dataSetID : 'new-data' flow : write requirements : flowParams : isNewDataSet : true catalog : fybrik-notebook-sample metadata : tags : finance : true columns : - name : nameOrig tags : PII : true - name : oldbalanceOrg tags : sensitive : true interface : protocol : fybrik-arrow-flight EOF Notice that: The selector field matches the labels of our Jupyter notebook workload. The data field includes a dataSetID that matches the asset identifier in the catalog. The protocol indicates that the developer wants to consume the data using Apache Arrow Flight. For some protocols a dataformat can be specified as well (e.g., s3 protocol and parquet format). The isNewDataSet field indicates is a new asset. The catalog field holds the catalog id. It will be used by fybrik to register the new asset in the catalog. metadata field specifies the dataset tags. These attributes can later be used in policies. Run the following command to wait until the FybrikApplication status is updated: while [[ $( kubectl get fybrikapplication my-notebook-write -o 'jsonpath={.status.ready}' ) ! = \"true\" ]] ; do echo \"waiting for FybrikApplication\" && sleep 5 ; done while [[ $( kubectl get fybrikapplication my-notebook-write -n fybrik-notebook-sample -o 'jsonpath={.status.assetStates.new-data.conditions[?(@.type == \"Deny\")].status}' ) ! = \"True\" ]] ; do echo \"waiting for my-notebook-write asset\" && sleep 5 ; done We expect the asset's status in FybrikApplication.status to be denied due to the policy defined above. Next, a new policy will be applied which will allow the writing to theshire object store. Cleanup scenario one Before prceeding to scenario two the OPA policy and fybrikapplications should be deleted: kubectl delete cm sample-policy-write -n fybrik-system kubectl delete fybrikapplications.app.fybrik.io my-notebook-write -n fybrik-notebook-sample Notice that FybrikStorageAccount resources are still applied after the cleanup. Scenario two: write new data To write the new data a new policy should be defined. Define data access policies for writing the data Define an OpenPolicyAgent policy to return the list of column names tagged as sensitive, whose destination is not neverland, when the actionType is write. The columns are passed to the FybrikModule together with RedactAction upon deployment of the module by Fybrik. Below is the policy (written in Rego language): package dataapi.authz rule[{\"action\": {\"name\":\"RedactAction\",\"columns\": column_names}, \"policy\": description}] { description := \"Redact written columns tagged as sensitive in datasets tagged with finance = true. The data should not be stored in `neverland` storage account\" input.action.actionType == \"write\" input.resource.metadata.tags.finance input.action.destination != \"neverland\" column_names := [input.resource.metadata.columns[i].name | input.resource.metadata.columns[i].tags.sensitive] } Copy the policy to a file named sample-policy-write.rego and then run: kubectl -n fybrik-system create configmap sample-policy-write --from-file = sample-policy-write.rego kubectl -n fybrik-system label configmap sample-policy-write openpolicyagent.org/policy = rego while [[ $( kubectl get cm sample-policy-write -n fybrik-system -o 'jsonpath={.metadata.annotations.openpolicyagent\\.org/policy-status}' ) ! = '{\"status\":\"ok\"}' ]] ; do echo \"waiting for policy to be applied\" && sleep 5 ; done Deploy a Jupyter notebook In this sample a Jupyter notebook is used as the user workload and its business logic requires writing the new asset. Deploy a notebook to your cluster: execute the instructions from Deploy a Jupyter notebook section in the notebook sample for the read flow to deploy a Jupyter notebook. Create a FybrikApplication resource associated with the notebook Re-apply FybrikApplication as defined in scenario 1. Run the following command to wait until the FybrikApplication status is ready: while [[ $( kubectl get fybrikapplication my-notebook-write -o 'jsonpath={.status.ready}' ) ! = \"true\" ]] ; do echo \"waiting for FybrikApplication\" && sleep 5 ; done while [[ $( kubectl get fybrikapplication my-notebook-write -o 'jsonpath={.status.assetStates.new-data.conditions[?(@.type == \"Ready\")].status}' ) ! = \"True\" ]] ; do echo \"waiting for new-data asset\" && sleep 5 ; done Run the following command to extract the new cataloged asset id from fybrikapplication status. This asset id will be used in the third secnario when we try to read the new asset. CATALOGED_ASSET = $( kubectl get fybrikapplication my-notebook-write -o 'jsonpath={.status.assetStates.new-data.catalogedAsset}' ) Write the data from the notebook This sample uses the Synthetic Financial Datasets For Fraud Detection dataset 1 as the data that the notebook needs to write. Download and extract the file to your machine. You should now see a file named PS_20174392719_1491204439457_log.csv . Alternatively, use a sample of 100 lines of the same dataset by downloading PS_20174392719_1491204439457_log.csv from GitHub. In your terminal , run the following command to print the endpoint to use for reading the data. It fetches the code from the FybrikApplication resource: ENDPOINT_SCHEME = $( kubectl get fybrikapplication my-notebook-write -o jsonpath ={ .status.assetStates.new-data.endpoint.fybrik-arrow-flight.scheme } ) ENDPOINT_HOSTNAME = $( kubectl get fybrikapplication my-notebook-write -o jsonpath ={ .status.assetStates.new-data.endpoint.fybrik-arrow-flight.hostname } ) ENDPOINT_PORT = $( kubectl get fybrikapplication my-notebook-write -o jsonpath ={ .status.assetStates.new-data.endpoint.fybrik-arrow-flight.port } ) printf \"\\n ${ ENDPOINT_SCHEME } :// ${ ENDPOINT_HOSTNAME } : ${ ENDPOINT_PORT } \\n\\n\" The next steps use the endpoint to write the data in a python notebook Insert a new notebook cell to install needed packages: % pip install pyarrow == 7.0 .* Insert a new notebook cell to write data using the endpoint value extracted from the FybrikApplication in the previous step: import pyarrow.flight as fl import json from pyarrow import csv # Create a Flight client client = fl.connect('<ENDPOINT>') # Prepare the request request = { \"asset\": \"new-data\", # To request specific columns add to the request a \"columns\" key with a list of column names # \"columns\": [...] } # write the new dataset file_path = \"/path/to/PS_20174392719_1491204439457_log.csv\" my_table = csv.read_csv(file_path) writer, _ = client.do_put(fl.FlightDescriptor.for_command(json.dumps(request)), my_table.schema) # Note that we do not indicate the data store nor allocate a bucket in which # to write the dataset. This is all done by Fybrik. writer.write_table(my_table) writer.close() Cleanup scenario two kubectl delete cm sample-policy-write -n fybrik-system kubectl delete fybrikapplications.app.fybrik.io my-notebook-write -n fybrik-notebook-sample Scenario 3: Read the newly written data Define data access policies to read the new data Define an OpenPolicyAgent policy to allow reading the data. Below is the policy (written in Rego language): package dataapi.authz rule[{}] { description := \"allow read datasets\" input.action.actionType == \"read\" } Copy the policy to a file named sample-policy-read.rego and then run: kubectl -n fybrik-system create configmap sample-policy-read --from-file = sample-policy-read.rego kubectl -n fybrik-system label configmap sample-policy-read openpolicyagent.org/policy = rego while [[ $( kubectl get cm sample-policy-read -n fybrik-system -o 'jsonpath={.metadata.annotations.openpolicyagent\\.org/policy-status}' ) ! = '{\"status\":\"ok\"}' ]] ; do echo \"waiting for policy to be applied\" && sleep 5 ; done Create a FybrikApplication resource to read the data for the notebook Create a FybrikApplication resource to register the notebook workload to the control plane of Fybrik: cat <<EOF | kubectl apply -f - apiVersion: app.fybrik.io/v1alpha1 kind: FybrikApplication metadata: name: my-notebook-read namespace: fybrik-notebook-sample labels: app: my-notebook-read spec: selector: clusterName: thegreendragon workloadSelector: matchLabels: app: my-notebook-read appInfo: intent: Fraud Detection data: - dataSetID: fybrik-notebook-sample/${CATALOGED_ASSET} flow: read requirements: interface: protocol: fybrik-arrow-flight EOF Run the following command to wait until the FybrikApplication is ready: while [[ $( kubectl get fybrikapplication my-notebook-read -o 'jsonpath={.status.ready}' ) ! = \"true\" ]] ; do echo \"waiting for FybrikApplication\" && sleep 5 ; done while [[ $( kubectl get fybrikapplication my-notebook-read -o \"jsonpath={.status.assetStates.fybrik-notebook-sample/ ${ CATALOGED_ASSET } .conditions[?(@.type == 'Ready')].status}\" ) ! = \"True\" ]] ; do echo \"waiting for fybrik-notebook-sample/ ${ CATALOGED_ASSET } asset\" && sleep 5 ; done Read the dataset from the notebook In your terminal , run the following command to print the endpoint to use for reading the data. It fetches the code from the FybrikApplication resource: ENDPOINT_SCHEME = $( kubectl get fybrikapplication my-notebook-read -o jsonpath ={ .status.assetStates.fybrik-notebook-sample/ ${ CATALOGED_ASSET } .endpoint.fybrik-arrow-flight.scheme } ) ENDPOINT_HOSTNAME = $( kubectl get fybrikapplication my-notebook-read -o jsonpath ={ .status.assetStates.fybrik-notebook-sample/ ${ CATALOGED_ASSET } .endpoint.fybrik-arrow-flight.hostname } ) ENDPOINT_PORT = $( kubectl get fybrikapplication my-notebook-read -o jsonpath ={ .status.assetStates.fybrik-notebook-sample/ ${ CATALOGED_ASSET } .endpoint.fybrik-arrow-flight.port } ) printf \"\\n ${ ENDPOINT_SCHEME } :// ${ ENDPOINT_HOSTNAME } : ${ ENDPOINT_PORT } \\n\\n\" The next steps use the endpoint to read the data in a python notebook. Insert a new notebook cell to install pandas and pyarrow packages: % pip install pandas pyarrow == 7.0 .* Insert a new notebook cell to read the data. Notice : ENDPOINT and CATALOGED_ASSET should be replaced with the values extracted from the FybrikApplication as described in previous steps. import json import pyarrow.flight as fl import pandas as pd # Create a Flight client client = fl.connect ( '<ENDPOINT>' ) # Prepare the request request = { \"asset\" : \"fybrik-notebook-sample/<CATALOGED_ASSET>\" , # To request specific columns add to the request a \"columns\" key with a list of column names # \"columns\": [...] } # show all columns pd.set_option ( 'display.max_columns' , None ) # Send request and fetch result as a pandas DataFrame info = client.get_flight_info ( fl.FlightDescriptor.for_command ( json.dumps ( request ))) reader: fl.FlightStreamReader = client.do_get ( info.endpoints [ 0 ] .ticket ) df_read: pd.DataFrame = reader.read_pandas () Insert a new notebook cell with the following command to visualize the result: df_read Execute all notebook cells and notice that the oldbalanceOrg column do not appear because it was redacted. Created by NTNU and shared under the CC BY-SA 4.0 license. \u21a9","title":"Notebook sample for the write flow"},{"location":"samples/notebook-write/#notebook-sample-for-the-write-flow","text":"This sample shows three scenarios: how fybrik prevents writing a new asset due to governance restrictions. how to write data generated by the workload to an object store. how to read data from a dataset stored in an object store. In this sample you play multiple roles: As a data governance officer you setup data governance policies. As a data user you specify your data usage requirements and use a notebook to write and read the data.","title":"Notebook sample for the write flow"},{"location":"samples/notebook-write/#create-an-account-in-object-storage","text":"Create an account in object storage of your choice such as AWS S3, IBM Cloud Object Storage or Ceph. Make a note of the service endpoint and access credentials. You will need them later. Setup localstack For experimentation you can install localstack to your cluster instead of using a cloud service. Define variables for access key and secret key export ACCESS_KEY = \"myaccesskey\" export SECRET_KEY = \"mysecretkey\" Install localstack to the currently active namespace and wait for it to be ready: helm repo add localstack-charts https://localstack.github.io/helm-charts helm install localstack localstack-charts/localstack --set startServices = \"s3\" --set service.type = ClusterIP kubectl wait --for = condition = ready --all pod -n fybrik-notebook-sample --timeout = 120s create a port-forward to communicate with localstack server: kubectl port-forward svc/localstack 4566 :4566 & Use AWS CLI to configure localstack server: aws configure set aws_access_key_id ${ ACCESS_KEY } && aws configure set aws_secret_access_key ${ SECRET_KEY }","title":"Create an account in object storage"},{"location":"samples/notebook-write/#deploy-resources-for-write-scenarios","text":"Deploy Datashim : kubectl apply -f https://raw.githubusercontent.com/fybrik/fybrik/master/third_party/datashim/dlf.yaml For more deployment options of Datashim based on your environment please refer to the datashim site . Register the credentials required for accessing the object storage. Replace the values for access_key and secret_key with the values from the object storage service that you used and run: cat << EOF | kubectl apply -f - apiVersion : v1 kind : Secret metadata : name : bucket-creds namespace : fybrik-system type : Opaque stringData : access_key : \"${ACCESS_KEY}\" accessKeyID : \"${ACCESS_KEY}\" secret_key : \"${SECRET_KEY}\" secretAccessKey : \"${SECRET_KEY}\" EOF Then, register two storage accounts: one in theshire and one in neverland . Replace the value for endpoint with value from the object storage service that you used and run: cat << EOF | kubectl apply -f - apiVersion : app.fybrik.io/v1alpha1 kind : FybrikStorageAccount metadata : name : theshire-storage-account namespace : fybrik-system spec : id : theshire-object-store region : theshire endpoint : \"http://localstack.fybrik-notebook-sample.svc.cluster.local:4566\" secretRef : bucket-creds EOF cat << EOF | kubectl apply -f - apiVersion : app.fybrik.io/v1alpha1 kind : FybrikStorageAccount metadata : name : neverland-storage-account namespace : fybrik-system spec : id : neverland-object-store region : neverland endpoint : \"http://localstack.fybrik-notebook-sample.svc.cluster.local:4566\" secretRef : bucket-creds EOF Note that for evaluation purposes the same object store is used for different regions in the storage accounts.","title":"Deploy resources for write scenarios"},{"location":"samples/notebook-write/#scenario-one-write-is-forbidden-due-to-governance-restrictions","text":"","title":"Scenario one: write is forbidden due to governance restrictions"},{"location":"samples/notebook-write/#define-data-governance-policies-for-write","text":"Define an OpenPolicyAgent policy to forbid the writing of sensitive data to regions neverland and theshire in datasets tagged with finance . This policy prevents the writing as the deployed fybrik storage account resources applied are in neverland and theshire . Below is the policy (written in Rego language): package dataapi.authz rule[{\"policy\": description}] { description := \"Forbid writing sensitive data in `theshire` and `neverland` storage accounts in datasets tagged with `finance`\" input.action.actionType == \"write\" input.resource.metadata.tags.finance input.action.destination != \"theshire\" input.action.destination != \"neverland\" input.resource.metadata.columns[i].tags.sensitive } For more details on OPA policies please refer to Using OPA for Data Governance task. Copy the policy to a file named sample-policy-write.rego and then run: kubectl -n fybrik-system create configmap sample-policy-write --from-file = sample-policy-write.rego kubectl -n fybrik-system label configmap sample-policy-write openpolicyagent.org/policy = rego while [[ $( kubectl get cm sample-policy-write -n fybrik-system -o 'jsonpath={.metadata.annotations.openpolicyagent\\.org/policy-status}' ) ! = '{\"status\":\"ok\"}' ]] ; do echo \"waiting for policy to be applied\" && sleep 5 ; done","title":"Define data governance policies for write"},{"location":"samples/notebook-write/#create-a-fybrikapplication-resource-to-write-the-new-asset","text":"Create a FybrikApplication resource to register the notebook workload to the control plane of Fybrik: cat <<EOF | kubectl apply -f - apiVersion : app.fybrik.io/v1alpha1 kind : FybrikApplication metadata : name : my-notebook-write namespace : fybrik-notebook-sample labels : app : my-notebook-write spec : selector : clusterName : thegreendragon workloadSelector : matchLabels : app : my-notebook-write appInfo : intent : Fraud Detection data : - dataSetID : 'new-data' flow : write requirements : flowParams : isNewDataSet : true catalog : fybrik-notebook-sample metadata : tags : finance : true columns : - name : nameOrig tags : PII : true - name : oldbalanceOrg tags : sensitive : true interface : protocol : fybrik-arrow-flight EOF Notice that: The selector field matches the labels of our Jupyter notebook workload. The data field includes a dataSetID that matches the asset identifier in the catalog. The protocol indicates that the developer wants to consume the data using Apache Arrow Flight. For some protocols a dataformat can be specified as well (e.g., s3 protocol and parquet format). The isNewDataSet field indicates is a new asset. The catalog field holds the catalog id. It will be used by fybrik to register the new asset in the catalog. metadata field specifies the dataset tags. These attributes can later be used in policies. Run the following command to wait until the FybrikApplication status is updated: while [[ $( kubectl get fybrikapplication my-notebook-write -o 'jsonpath={.status.ready}' ) ! = \"true\" ]] ; do echo \"waiting for FybrikApplication\" && sleep 5 ; done while [[ $( kubectl get fybrikapplication my-notebook-write -n fybrik-notebook-sample -o 'jsonpath={.status.assetStates.new-data.conditions[?(@.type == \"Deny\")].status}' ) ! = \"True\" ]] ; do echo \"waiting for my-notebook-write asset\" && sleep 5 ; done We expect the asset's status in FybrikApplication.status to be denied due to the policy defined above. Next, a new policy will be applied which will allow the writing to theshire object store.","title":"Create a FybrikApplication resource to write the new asset"},{"location":"samples/notebook-write/#cleanup-scenario-one","text":"Before prceeding to scenario two the OPA policy and fybrikapplications should be deleted: kubectl delete cm sample-policy-write -n fybrik-system kubectl delete fybrikapplications.app.fybrik.io my-notebook-write -n fybrik-notebook-sample Notice that FybrikStorageAccount resources are still applied after the cleanup.","title":"Cleanup scenario one"},{"location":"samples/notebook-write/#scenario-two-write-new-data","text":"To write the new data a new policy should be defined.","title":"Scenario two: write new data"},{"location":"samples/notebook-write/#define-data-access-policies-for-writing-the-data","text":"Define an OpenPolicyAgent policy to return the list of column names tagged as sensitive, whose destination is not neverland, when the actionType is write. The columns are passed to the FybrikModule together with RedactAction upon deployment of the module by Fybrik. Below is the policy (written in Rego language): package dataapi.authz rule[{\"action\": {\"name\":\"RedactAction\",\"columns\": column_names}, \"policy\": description}] { description := \"Redact written columns tagged as sensitive in datasets tagged with finance = true. The data should not be stored in `neverland` storage account\" input.action.actionType == \"write\" input.resource.metadata.tags.finance input.action.destination != \"neverland\" column_names := [input.resource.metadata.columns[i].name | input.resource.metadata.columns[i].tags.sensitive] } Copy the policy to a file named sample-policy-write.rego and then run: kubectl -n fybrik-system create configmap sample-policy-write --from-file = sample-policy-write.rego kubectl -n fybrik-system label configmap sample-policy-write openpolicyagent.org/policy = rego while [[ $( kubectl get cm sample-policy-write -n fybrik-system -o 'jsonpath={.metadata.annotations.openpolicyagent\\.org/policy-status}' ) ! = '{\"status\":\"ok\"}' ]] ; do echo \"waiting for policy to be applied\" && sleep 5 ; done","title":"Define data access policies for writing the data"},{"location":"samples/notebook-write/#deploy-a-jupyter-notebook","text":"In this sample a Jupyter notebook is used as the user workload and its business logic requires writing the new asset. Deploy a notebook to your cluster: execute the instructions from Deploy a Jupyter notebook section in the notebook sample for the read flow to deploy a Jupyter notebook.","title":"Deploy a Jupyter notebook"},{"location":"samples/notebook-write/#create-a-fybrikapplication-resource-associated-with-the-notebook","text":"Re-apply FybrikApplication as defined in scenario 1. Run the following command to wait until the FybrikApplication status is ready: while [[ $( kubectl get fybrikapplication my-notebook-write -o 'jsonpath={.status.ready}' ) ! = \"true\" ]] ; do echo \"waiting for FybrikApplication\" && sleep 5 ; done while [[ $( kubectl get fybrikapplication my-notebook-write -o 'jsonpath={.status.assetStates.new-data.conditions[?(@.type == \"Ready\")].status}' ) ! = \"True\" ]] ; do echo \"waiting for new-data asset\" && sleep 5 ; done Run the following command to extract the new cataloged asset id from fybrikapplication status. This asset id will be used in the third secnario when we try to read the new asset. CATALOGED_ASSET = $( kubectl get fybrikapplication my-notebook-write -o 'jsonpath={.status.assetStates.new-data.catalogedAsset}' )","title":"Create a FybrikApplication resource associated with the notebook"},{"location":"samples/notebook-write/#write-the-data-from-the-notebook","text":"This sample uses the Synthetic Financial Datasets For Fraud Detection dataset 1 as the data that the notebook needs to write. Download and extract the file to your machine. You should now see a file named PS_20174392719_1491204439457_log.csv . Alternatively, use a sample of 100 lines of the same dataset by downloading PS_20174392719_1491204439457_log.csv from GitHub. In your terminal , run the following command to print the endpoint to use for reading the data. It fetches the code from the FybrikApplication resource: ENDPOINT_SCHEME = $( kubectl get fybrikapplication my-notebook-write -o jsonpath ={ .status.assetStates.new-data.endpoint.fybrik-arrow-flight.scheme } ) ENDPOINT_HOSTNAME = $( kubectl get fybrikapplication my-notebook-write -o jsonpath ={ .status.assetStates.new-data.endpoint.fybrik-arrow-flight.hostname } ) ENDPOINT_PORT = $( kubectl get fybrikapplication my-notebook-write -o jsonpath ={ .status.assetStates.new-data.endpoint.fybrik-arrow-flight.port } ) printf \"\\n ${ ENDPOINT_SCHEME } :// ${ ENDPOINT_HOSTNAME } : ${ ENDPOINT_PORT } \\n\\n\" The next steps use the endpoint to write the data in a python notebook Insert a new notebook cell to install needed packages: % pip install pyarrow == 7.0 .* Insert a new notebook cell to write data using the endpoint value extracted from the FybrikApplication in the previous step: import pyarrow.flight as fl import json from pyarrow import csv # Create a Flight client client = fl.connect('<ENDPOINT>') # Prepare the request request = { \"asset\": \"new-data\", # To request specific columns add to the request a \"columns\" key with a list of column names # \"columns\": [...] } # write the new dataset file_path = \"/path/to/PS_20174392719_1491204439457_log.csv\" my_table = csv.read_csv(file_path) writer, _ = client.do_put(fl.FlightDescriptor.for_command(json.dumps(request)), my_table.schema) # Note that we do not indicate the data store nor allocate a bucket in which # to write the dataset. This is all done by Fybrik. writer.write_table(my_table) writer.close()","title":"Write the data from the notebook"},{"location":"samples/notebook-write/#cleanup-scenario-two","text":"kubectl delete cm sample-policy-write -n fybrik-system kubectl delete fybrikapplications.app.fybrik.io my-notebook-write -n fybrik-notebook-sample","title":"Cleanup scenario two"},{"location":"samples/notebook-write/#scenario-3-read-the-newly-written-data","text":"","title":"Scenario 3: Read the newly written data"},{"location":"samples/notebook-write/#define-data-access-policies-to-read-the-new-data","text":"Define an OpenPolicyAgent policy to allow reading the data. Below is the policy (written in Rego language): package dataapi.authz rule[{}] { description := \"allow read datasets\" input.action.actionType == \"read\" } Copy the policy to a file named sample-policy-read.rego and then run: kubectl -n fybrik-system create configmap sample-policy-read --from-file = sample-policy-read.rego kubectl -n fybrik-system label configmap sample-policy-read openpolicyagent.org/policy = rego while [[ $( kubectl get cm sample-policy-read -n fybrik-system -o 'jsonpath={.metadata.annotations.openpolicyagent\\.org/policy-status}' ) ! = '{\"status\":\"ok\"}' ]] ; do echo \"waiting for policy to be applied\" && sleep 5 ; done","title":"Define data access policies to read the new data"},{"location":"samples/notebook-write/#create-a-fybrikapplication-resource-to-read-the-data-for-the-notebook","text":"Create a FybrikApplication resource to register the notebook workload to the control plane of Fybrik: cat <<EOF | kubectl apply -f - apiVersion: app.fybrik.io/v1alpha1 kind: FybrikApplication metadata: name: my-notebook-read namespace: fybrik-notebook-sample labels: app: my-notebook-read spec: selector: clusterName: thegreendragon workloadSelector: matchLabels: app: my-notebook-read appInfo: intent: Fraud Detection data: - dataSetID: fybrik-notebook-sample/${CATALOGED_ASSET} flow: read requirements: interface: protocol: fybrik-arrow-flight EOF Run the following command to wait until the FybrikApplication is ready: while [[ $( kubectl get fybrikapplication my-notebook-read -o 'jsonpath={.status.ready}' ) ! = \"true\" ]] ; do echo \"waiting for FybrikApplication\" && sleep 5 ; done while [[ $( kubectl get fybrikapplication my-notebook-read -o \"jsonpath={.status.assetStates.fybrik-notebook-sample/ ${ CATALOGED_ASSET } .conditions[?(@.type == 'Ready')].status}\" ) ! = \"True\" ]] ; do echo \"waiting for fybrik-notebook-sample/ ${ CATALOGED_ASSET } asset\" && sleep 5 ; done","title":"Create a FybrikApplication resource to read the data for the notebook"},{"location":"samples/notebook-write/#read-the-dataset-from-the-notebook","text":"In your terminal , run the following command to print the endpoint to use for reading the data. It fetches the code from the FybrikApplication resource: ENDPOINT_SCHEME = $( kubectl get fybrikapplication my-notebook-read -o jsonpath ={ .status.assetStates.fybrik-notebook-sample/ ${ CATALOGED_ASSET } .endpoint.fybrik-arrow-flight.scheme } ) ENDPOINT_HOSTNAME = $( kubectl get fybrikapplication my-notebook-read -o jsonpath ={ .status.assetStates.fybrik-notebook-sample/ ${ CATALOGED_ASSET } .endpoint.fybrik-arrow-flight.hostname } ) ENDPOINT_PORT = $( kubectl get fybrikapplication my-notebook-read -o jsonpath ={ .status.assetStates.fybrik-notebook-sample/ ${ CATALOGED_ASSET } .endpoint.fybrik-arrow-flight.port } ) printf \"\\n ${ ENDPOINT_SCHEME } :// ${ ENDPOINT_HOSTNAME } : ${ ENDPOINT_PORT } \\n\\n\" The next steps use the endpoint to read the data in a python notebook. Insert a new notebook cell to install pandas and pyarrow packages: % pip install pandas pyarrow == 7.0 .* Insert a new notebook cell to read the data. Notice : ENDPOINT and CATALOGED_ASSET should be replaced with the values extracted from the FybrikApplication as described in previous steps. import json import pyarrow.flight as fl import pandas as pd # Create a Flight client client = fl.connect ( '<ENDPOINT>' ) # Prepare the request request = { \"asset\" : \"fybrik-notebook-sample/<CATALOGED_ASSET>\" , # To request specific columns add to the request a \"columns\" key with a list of column names # \"columns\": [...] } # show all columns pd.set_option ( 'display.max_columns' , None ) # Send request and fetch result as a pandas DataFrame info = client.get_flight_info ( fl.FlightDescriptor.for_command ( json.dumps ( request ))) reader: fl.FlightStreamReader = client.do_get ( info.endpoints [ 0 ] .ticket ) df_read: pd.DataFrame = reader.read_pandas () Insert a new notebook cell with the following command to visualize the result: df_read Execute all notebook cells and notice that the oldbalanceOrg column do not appear because it was redacted. Created by NTNU and shared under the CC BY-SA 4.0 license. \u21a9","title":"Read the dataset from the notebook"},{"location":"samples/pre-steps/","text":"Tools used by the actors The data owner would typically register the dataset in a proprietary or open source catalog. We have provided katalog - a thin layer acting as a replacement for the data catalog for evaluation purposes. This simplifies the sample deployment. The data owner stores credentials for accessing the dataset in kubernetes secrets. Proprietary and open source data governance systems are available either as part of a data catalog or as stand-alone systems. This sample uses the open source OpenPolicyAgent . The data governance officer writes the policies in OPA's rego language. Any editor can be used to write the FybrikApplication.yaml via which the data user expresses the data usage requirements. A jupyter notebook is the workload from which the data is consumed by the data user. A Web Browser Prepare Fybrik environment Typically, this would be done by an IT administrator. Install Fybrik using the Quick Start guide. This sample assumes the use of the built-in catalog, Open Policy Agent (OPA) and flight module. Create a namespace for the sample Create a new Kubernetes namespace and set it as the active namespace: kubectl create namespace fybrik-notebook-sample kubectl config set-context --current --namespace = fybrik-notebook-sample This enables easy cleanup once you're done experimenting with the sample.","title":"Before we begin"},{"location":"samples/pre-steps/#tools-used-by-the-actors","text":"The data owner would typically register the dataset in a proprietary or open source catalog. We have provided katalog - a thin layer acting as a replacement for the data catalog for evaluation purposes. This simplifies the sample deployment. The data owner stores credentials for accessing the dataset in kubernetes secrets. Proprietary and open source data governance systems are available either as part of a data catalog or as stand-alone systems. This sample uses the open source OpenPolicyAgent . The data governance officer writes the policies in OPA's rego language. Any editor can be used to write the FybrikApplication.yaml via which the data user expresses the data usage requirements. A jupyter notebook is the workload from which the data is consumed by the data user. A Web Browser","title":"Tools used by the actors"},{"location":"samples/pre-steps/#prepare-fybrik-environment","text":"Typically, this would be done by an IT administrator. Install Fybrik using the Quick Start guide. This sample assumes the use of the built-in catalog, Open Policy Agent (OPA) and flight module.","title":"Prepare Fybrik environment"},{"location":"samples/pre-steps/#create-a-namespace-for-the-sample","text":"Create a new Kubernetes namespace and set it as the active namespace: kubectl create namespace fybrik-notebook-sample kubectl config set-context --current --namespace = fybrik-notebook-sample This enables easy cleanup once you're done experimenting with the sample.","title":"Create a namespace for the sample"},{"location":"tasks/control-plane-security/","text":"Enable Control Plane Security Kubernetes NetworkPolicies , TLS/mTLS and optionally Istio can be used to protect components of the control plane. Specifically, traffic to connectors that run as part of the control plane must be secured. Follow this page to enable control plane security. Ingress traffic policy The installation of Fybrik applies a Kubernetes NetworkPolicy resource to the fybrik-system namespace. This resource ensures that ingress traffic to connectors is only allowed from workloads that run in the fybrik-system namespace and thus disallow access to connectors from other namespaces or external parties. The NetworkPolicy is always created. However, your Kubernetes cluster must have a Network Plugin with NetworkPolicy support. Otherwise, NetworkPolicy resources will have no affect. While most Kubernetes distributions include a network plugin that enfoces network policies, some like Kind do not and require you to install a separate network plugin instead. Transport Layer Security (TLS) Configure Fybrik to use TLS Fybrik can be configured to protect traffic between the manager and connectors by using TLS. In addition, mutual TLS authentication is possible too. In the TLS mode, the connectors (aka the servers) should have their certificates available to provide them to the manager (aka client) in the TLS protocol handshake process. In mutual TLS mode, both the manager and connector should have their certificates available. If private Certificate Authorities (CA) is used then its credentials should be installed too. Generating TLS certificates and keys For development and testing the TLS certificates and certificate keys can be generated using openSSL library. For more information on the process please refer to online documentation such as this useful tutorial . Cert-manager can also be used to automatically generate and renew the TLS certificate using its Certificate resource. The following is an example of a Certificate resource for the opa-connector where a tls type secret named tls-opa-connector-certs containing the certificate and certificate key is automatically created by the cert-manager. The issuerRef field points to a cert-manager resource name Issuer that holds the information about the CA that signs the certificate. apiVersion: cert-manager.io/v1 kind: Certificate metadata: name: opa-connector-cert namespace: fybrik-system spec: dnsNames: - opa-connector issuerRef: kind: Issuer name: ca-issuer secretName: tls-opa-connector-certs Adding TLS Secrets The manager/connectors certificates are kept in Kubernetes secret: For each component copy its certificate into a file names tls.crt. Copy the certificate key into a file named tls.key. Use kubectl with the tls secret type to create the secrets. kubectl -n fybrik-system create secret tls tls-opa-connector-certs \\ --cert = tls.crt \\ --key = tls.key If cert-manager is used to manage the certificates then the secret is automatically created as shown above. Using a Private CA Signed Certificate If you are using a private CA, Fybrik requires a copy of the CA certificate which is used by connector/manager to validate the connection to the manager/connectors. For each component copy the CA certificate into a file named cacerts.pem and use kubectl to create the tls-ca secret in the fybrik-system namespace. kubectl -n fybrik-system create secret generic tls-ca \\ --from-file = cacerts.pem = ./cacerts.pem Here is an example of a self-signed issuer managed by cert-manager. The secret tls-ca that holds the CA certificate is created and automatically renewed by cert-manager. apiVersion: cert-manager.io/v1 kind: ClusterIssuer metadata: name: ca-issuer-self-signed spec: selfSigned: {} --- apiVersion: cert-manager.io/v1 kind: Issuer metadata: name: ca-issuer namespace: fybrik-system spec: ca: secretName: tls-ca --- apiVersion: cert-manager.io/v1 kind: Certificate metadata: name: ca-certificate namespace: fybrik-system spec: isCA: true commonName: fybrik secretName: tls-ca issuerRef: name: ca-issuer-self-signed kind: ClusterIssuer group: cert-manager.io Update Values.yaml file To use TLS the infomation about the secrets above should be inserted to the fields in values.yaml file upon Fybrik deployment using helm. Here is an example of the tls related fields in the opa-connector section that are filled based on the secrets created above: opaConnector: tls: # Specifies whether the opa connector communication should use tls. use_tls: true # Specifies whether the opa connector communication should use mutual tls. use_mtls: true # Relavent if the connection between the manager and the connectors # uses tls. certs: # Name of kubernetes tls secret that holds opa-connector certificates. # The secret should be of `kubernetes.io/tls` type. # Relavent if tls is used. certSecretName: \"tls-opa-connector-certs\" # Name of kubernetes tls secret namespace that holds opa-connector certificate. certSecretNamespace: \"fybrik-system\" # Name of kubernetes secret that holds the certificate authority (CA) certificate which is used # by opa-connector to validate the connection to the manager if mtls is enabled. cacertSecretName: \"tls-ca\" # Name of kubernetes secret namespace that holds the certificate authority (CA) certificate which # is used by opa-connector to validate the connection to the manager if mtls is enabled. cacertSecretNamespace: \"fybrik-system\" Using Istio Alternatively, if Istio is installed in the cluster then you can use automatic mutual TLS to encrypt the traffic to the connectors. Follow these steps to enable mutual TLS: Ensure that Istio 1.6 or above is installed. Enable Istio sidecar injection in the fybrik-system namespace: kubectl label namespace fybrik-system istio-injection = enabled - Create Istio PeerAuthentication resource to enable mutual TLS between containers with Istio sidecars: cat << EOF | kubectl apply -f - apiVersion: \"security.istio.io/v1beta1\" kind: \"PeerAuthentication\" metadata: name: \"premissive-mtls-in-control-plane\" namespace: fybrik-system spec: mtls: mode: PERMISSIVE EOF - Create Istio Sidecar resource to allow any egress traffic from the control plane containers: cat << EOF | kubectl apply -f - apiVersion: networking.istio.io/v1alpha3 kind: Sidecar metadata: name: sidecar-default namespace: fybrik-system spec: egress: - hosts: - \"*/*\" outboundTrafficPolicy: mode: ALLOW_ANY EOF - Restart the control plane pods: kubectl delete pod --all -n fybrik-system","title":"Enable Control Plane Security"},{"location":"tasks/control-plane-security/#enable-control-plane-security","text":"Kubernetes NetworkPolicies , TLS/mTLS and optionally Istio can be used to protect components of the control plane. Specifically, traffic to connectors that run as part of the control plane must be secured. Follow this page to enable control plane security.","title":"Enable Control Plane Security"},{"location":"tasks/control-plane-security/#ingress-traffic-policy","text":"The installation of Fybrik applies a Kubernetes NetworkPolicy resource to the fybrik-system namespace. This resource ensures that ingress traffic to connectors is only allowed from workloads that run in the fybrik-system namespace and thus disallow access to connectors from other namespaces or external parties. The NetworkPolicy is always created. However, your Kubernetes cluster must have a Network Plugin with NetworkPolicy support. Otherwise, NetworkPolicy resources will have no affect. While most Kubernetes distributions include a network plugin that enfoces network policies, some like Kind do not and require you to install a separate network plugin instead.","title":"Ingress traffic policy"},{"location":"tasks/control-plane-security/#transport-layer-security-tls","text":"","title":"Transport Layer Security (TLS)"},{"location":"tasks/control-plane-security/#configure-fybrik-to-use-tls","text":"Fybrik can be configured to protect traffic between the manager and connectors by using TLS. In addition, mutual TLS authentication is possible too. In the TLS mode, the connectors (aka the servers) should have their certificates available to provide them to the manager (aka client) in the TLS protocol handshake process. In mutual TLS mode, both the manager and connector should have their certificates available. If private Certificate Authorities (CA) is used then its credentials should be installed too.","title":"Configure Fybrik to use TLS"},{"location":"tasks/control-plane-security/#generating-tls-certificates-and-keys","text":"For development and testing the TLS certificates and certificate keys can be generated using openSSL library. For more information on the process please refer to online documentation such as this useful tutorial . Cert-manager can also be used to automatically generate and renew the TLS certificate using its Certificate resource. The following is an example of a Certificate resource for the opa-connector where a tls type secret named tls-opa-connector-certs containing the certificate and certificate key is automatically created by the cert-manager. The issuerRef field points to a cert-manager resource name Issuer that holds the information about the CA that signs the certificate. apiVersion: cert-manager.io/v1 kind: Certificate metadata: name: opa-connector-cert namespace: fybrik-system spec: dnsNames: - opa-connector issuerRef: kind: Issuer name: ca-issuer secretName: tls-opa-connector-certs","title":"Generating TLS certificates and keys"},{"location":"tasks/control-plane-security/#adding-tls-secrets","text":"The manager/connectors certificates are kept in Kubernetes secret: For each component copy its certificate into a file names tls.crt. Copy the certificate key into a file named tls.key. Use kubectl with the tls secret type to create the secrets. kubectl -n fybrik-system create secret tls tls-opa-connector-certs \\ --cert = tls.crt \\ --key = tls.key If cert-manager is used to manage the certificates then the secret is automatically created as shown above.","title":"Adding TLS Secrets"},{"location":"tasks/control-plane-security/#using-a-private-ca-signed-certificate","text":"If you are using a private CA, Fybrik requires a copy of the CA certificate which is used by connector/manager to validate the connection to the manager/connectors. For each component copy the CA certificate into a file named cacerts.pem and use kubectl to create the tls-ca secret in the fybrik-system namespace. kubectl -n fybrik-system create secret generic tls-ca \\ --from-file = cacerts.pem = ./cacerts.pem Here is an example of a self-signed issuer managed by cert-manager. The secret tls-ca that holds the CA certificate is created and automatically renewed by cert-manager. apiVersion: cert-manager.io/v1 kind: ClusterIssuer metadata: name: ca-issuer-self-signed spec: selfSigned: {} --- apiVersion: cert-manager.io/v1 kind: Issuer metadata: name: ca-issuer namespace: fybrik-system spec: ca: secretName: tls-ca --- apiVersion: cert-manager.io/v1 kind: Certificate metadata: name: ca-certificate namespace: fybrik-system spec: isCA: true commonName: fybrik secretName: tls-ca issuerRef: name: ca-issuer-self-signed kind: ClusterIssuer group: cert-manager.io","title":"Using a Private CA Signed Certificate"},{"location":"tasks/control-plane-security/#update-valuesyaml-file","text":"To use TLS the infomation about the secrets above should be inserted to the fields in values.yaml file upon Fybrik deployment using helm. Here is an example of the tls related fields in the opa-connector section that are filled based on the secrets created above: opaConnector: tls: # Specifies whether the opa connector communication should use tls. use_tls: true # Specifies whether the opa connector communication should use mutual tls. use_mtls: true # Relavent if the connection between the manager and the connectors # uses tls. certs: # Name of kubernetes tls secret that holds opa-connector certificates. # The secret should be of `kubernetes.io/tls` type. # Relavent if tls is used. certSecretName: \"tls-opa-connector-certs\" # Name of kubernetes tls secret namespace that holds opa-connector certificate. certSecretNamespace: \"fybrik-system\" # Name of kubernetes secret that holds the certificate authority (CA) certificate which is used # by opa-connector to validate the connection to the manager if mtls is enabled. cacertSecretName: \"tls-ca\" # Name of kubernetes secret namespace that holds the certificate authority (CA) certificate which # is used by opa-connector to validate the connection to the manager if mtls is enabled. cacertSecretNamespace: \"fybrik-system\"","title":"Update Values.yaml file"},{"location":"tasks/control-plane-security/#using-istio","text":"Alternatively, if Istio is installed in the cluster then you can use automatic mutual TLS to encrypt the traffic to the connectors. Follow these steps to enable mutual TLS: Ensure that Istio 1.6 or above is installed. Enable Istio sidecar injection in the fybrik-system namespace: kubectl label namespace fybrik-system istio-injection = enabled - Create Istio PeerAuthentication resource to enable mutual TLS between containers with Istio sidecars: cat << EOF | kubectl apply -f - apiVersion: \"security.istio.io/v1beta1\" kind: \"PeerAuthentication\" metadata: name: \"premissive-mtls-in-control-plane\" namespace: fybrik-system spec: mtls: mode: PERMISSIVE EOF - Create Istio Sidecar resource to allow any egress traffic from the control plane containers: cat << EOF | kubectl apply -f - apiVersion: networking.istio.io/v1alpha3 kind: Sidecar metadata: name: sidecar-default namespace: fybrik-system spec: egress: - hosts: - \"*/*\" outboundTrafficPolicy: mode: ALLOW_ANY EOF - Restart the control plane pods: kubectl delete pod --all -n fybrik-system","title":"Using Istio"},{"location":"tasks/custom-taxonomy/","text":"Using a Custom Taxonomy for Resource Validation Background Fybrik acts as an orchestrator of independent components. For example, the author of the data governance policy manager, which provides the governance decisions, and the components that enforce those decisions are not necessarily the same. Thus, there is no common terminology between them. Similarly, the data formats and protocols defined in the data catalog may be defined differently than the components used for reading/writing data. In order to enable all these independent components to be used in a single architecture, Fybrik provides a taxonomy. It provides a mechanism for all the components to interact using a common dialect. The project defines a set of immutable structural JSON schemas, or \"taxonomies\" for resources deployed in Fybrik. However, since the taxonomy is meant to be configurable, a taxonomy.json file is referenced from these schemas for any definition that is customizable. The taxonomy.json file is generated from a base taxonomy and zero or more taxonomy layers: The base taxonomy is maintained by the project and includes all of the structural definitions that are subject to customization (e.g.: tags, actions). The taxonomy layers are maintained by users and external systems that add customizations over the base taxonomy (e.g., defining specific tags, actions). This task describes how to deploy Fybrik with a custom taxonomy.json file that is generated with the Taxonomy Compile CLI tool. Taxonomy Compile CLI tool A CLI tool for compiling a base taxonomy and zero or more taxonomy layers is provided in our repo. The base taxonomy can be found in charts/fybrik/files/taxonomy/taxonomy.json and example layers can be found in samples/taxonomy/example . The following command can be used from the root directory of our repo to run the Taxonomy Compile CLI tool. Usage: go run main.go taxonomy compile --out <outputFile> --base <baseFile> [ <layerFile> ... ] [ --codegen ] Flags: -b, --base string : File with base taxonomy definitions (required) --codegen : Best effort to make output suitable for code generation tools -o, --out string : Path for output file (default \"taxonomy.json\") This will generate a taxonomy.json file with the layers specified. Deploy Fybrik with Custom Taxonomy To deploy Fybrik with the generated taxonomy.json file, follow the quickstart guide but use the command below instead of helm install fybrik fybrik-charts/fybrik -n fybrik-system --wait : helm install fybrik fybrik-charts/fybrik -n fybrik-system --wait --set-file taxonomyOverride = taxonomy.json The --set-file flag will pass in your custom taxonomy.json file to use for taxonomy validation in Fybrik. If this flag is not provided, Fybrik will use the default taxonomy.json file with no layers compiled into it. For an already deployed fybrik instance, it is possible to upgrade fybrik with an updated custom taxonomy file ( taxonomy.json ) with the following command: helm upgrade fybrik fybrik-charts/fybrik -n fybrik-system --wait --set-file taxonomyOverride = taxonomy.json Examples of changing taxonomy Example 1: Add new intent for FybrikApplication In this example we show how to update the application taxonomy. We show that when a FybrikApplication yaml containing a Marketing intent is submitted, it's validation fails because initially the application's taxonomy does not include Marketing . We then describe how to add Marketing to the taxonomy, enabling the validation to pass when we re-submit the FybrikApplication yaml. Follow the quickstart guide but stop before the command helm install fybrik fybrik-charts/fybrik -n fybrik-system --wait (or helm install fybrik charts/fybrik --set global.tag=master --set global.imagePullPolicy=Always -n fybrik-system --wait in development mode). The initial taxonomy to be used in this example is a base taxonomy that can be found in charts/fybrik/files/taxonomy/taxonomy.json with the following taxonomy layer: definitions : AppInfo : properties : intent : type : string enum : - Customer Support - Fraud Detection - Customer Behavior Analysis required : - intent Copy the taxonomy layer to a taxonomy-layer.yaml file. The working directory is the fybrik repository. In order to compile and merge the two taxonomies, the Taxonomy Compile CLI tool is used in the following way: go run main.go taxonomy compile --out custom-taxonomy.json --base charts/fybrik/files/taxonomy/taxonomy.json taxonomy-layer.yaml This command creates a custom-taxonomy.json file, which is included in the helm installation of fybrik using the following command: helm install fybrik charts/fybrik --set global.tag = master --set global.imagePullPolicy = Always -n fybrik-system --wait --set-file taxonomyOverride = custom-taxonomy.json Trying to deploy a fybrikapplication.yaml that has an intent of Marketing should fail validation beacuse there is no Marketing intent in the taxonomy. The following command should fail with a description of a validation error : cat << EOF | kubectl apply -f - apiVersion : app.fybrik.io/v1alpha1 kind : FybrikApplication metadata : name : taxonomy-test spec : selector : workloadSelector : matchLabels : { app : notebook } appInfo : intent : Marketing role : Business Analyst data : - dataSetID : \"default/fake.csv\" requirements : interface : protocol : s3 dataformat : csv EOF The expected error is The FybrikApplication \"taxonomy-test\" is invalid: spec.appInfo.intent: Invalid value: \"Marketing\": spec.appInfo.intent must be one of the following: \"Customer Behavior Analysis\", \"Customer Support\", \"Fraud Detection\" . Thus, no FybrikApplication CRD was created. To fix this, a new intent with Marketing value should be added to the taxonomy. Add a new value of \"Marketing\" in custom-taxonomy.json file in intent property as follows: \"intent\": { \"type\": \"string\", \"enum\": [ \"Customer Behavior Analysis\", \"Customer Support\", \"Fraud Detection\", \"Marketing\" ] } Now we upgrade the fybrik helm chart using the following command: helm upgrade fybrik charts/fybrik --set global.tag = master --set global.imagePullPolicy = Always -n fybrik-system --wait --set-file taxonomyOverride = custom-taxonomy.json After updating fybrik to get fybrikapplications with Marketing intent, the deployment of a fybrikapplication.yaml that has an intent of Marketing will succeed: cat << EOF | kubectl apply -f - apiVersion : app.fybrik.io/v1alpha1 kind : FybrikApplication metadata : name : taxonomy-test spec : selector : workloadSelector : matchLabels : { app : notebook } appInfo : intent : Marketing role : Business Analyst data : - dataSetID : \"default/fake.csv\" requirements : interface : protocol : s3 dataformat : csv EOF The result is a FybrikApplication Custom Resource Definition instance called taxonomy-test. Example 2: Add new action for FybrikModule In this example we show how to update the module taxonomy. We show that when a FybrikModule yaml containing a FilterAction action is submitted, it's validation fails because initially the module's taxonomy does not include FilterAction . We then describe how to add a new action FilterAction to the taxonomy, enabling the validation to pass when we re-submit the FybrikModule yaml. Follow the quickstart guide but stop before the command helm install fybrik fybrik-charts/fybrik -n fybrik-system --wait (or helm install fybrik charts/fybrik --set global.tag=master --set global.imagePullPolicy=Always -n fybrik-system --wait in development mode). The initial taxonomy to be used in this example is a base taxonomy that can be found in charts/fybrik/files/taxonomy/taxonomy.json with the following taxonomy layer: definitions : Action : oneOf : - $ref : \"#/definitions/RedactAction\" - $ref : \"#/definitions/RemoveAction\" - $ref : \"#/definitions/Deny\" RedactAction : type : object properties : columns : items : type : string type : array required : - columns RemoveAction : type : object properties : columns : items : type : string type : array required : - columns Deny : type : object additionalProperties : false Copy the taxonomy layer to a taxonomy-layer.yaml file. The working directory is the fybrik repository. In order to compile and merge the two taxonomies, the Taxonomy Compile CLI tool is used in the following way: go run main.go taxonomy compile --out custom-taxonomy.json --base charts/fybrik/files/taxonomy/taxonomy.json taxonomy-layer.yaml This command creates a custom-taxonomy.json file, which is included in the helm installation of fybrik using the following command: helm install fybrik charts/fybrik --set global.tag = master --set global.imagePullPolicy = Always -n fybrik-system --wait --set-file taxonomyOverride = custom-taxonomy.json Trying to deploy a fybrikmodule.yaml that has a FilterAction should fail validation beacuse there is no FilterAction in the taxonomy. The following command should fail with a description of a validation error : cat << EOF | kubectl apply -f - apiVersion : app.fybrik.io/v1alpha1 kind : FybrikModule metadata : name : taxonomy-module-test spec : type : service chart : name : ghcr.io/fybrik/fake values : image.tag : master capabilities : - capability : read scope : workload supportedInterfaces : - source : protocol : s3 dataformat : parquet - source : protocol : s3 dataformat : csv actions : - name : RedactAction - name : FilterAction EOF The expected error is The FybrikModule \"taxonomy-module-test\" is invalid: spec.capabilities.0.actions.0.name: Invalid value: \"FilterAction\": spec.capabilities.0.actions.0.name must be one of the following: \"Deny\", \"RedactAction\", \"RemoveAction\" . Thus, no FybrikModule CRD was created. To fix this, a new action FilterAction should be added to the taxonomy. Add a new file taxonomy-layer2.yaml with the new action FilterAction as follows: definitions : Action : oneOf : - $ref : \"#/definitions/FilterAction\" FilterAction : type : object properties : threshold : type : integer operation : type : string required : - threshold Now we create the custom-taxonomy.json file as before, by using the following command: go run main.go taxonomy compile --out custom-taxonomy.json --base charts/fybrik/files/taxonomy/taxonomy.json taxonomy-layer.yaml taxonomy-layer2.yaml Now we upgrade the fybrik helm chart using the following command: helm upgrade fybrik charts/fybrik --set global.tag = master --set global.imagePullPolicy = Always -n fybrik-system --wait --set-file taxonomyOverride = custom-taxonomy.json After updating fybrik to get fybrikmodule with FilterAction , the deployment of a fybrikmodule.yaml that has a FilterAction will succeed: cat << EOF | kubectl apply -f - apiVersion : app.fybrik.io/v1alpha1 kind : FybrikModule metadata : name : taxonomy-module-test spec : type : service chart : name : ghcr.io/fybrik/fake values : image.tag : master capabilities : - capability : read scope : workload supportedInterfaces : - source : protocol : s3 dataformat : parquet - source : protocol : s3 dataformat : csv actions : - name : RedactAction - name : FilterAction EOF The result is a FybrikModule Custom Resource Definition instance called taxonomy-module-test.","title":"Using a Custom Taxonomy for Resource Validation"},{"location":"tasks/custom-taxonomy/#using-a-custom-taxonomy-for-resource-validation","text":"","title":"Using a Custom Taxonomy for Resource Validation"},{"location":"tasks/custom-taxonomy/#background","text":"Fybrik acts as an orchestrator of independent components. For example, the author of the data governance policy manager, which provides the governance decisions, and the components that enforce those decisions are not necessarily the same. Thus, there is no common terminology between them. Similarly, the data formats and protocols defined in the data catalog may be defined differently than the components used for reading/writing data. In order to enable all these independent components to be used in a single architecture, Fybrik provides a taxonomy. It provides a mechanism for all the components to interact using a common dialect. The project defines a set of immutable structural JSON schemas, or \"taxonomies\" for resources deployed in Fybrik. However, since the taxonomy is meant to be configurable, a taxonomy.json file is referenced from these schemas for any definition that is customizable. The taxonomy.json file is generated from a base taxonomy and zero or more taxonomy layers: The base taxonomy is maintained by the project and includes all of the structural definitions that are subject to customization (e.g.: tags, actions). The taxonomy layers are maintained by users and external systems that add customizations over the base taxonomy (e.g., defining specific tags, actions). This task describes how to deploy Fybrik with a custom taxonomy.json file that is generated with the Taxonomy Compile CLI tool.","title":"Background"},{"location":"tasks/custom-taxonomy/#taxonomy-compile-cli-tool","text":"A CLI tool for compiling a base taxonomy and zero or more taxonomy layers is provided in our repo. The base taxonomy can be found in charts/fybrik/files/taxonomy/taxonomy.json and example layers can be found in samples/taxonomy/example . The following command can be used from the root directory of our repo to run the Taxonomy Compile CLI tool. Usage: go run main.go taxonomy compile --out <outputFile> --base <baseFile> [ <layerFile> ... ] [ --codegen ] Flags: -b, --base string : File with base taxonomy definitions (required) --codegen : Best effort to make output suitable for code generation tools -o, --out string : Path for output file (default \"taxonomy.json\") This will generate a taxonomy.json file with the layers specified.","title":"Taxonomy Compile CLI tool"},{"location":"tasks/custom-taxonomy/#deploy-fybrik-with-custom-taxonomy","text":"To deploy Fybrik with the generated taxonomy.json file, follow the quickstart guide but use the command below instead of helm install fybrik fybrik-charts/fybrik -n fybrik-system --wait : helm install fybrik fybrik-charts/fybrik -n fybrik-system --wait --set-file taxonomyOverride = taxonomy.json The --set-file flag will pass in your custom taxonomy.json file to use for taxonomy validation in Fybrik. If this flag is not provided, Fybrik will use the default taxonomy.json file with no layers compiled into it. For an already deployed fybrik instance, it is possible to upgrade fybrik with an updated custom taxonomy file ( taxonomy.json ) with the following command: helm upgrade fybrik fybrik-charts/fybrik -n fybrik-system --wait --set-file taxonomyOverride = taxonomy.json","title":"Deploy Fybrik with Custom Taxonomy"},{"location":"tasks/custom-taxonomy/#examples-of-changing-taxonomy","text":"","title":"Examples of changing taxonomy"},{"location":"tasks/custom-taxonomy/#example-1-add-new-intent-for-fybrikapplication","text":"In this example we show how to update the application taxonomy. We show that when a FybrikApplication yaml containing a Marketing intent is submitted, it's validation fails because initially the application's taxonomy does not include Marketing . We then describe how to add Marketing to the taxonomy, enabling the validation to pass when we re-submit the FybrikApplication yaml. Follow the quickstart guide but stop before the command helm install fybrik fybrik-charts/fybrik -n fybrik-system --wait (or helm install fybrik charts/fybrik --set global.tag=master --set global.imagePullPolicy=Always -n fybrik-system --wait in development mode). The initial taxonomy to be used in this example is a base taxonomy that can be found in charts/fybrik/files/taxonomy/taxonomy.json with the following taxonomy layer: definitions : AppInfo : properties : intent : type : string enum : - Customer Support - Fraud Detection - Customer Behavior Analysis required : - intent Copy the taxonomy layer to a taxonomy-layer.yaml file. The working directory is the fybrik repository. In order to compile and merge the two taxonomies, the Taxonomy Compile CLI tool is used in the following way: go run main.go taxonomy compile --out custom-taxonomy.json --base charts/fybrik/files/taxonomy/taxonomy.json taxonomy-layer.yaml This command creates a custom-taxonomy.json file, which is included in the helm installation of fybrik using the following command: helm install fybrik charts/fybrik --set global.tag = master --set global.imagePullPolicy = Always -n fybrik-system --wait --set-file taxonomyOverride = custom-taxonomy.json Trying to deploy a fybrikapplication.yaml that has an intent of Marketing should fail validation beacuse there is no Marketing intent in the taxonomy. The following command should fail with a description of a validation error : cat << EOF | kubectl apply -f - apiVersion : app.fybrik.io/v1alpha1 kind : FybrikApplication metadata : name : taxonomy-test spec : selector : workloadSelector : matchLabels : { app : notebook } appInfo : intent : Marketing role : Business Analyst data : - dataSetID : \"default/fake.csv\" requirements : interface : protocol : s3 dataformat : csv EOF The expected error is The FybrikApplication \"taxonomy-test\" is invalid: spec.appInfo.intent: Invalid value: \"Marketing\": spec.appInfo.intent must be one of the following: \"Customer Behavior Analysis\", \"Customer Support\", \"Fraud Detection\" . Thus, no FybrikApplication CRD was created. To fix this, a new intent with Marketing value should be added to the taxonomy. Add a new value of \"Marketing\" in custom-taxonomy.json file in intent property as follows: \"intent\": { \"type\": \"string\", \"enum\": [ \"Customer Behavior Analysis\", \"Customer Support\", \"Fraud Detection\", \"Marketing\" ] } Now we upgrade the fybrik helm chart using the following command: helm upgrade fybrik charts/fybrik --set global.tag = master --set global.imagePullPolicy = Always -n fybrik-system --wait --set-file taxonomyOverride = custom-taxonomy.json After updating fybrik to get fybrikapplications with Marketing intent, the deployment of a fybrikapplication.yaml that has an intent of Marketing will succeed: cat << EOF | kubectl apply -f - apiVersion : app.fybrik.io/v1alpha1 kind : FybrikApplication metadata : name : taxonomy-test spec : selector : workloadSelector : matchLabels : { app : notebook } appInfo : intent : Marketing role : Business Analyst data : - dataSetID : \"default/fake.csv\" requirements : interface : protocol : s3 dataformat : csv EOF The result is a FybrikApplication Custom Resource Definition instance called taxonomy-test.","title":"Example 1: Add new intent for FybrikApplication"},{"location":"tasks/custom-taxonomy/#example-2-add-new-action-for-fybrikmodule","text":"In this example we show how to update the module taxonomy. We show that when a FybrikModule yaml containing a FilterAction action is submitted, it's validation fails because initially the module's taxonomy does not include FilterAction . We then describe how to add a new action FilterAction to the taxonomy, enabling the validation to pass when we re-submit the FybrikModule yaml. Follow the quickstart guide but stop before the command helm install fybrik fybrik-charts/fybrik -n fybrik-system --wait (or helm install fybrik charts/fybrik --set global.tag=master --set global.imagePullPolicy=Always -n fybrik-system --wait in development mode). The initial taxonomy to be used in this example is a base taxonomy that can be found in charts/fybrik/files/taxonomy/taxonomy.json with the following taxonomy layer: definitions : Action : oneOf : - $ref : \"#/definitions/RedactAction\" - $ref : \"#/definitions/RemoveAction\" - $ref : \"#/definitions/Deny\" RedactAction : type : object properties : columns : items : type : string type : array required : - columns RemoveAction : type : object properties : columns : items : type : string type : array required : - columns Deny : type : object additionalProperties : false Copy the taxonomy layer to a taxonomy-layer.yaml file. The working directory is the fybrik repository. In order to compile and merge the two taxonomies, the Taxonomy Compile CLI tool is used in the following way: go run main.go taxonomy compile --out custom-taxonomy.json --base charts/fybrik/files/taxonomy/taxonomy.json taxonomy-layer.yaml This command creates a custom-taxonomy.json file, which is included in the helm installation of fybrik using the following command: helm install fybrik charts/fybrik --set global.tag = master --set global.imagePullPolicy = Always -n fybrik-system --wait --set-file taxonomyOverride = custom-taxonomy.json Trying to deploy a fybrikmodule.yaml that has a FilterAction should fail validation beacuse there is no FilterAction in the taxonomy. The following command should fail with a description of a validation error : cat << EOF | kubectl apply -f - apiVersion : app.fybrik.io/v1alpha1 kind : FybrikModule metadata : name : taxonomy-module-test spec : type : service chart : name : ghcr.io/fybrik/fake values : image.tag : master capabilities : - capability : read scope : workload supportedInterfaces : - source : protocol : s3 dataformat : parquet - source : protocol : s3 dataformat : csv actions : - name : RedactAction - name : FilterAction EOF The expected error is The FybrikModule \"taxonomy-module-test\" is invalid: spec.capabilities.0.actions.0.name: Invalid value: \"FilterAction\": spec.capabilities.0.actions.0.name must be one of the following: \"Deny\", \"RedactAction\", \"RemoveAction\" . Thus, no FybrikModule CRD was created. To fix this, a new action FilterAction should be added to the taxonomy. Add a new file taxonomy-layer2.yaml with the new action FilterAction as follows: definitions : Action : oneOf : - $ref : \"#/definitions/FilterAction\" FilterAction : type : object properties : threshold : type : integer operation : type : string required : - threshold Now we create the custom-taxonomy.json file as before, by using the following command: go run main.go taxonomy compile --out custom-taxonomy.json --base charts/fybrik/files/taxonomy/taxonomy.json taxonomy-layer.yaml taxonomy-layer2.yaml Now we upgrade the fybrik helm chart using the following command: helm upgrade fybrik charts/fybrik --set global.tag = master --set global.imagePullPolicy = Always -n fybrik-system --wait --set-file taxonomyOverride = custom-taxonomy.json After updating fybrik to get fybrikmodule with FilterAction , the deployment of a fybrikmodule.yaml that has a FilterAction will succeed: cat << EOF | kubectl apply -f - apiVersion : app.fybrik.io/v1alpha1 kind : FybrikModule metadata : name : taxonomy-module-test spec : type : service chart : name : ghcr.io/fybrik/fake values : image.tag : master capabilities : - capability : read scope : workload supportedInterfaces : - source : protocol : s3 dataformat : parquet - source : protocol : s3 dataformat : csv actions : - name : RedactAction - name : FilterAction EOF The result is a FybrikModule Custom Resource Definition instance called taxonomy-module-test.","title":"Example 2: Add new action for FybrikModule"},{"location":"tasks/high-availability/","text":"High Availability When having more then one manager instance running Active/Passive high availability method is used: only one of them is being the leader (doing all the exclusive tasks) and other instances waiting in standby mode in case the leader dies to take over the leader role. The Active/Passive high availability is implemented with Kubernetes leader election mechanism and it is turned on by default. In this implementation a config-map resource called fybrik-operator-leader-election serves as a lock. The config-map also contains information about the chosen leader. To change Fybrik manager number of replicas the following setting should be added to Fybrik helm chart deployment command --set manager.replicaCount=<desired replica value> .","title":"High Availability"},{"location":"tasks/high-availability/#high-availability","text":"When having more then one manager instance running Active/Passive high availability method is used: only one of them is being the leader (doing all the exclusive tasks) and other instances waiting in standby mode in case the leader dies to take over the leader role. The Active/Passive high availability is implemented with Kubernetes leader election mechanism and it is turned on by default. In this implementation a config-map resource called fybrik-operator-leader-election serves as a lock. The config-map also contains information about the chosen leader. To change Fybrik manager number of replicas the following setting should be added to Fybrik helm chart deployment command --set manager.replicaCount=<desired replica value> .","title":"High Availability"},{"location":"tasks/multicluster/","text":"Multicluster setup Fybrik is dynamic in its multi cluster capabilities in that it has abstractions to support multiple different cross-cluster orchestration mechanisms. Currently, only one multi cluster orchestration mechanism is implemented and is using Razee for the orchestration. Multicluster operation with Razee Razee is a multi-cluster continuous delivery tool for Kubernetes that can deploy software on remote clusters and track the deployment status of such deployments. There are multiple ways to run Razee. The two described here are a vanilla open source deployment on your own Kubernetes or as a managed service from a cloud provider. Due to the complex nature of installing Razee a managed service from a cloud provider is recommended. It's possible to define a multicluster group name that groups clusters that are used in a Fybrik instance. This will restrict the clusters that are usable in the Fybrik instance to the ones that are registered in the specified Razee group. This is especially helpful if Razee is also used for different purposes than Fybrik or multiple Fybrik instances should be used under the same Razee installation. In general there is a need for the following Razee components to be installed: Razee watch keeper (installed on all clusters) Razee cluster subscription manager (installed on all clusters) RazeeDash API (installed on coordinator cluster/as cloud service) Both methods below describe how the above components can be installed depending on what RazeeDash deployment method is used. Installing Razee on Kubernetes Coordinator cluster An installation of the open source components is described here . Please follow the instructions in the Razee documentation to install RazeeDash , Watch keeper and the cluster subscription agent . At the moment Razee supports GitHub, GitHub Enterprise and BitBucket for the OAUTH Authentication of this installation. Please be aware that the RazeeDash API needs to be reachable from all clusters. Thus, there may be the need for routes, ingresses or node ports in order to expose it to other networks and clusters. Once RazeeDash is installed the UI can be used to group registered clusters in a multicluster group that can be configured below. The API Key can also be retrieved from the UI following these two steps. From the RazeeDash console, click the arrow icon in the upper right corner. Then, select Profile. Copy the API key value. If no API key exists, click Generate to generate one. In order to configure Fybrik to use the installed Razee on Kubernetes the values of the helm charts have to be adapted to the following: coordinator: razee: # URL for Razee deployment url: \"https://your-razee-service:3333/graphql\" # Razee deployment with oauth API key authentication requires the apiKey parameter apiKey: \"<your Razee X_API_KEY>\" multiclusterGroup: \"<your group name>\" Remote cluster The remote clusters only need the watch keeper and cluster subscription agents installed. The remote clusters do not need the coordinator component of Fybrik. It's enough to follow this guide to install the agents and configure a group via the RazeeDash UI if needed. The coordinator configuration would look like the following: coordinator: enabled: false Installing using IBM Satellite Config When using IBM Satellite Config the RazeeDash API is running as a service in the cloud and all custom resource distribution is handled by the cloud. The process here describes how an already existing Kubernetes cluster can be registered and configured. Prerequisites: An IBM Cloud Account IBM Cloud Satellite service IAM API Keys with access to IBM Cloud Satellite service The step below has to be executed for each cluster that should be added to the Fybrik instance. This step is the same for coordinator and remote clusters. In the IBM Satellite Cloud service under the Clusters tab click on Register cluster . Enter a cluster name in the popup dialog and click Register cluster . (Please don't use spaces in the name) The next dialog will offer you a kubectl command that can be executed on the cluster that should be attached. After executing the kubectl command the Razee services will be installed in the razeedeploy namespace and the cluster will show up in your cluster list (like in the picture above). This installs the watch keeper and cluster subscription components. Create clusters groups by clicking on the Cluster groups tab: for each cluster create a group named fybrik-<cluster-name> and add the cluster to that group. In addition, create a single group for all the clusters: the name of this group is used when deploying the coordinator cluster as shown below. The next step is to configure Fybrik to use IBM Satellite config as multicluster orchestrator. This configuration is done via a Kubernetes secret that is created by the helm chart. Overwriting the coordinator.razee values in your deployment will make use of the multicluster tooling. A configuration using IBM Satellite Config would look like the following for the coordinator cluster: coordinator: # Configures the Razee instance to be used by the coordinator manager in a multicluster setup razee: # IBM Cloud IAM API Key of a user or service account that have access to IBM Cloud Satellite Config iamKey: \"<your IAM API KEY key>\" multiclusterGroup: \"<your group name>\" For the remote cluster the coordinator will be disabled: coordinator: enabled: false Configure Vault for multi-cluster deployment The Fybrik uses HashiCorp Vault to provide running Fybrik modules in the clusters with the dataset credentials when accessing data. This is done using Vault plugin system as described in vault plugin page . This section describes steps for the Fybrik modules to authenticate with Vault, in order to obtain database credentials. Some of the steps described below are not specific to the Fybrik project but rather are Vault specific and can be found in Vault-related online tutorials. Module authentication is done by configuring Vault to use Kubernetes auth method in each cluster. Using this method, the modules can authenticate to Vault by providing their service account token. Behind the scenes, Vault authenticates the token by submitting a TokenReview request to the API server of the Kubernetes cluster where the module is running. Prerequisites unless Fybrik modules are running on the same cluster as the Vault instance: The running Vault instance should have connectivity to the cluster API server for each cluster running Fybrik modules. The running Vault instance should have an Ingress resource to enable Fybrik modules to get the credentials. Before you begin Ensure that you have the Vault v1.9.x to execute Vault CLI commands. Enabling Kubernetes authentication for each cluster with running Fybrik modules: Create a token reviewer service account called vault-auth in the fybrik-system namespace and give it permissions to create tokenreviews.authentication.k8s.io at the cluster scope: apiVersion: v1 kind: ServiceAccount metadata: name: vault-auth namespace: fybrik-system --- apiVersion: v1 kind: Secret metadata: name: vault-auth namespace: fybrik-system annotations: kubernetes.io/service-account.name: vault-auth type: kubernetes.io/service-account-token --- apiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRoleBinding metadata: name: role-tokenreview-binding roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: system:auth-delegator subjects: - kind: ServiceAccount name: vault-auth namespace: fybrik-system Login to Vault: vault login Enable the Kubernetes auth method in a new path: vault auth enable -path = <auth path> kubernetes Use the /config endpoint to configure Vault to talk to Kubernetes: TOKEN_REVIEW_JWT = $( kubectl get secret vault-auth -n fybrik-system -o jsonpath = \"{.data.token}\" | base64 --decode ) vault write auth/<auth path>/config \\ token_reviewer_jwt = \" $TOKEN_REVIEW_JWT \" \\ kubernetes_host = <Kubernetes api server address> \\ kubernetes_ca_cert = @ca.crt More details on the parameters in the command above can be found in Vault's documentation . Add the Vault policy and role to allow the modules to get the dataset credentials. More details on defining Vault policy for Fybrik can be found in HashiCorp Vault plugins page . The Vault role which binds the policy to the modules can be defined in the following example: vault write auth/<auth path>/role/module \\ bound_service_account_names = \"*\" \\ bound_service_account_namespaces = <modules namespace> \\ policies = \"allow-all-dataset-creds\" \\ ttl = 24h Deploy the Fybrik helm chart with --set cluster.vaultAuthPath=<auth path> parameter","title":"Multicluster setup"},{"location":"tasks/multicluster/#multicluster-setup","text":"Fybrik is dynamic in its multi cluster capabilities in that it has abstractions to support multiple different cross-cluster orchestration mechanisms. Currently, only one multi cluster orchestration mechanism is implemented and is using Razee for the orchestration.","title":"Multicluster setup"},{"location":"tasks/multicluster/#multicluster-operation-with-razee","text":"Razee is a multi-cluster continuous delivery tool for Kubernetes that can deploy software on remote clusters and track the deployment status of such deployments. There are multiple ways to run Razee. The two described here are a vanilla open source deployment on your own Kubernetes or as a managed service from a cloud provider. Due to the complex nature of installing Razee a managed service from a cloud provider is recommended. It's possible to define a multicluster group name that groups clusters that are used in a Fybrik instance. This will restrict the clusters that are usable in the Fybrik instance to the ones that are registered in the specified Razee group. This is especially helpful if Razee is also used for different purposes than Fybrik or multiple Fybrik instances should be used under the same Razee installation. In general there is a need for the following Razee components to be installed: Razee watch keeper (installed on all clusters) Razee cluster subscription manager (installed on all clusters) RazeeDash API (installed on coordinator cluster/as cloud service) Both methods below describe how the above components can be installed depending on what RazeeDash deployment method is used.","title":"Multicluster operation with Razee"},{"location":"tasks/multicluster/#installing-razee-on-kubernetes","text":"","title":"Installing Razee on Kubernetes"},{"location":"tasks/multicluster/#coordinator-cluster","text":"An installation of the open source components is described here . Please follow the instructions in the Razee documentation to install RazeeDash , Watch keeper and the cluster subscription agent . At the moment Razee supports GitHub, GitHub Enterprise and BitBucket for the OAUTH Authentication of this installation. Please be aware that the RazeeDash API needs to be reachable from all clusters. Thus, there may be the need for routes, ingresses or node ports in order to expose it to other networks and clusters. Once RazeeDash is installed the UI can be used to group registered clusters in a multicluster group that can be configured below. The API Key can also be retrieved from the UI following these two steps. From the RazeeDash console, click the arrow icon in the upper right corner. Then, select Profile. Copy the API key value. If no API key exists, click Generate to generate one. In order to configure Fybrik to use the installed Razee on Kubernetes the values of the helm charts have to be adapted to the following: coordinator: razee: # URL for Razee deployment url: \"https://your-razee-service:3333/graphql\" # Razee deployment with oauth API key authentication requires the apiKey parameter apiKey: \"<your Razee X_API_KEY>\" multiclusterGroup: \"<your group name>\"","title":"Coordinator cluster"},{"location":"tasks/multicluster/#remote-cluster","text":"The remote clusters only need the watch keeper and cluster subscription agents installed. The remote clusters do not need the coordinator component of Fybrik. It's enough to follow this guide to install the agents and configure a group via the RazeeDash UI if needed. The coordinator configuration would look like the following: coordinator: enabled: false","title":"Remote cluster"},{"location":"tasks/multicluster/#installing-using-ibm-satellite-config","text":"When using IBM Satellite Config the RazeeDash API is running as a service in the cloud and all custom resource distribution is handled by the cloud. The process here describes how an already existing Kubernetes cluster can be registered and configured. Prerequisites: An IBM Cloud Account IBM Cloud Satellite service IAM API Keys with access to IBM Cloud Satellite service The step below has to be executed for each cluster that should be added to the Fybrik instance. This step is the same for coordinator and remote clusters. In the IBM Satellite Cloud service under the Clusters tab click on Register cluster . Enter a cluster name in the popup dialog and click Register cluster . (Please don't use spaces in the name) The next dialog will offer you a kubectl command that can be executed on the cluster that should be attached. After executing the kubectl command the Razee services will be installed in the razeedeploy namespace and the cluster will show up in your cluster list (like in the picture above). This installs the watch keeper and cluster subscription components. Create clusters groups by clicking on the Cluster groups tab: for each cluster create a group named fybrik-<cluster-name> and add the cluster to that group. In addition, create a single group for all the clusters: the name of this group is used when deploying the coordinator cluster as shown below. The next step is to configure Fybrik to use IBM Satellite config as multicluster orchestrator. This configuration is done via a Kubernetes secret that is created by the helm chart. Overwriting the coordinator.razee values in your deployment will make use of the multicluster tooling. A configuration using IBM Satellite Config would look like the following for the coordinator cluster: coordinator: # Configures the Razee instance to be used by the coordinator manager in a multicluster setup razee: # IBM Cloud IAM API Key of a user or service account that have access to IBM Cloud Satellite Config iamKey: \"<your IAM API KEY key>\" multiclusterGroup: \"<your group name>\" For the remote cluster the coordinator will be disabled: coordinator: enabled: false","title":"Installing using IBM Satellite Config"},{"location":"tasks/multicluster/#configure-vault-for-multi-cluster-deployment","text":"The Fybrik uses HashiCorp Vault to provide running Fybrik modules in the clusters with the dataset credentials when accessing data. This is done using Vault plugin system as described in vault plugin page . This section describes steps for the Fybrik modules to authenticate with Vault, in order to obtain database credentials. Some of the steps described below are not specific to the Fybrik project but rather are Vault specific and can be found in Vault-related online tutorials. Module authentication is done by configuring Vault to use Kubernetes auth method in each cluster. Using this method, the modules can authenticate to Vault by providing their service account token. Behind the scenes, Vault authenticates the token by submitting a TokenReview request to the API server of the Kubernetes cluster where the module is running.","title":"Configure Vault for multi-cluster deployment"},{"location":"tasks/multicluster/#prerequisites-unless-fybrik-modules-are-running-on-the-same-cluster-as-the-vault-instance","text":"The running Vault instance should have connectivity to the cluster API server for each cluster running Fybrik modules. The running Vault instance should have an Ingress resource to enable Fybrik modules to get the credentials.","title":"Prerequisites unless Fybrik modules are running on the same cluster as the Vault instance:"},{"location":"tasks/multicluster/#before-you-begin","text":"Ensure that you have the Vault v1.9.x to execute Vault CLI commands.","title":"Before you begin"},{"location":"tasks/multicluster/#enabling-kubernetes-authentication-for-each-cluster-with-running-fybrik-modules","text":"Create a token reviewer service account called vault-auth in the fybrik-system namespace and give it permissions to create tokenreviews.authentication.k8s.io at the cluster scope: apiVersion: v1 kind: ServiceAccount metadata: name: vault-auth namespace: fybrik-system --- apiVersion: v1 kind: Secret metadata: name: vault-auth namespace: fybrik-system annotations: kubernetes.io/service-account.name: vault-auth type: kubernetes.io/service-account-token --- apiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRoleBinding metadata: name: role-tokenreview-binding roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: system:auth-delegator subjects: - kind: ServiceAccount name: vault-auth namespace: fybrik-system Login to Vault: vault login Enable the Kubernetes auth method in a new path: vault auth enable -path = <auth path> kubernetes Use the /config endpoint to configure Vault to talk to Kubernetes: TOKEN_REVIEW_JWT = $( kubectl get secret vault-auth -n fybrik-system -o jsonpath = \"{.data.token}\" | base64 --decode ) vault write auth/<auth path>/config \\ token_reviewer_jwt = \" $TOKEN_REVIEW_JWT \" \\ kubernetes_host = <Kubernetes api server address> \\ kubernetes_ca_cert = @ca.crt More details on the parameters in the command above can be found in Vault's documentation . Add the Vault policy and role to allow the modules to get the dataset credentials. More details on defining Vault policy for Fybrik can be found in HashiCorp Vault plugins page . The Vault role which binds the policy to the modules can be defined in the following example: vault write auth/<auth path>/role/module \\ bound_service_account_names = \"*\" \\ bound_service_account_namespaces = <modules namespace> \\ policies = \"allow-all-dataset-creds\" \\ ttl = 24h Deploy the Fybrik helm chart with --set cluster.vaultAuthPath=<auth path> parameter","title":"Enabling Kubernetes authentication for each cluster with running Fybrik modules:"},{"location":"tasks/performance/","text":"Performance When using many fybric applications at the same time the custom resource operations may take some time. This is due to the default concurrency of controllers being one at a time and the Kubernetes client being rate limited by default. In order to increase the parallelism there are multiple parameters that can be controlled. Each controller parallelism (for each fybrik custom resource) can be controlled separately. When increasing this number it's highly recommended to also increase the managers Kubernetes client QPS and Boost settings so that the controller won't be limited by the amount of queries it can execute to the Kubernetes API. An adapted helm values configuration looks like the following: # Manager component manager: extraEnvs: - name: APPLICATION_CONCURRENT_RECONCILES value: \"5\" - name: BLUEPRINT_CONCURRENT_RECONCILES value: \"20\" - name: PLOTTER_CONCURRENT_RECONCILES value: \"2\" - name: CLIENT_QPS value: \"100.0\" - name: CLIENT_BURST value: \"200\" Please notice that QPS is a float while the other values are integer values.","title":"Performance"},{"location":"tasks/performance/#performance","text":"When using many fybric applications at the same time the custom resource operations may take some time. This is due to the default concurrency of controllers being one at a time and the Kubernetes client being rate limited by default. In order to increase the parallelism there are multiple parameters that can be controlled. Each controller parallelism (for each fybrik custom resource) can be controlled separately. When increasing this number it's highly recommended to also increase the managers Kubernetes client QPS and Boost settings so that the controller won't be limited by the amount of queries it can execute to the Kubernetes API. An adapted helm values configuration looks like the following: # Manager component manager: extraEnvs: - name: APPLICATION_CONCURRENT_RECONCILES value: \"5\" - name: BLUEPRINT_CONCURRENT_RECONCILES value: \"20\" - name: PLOTTER_CONCURRENT_RECONCILES value: \"2\" - name: CLIENT_QPS value: \"100.0\" - name: CLIENT_BURST value: \"200\" Please notice that QPS is a float while the other values are integer values.","title":"Performance"},{"location":"tasks/using-opa/","text":"Using OPA for Data Governance Open Policy Agent may be used as a data governance policy engine with Fybrik via the connector mechanism. When OPA is used for data governance, it is deployed as a stand-alone service. Policies are defined in rego and uploaded to OPA. For more details on OPA policies please refer to OPA documentation in particulate to the basics section which explains how a policy is evaluated. Fybrik Default Policies Fybrik denys by default any request if no rule is triggered. This behavior can be changed to allow by default by creating the following rule and upload it to OPA using methods described in this page: package dataapi.authz rule [{}] { true } You can also add conditions like rule[{}] { // conditions here } The verdict allow will be reached only if the conditions hold, and no other rule has been triggered, e.g. a rule requiring column redaction. Managing OPA policies There are several ways to manage policies and data of the OPA service. One simple approach is to use OPA kube-mgmt and manage Rego policies in Kubernetes Configmap resources. By default, Fybrik installs OPA with kube-mgmt enabled. The following two sections show how to use OPA with kube-mgmt. Warning Due to size limits you must ensure that each configmap is smaller than 1MB when base64 encoded. Using a configmap YAML Create a configmap with a Rego policy and a openpolicyagent.org/policy=rego label in the fybrik-system namespace: apiVersion : v1 kind : ConfigMap metadata : name : <policy-name> namespace : fybrik-system labels : openpolicyagent.org/policy : rego data : main : | <you rego policy here> Apply the configmap: kubectl apply -f <policy-name>.yaml To remove the policy just remove the configmap: kubectl delete -f <policy-name>.yaml Using a Rego file You can use kubectl to create a configmap from a Rego file. To create a configmap named <policy-name> from a Rego file in path <policy-name.rego> : kubectl create configmap <policy-name> --from-file = main = <policy-name.rego> -n fybrik-system kubectl label configmap <policy-name> openpolicyagent.org/policy = rego -n fybrik-system Delete the policy with kubectl delete configmap <policy-name> -n fybrik-system . Using opaServer.bootstrapPolicies field Another method to upload policies to OPA is to write them as opaServer.bootstrapPolicies field in values.yaml file used for the Fybrik deployment. In this approach the policies are uploaded upon OPA startup. opaServer: # Bootstrap policies to load upon startup bootstrapPolicies: allowSamplePolicy: | - package dataapi.authz rule [{}] { true }","title":"Using OPA for Data Governance"},{"location":"tasks/using-opa/#using-opa-for-data-governance","text":"Open Policy Agent may be used as a data governance policy engine with Fybrik via the connector mechanism. When OPA is used for data governance, it is deployed as a stand-alone service. Policies are defined in rego and uploaded to OPA. For more details on OPA policies please refer to OPA documentation in particulate to the basics section which explains how a policy is evaluated.","title":"Using OPA for Data Governance"},{"location":"tasks/using-opa/#fybrik-default-policies","text":"Fybrik denys by default any request if no rule is triggered. This behavior can be changed to allow by default by creating the following rule and upload it to OPA using methods described in this page: package dataapi.authz rule [{}] { true } You can also add conditions like rule[{}] { // conditions here } The verdict allow will be reached only if the conditions hold, and no other rule has been triggered, e.g. a rule requiring column redaction.","title":"Fybrik Default Policies"},{"location":"tasks/using-opa/#managing-opa-policies","text":"There are several ways to manage policies and data of the OPA service. One simple approach is to use OPA kube-mgmt and manage Rego policies in Kubernetes Configmap resources. By default, Fybrik installs OPA with kube-mgmt enabled. The following two sections show how to use OPA with kube-mgmt. Warning Due to size limits you must ensure that each configmap is smaller than 1MB when base64 encoded.","title":"Managing OPA policies"},{"location":"tasks/using-opa/#using-a-configmap-yaml","text":"Create a configmap with a Rego policy and a openpolicyagent.org/policy=rego label in the fybrik-system namespace: apiVersion : v1 kind : ConfigMap metadata : name : <policy-name> namespace : fybrik-system labels : openpolicyagent.org/policy : rego data : main : | <you rego policy here> Apply the configmap: kubectl apply -f <policy-name>.yaml To remove the policy just remove the configmap: kubectl delete -f <policy-name>.yaml","title":"Using a configmap YAML"},{"location":"tasks/using-opa/#using-a-rego-file","text":"You can use kubectl to create a configmap from a Rego file. To create a configmap named <policy-name> from a Rego file in path <policy-name.rego> : kubectl create configmap <policy-name> --from-file = main = <policy-name.rego> -n fybrik-system kubectl label configmap <policy-name> openpolicyagent.org/policy = rego -n fybrik-system Delete the policy with kubectl delete configmap <policy-name> -n fybrik-system .","title":"Using a Rego file"},{"location":"tasks/using-opa/#using-opaserverbootstrappolicies-field","text":"Another method to upload policies to OPA is to write them as opaServer.bootstrapPolicies field in values.yaml file used for the Fybrik deployment. In this approach the policies are uploaded upon OPA startup. opaServer: # Bootstrap policies to load upon startup bootstrapPolicies: allowSamplePolicy: | - package dataapi.authz rule [{}] { true }","title":"Using opaServer.bootstrapPolicies field"}]}